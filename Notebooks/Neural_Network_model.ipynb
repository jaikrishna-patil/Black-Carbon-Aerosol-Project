{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv(\"database.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q_abs</th>\n",
       "      <th>q_sca</th>\n",
       "      <th>g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.11691</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>0.003798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.15300</td>\n",
       "      <td>0.001005</td>\n",
       "      <td>0.008979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.20060</td>\n",
       "      <td>0.002514</td>\n",
       "      <td>0.022970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.23162</td>\n",
       "      <td>0.004187</td>\n",
       "      <td>0.037044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.25746</td>\n",
       "      <td>0.005988</td>\n",
       "      <td>0.051049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     q_abs     q_sca         g\n",
       "0  0.11691  0.000389  0.003798\n",
       "1  0.15300  0.001005  0.008979\n",
       "2  0.20060  0.002514  0.022970\n",
       "3  0.23162  0.004187  0.037044\n",
       "4  0.25746  0.005988  0.051049"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = df1.iloc[:,25:28]\n",
    "X = df1.iloc[:,:8]\n",
    "Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7370, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "#         X, Y, \n",
    "#         test_size=0.25, \n",
    "#         random_state=42)\n",
    "# Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling_x=StandardScaler()\n",
    "# scaling_y=StandardScaler()\n",
    "# X_train=scaling_x.fit_transform(X_train)\n",
    "# X_test=scaling_x.transform(X_test)\n",
    "# Y_train=scaling_y.fit_transform(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               1152      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 166,531\n",
      "Trainable params: 166,531\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# NN_model1 = Sequential()\n",
    "\n",
    "# # The Input Layer :\n",
    "# NN_model1.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n",
    "\n",
    "# # The Hidden Layers :\n",
    "# NN_model1.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "# NN_model1.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "# NN_model1.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "\n",
    "# # The Output Layer :\n",
    "# NN_model1.add(Dense(3, kernel_initializer='normal',activation='linear'))\n",
    "# # Compile the network :\n",
    "# NN_model1.compile(loss='mean_absolute_percentage_error', optimizer='adam', metrics=['accuracy'])\n",
    "# NN_model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath=\"random_split_with_min_max/Weights-{epoch:03d}--{val_loss:.5f}.hdf5\"\n",
    "\n",
    "# checkpoint = ModelCheckpoint(filepath, verbose=1, monitor='val_loss',save_best_only=True, mode='auto')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # patient early stopping\n",
    "# es = EarlyStopping(monitor='val_loss', patience=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_csv=CSVLogger('random_split_with_min_max_loss_logs.csv', separator=',', append=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback_list=[checkpoint, es, log_csv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "142/185 [======================>.......] - ETA: 0s - loss: 87.8542 - accuracy: 0.3834\n",
      "Epoch 00001: val_loss improved from inf to 67.42806, saving model to random_split_with_min_max\\Weights-001--67.42806.hdf5\n",
      "185/185 [==============================] - 0s 2ms/step - loss: 83.9793 - accuracy: 0.4374 - val_loss: 67.4281 - val_accuracy: 0.4986\n",
      "Epoch 2/500\n",
      "150/185 [=======================>......] - ETA: 0s - loss: 80.3850 - accuracy: 0.5365\n",
      "Epoch 00002: val_loss improved from 67.42806 to 62.28162, saving model to random_split_with_min_max\\Weights-002--62.28162.hdf5\n",
      "185/185 [==============================] - 0s 2ms/step - loss: 77.3701 - accuracy: 0.5516 - val_loss: 62.2816 - val_accuracy: 0.6520\n",
      "Epoch 3/500\n",
      "166/185 [=========================>....] - ETA: 0s - loss: 67.9725 - accuracy: 0.6126\n",
      "Epoch 00003: val_loss improved from 62.28162 to 48.57145, saving model to random_split_with_min_max\\Weights-003--48.57145.hdf5\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 65.7397 - accuracy: 0.6272 - val_loss: 48.5714 - val_accuracy: 0.7307\n",
      "Epoch 4/500\n",
      "183/185 [============================>.] - ETA: 0s - loss: 64.4860 - accuracy: 0.5601\n",
      "Epoch 00004: val_loss improved from 48.57145 to 46.26229, saving model to random_split_with_min_max\\Weights-004--46.26229.hdf5\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 64.3029 - accuracy: 0.5612 - val_loss: 46.2623 - val_accuracy: 0.7015\n",
      "Epoch 5/500\n",
      "139/185 [=====================>........] - ETA: 0s - loss: 65.9193 - accuracy: 0.6866\n",
      "Epoch 00005: val_loss did not improve from 46.26229\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 65.8819 - accuracy: 0.6640 - val_loss: 56.7477 - val_accuracy: 0.7022\n",
      "Epoch 6/500\n",
      "185/185 [==============================] - ETA: 0s - loss: 57.8961 - accuracy: 0.6681\n",
      "Epoch 00006: val_loss improved from 46.26229 to 44.70992, saving model to random_split_with_min_max\\Weights-006--44.70992.hdf5\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 57.8961 - accuracy: 0.6681 - val_loss: 44.7099 - val_accuracy: 0.7720\n",
      "Epoch 7/500\n",
      "185/185 [==============================] - ETA: 0s - loss: 63.1080 - accuracy: 0.6930\n",
      "Epoch 00007: val_loss did not improve from 44.70992\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 63.1080 - accuracy: 0.6930 - val_loss: 49.9849 - val_accuracy: 0.6024\n",
      "Epoch 8/500\n",
      "174/185 [===========================>..] - ETA: 0s - loss: 50.7266 - accuracy: 0.7313\n",
      "Epoch 00008: val_loss did not improve from 44.70992\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 52.2962 - accuracy: 0.7271 - val_loss: 52.9515 - val_accuracy: 0.6391\n",
      "Epoch 9/500\n",
      "157/185 [========================>.....] - ETA: 0s - loss: 50.6926 - accuracy: 0.7315\n",
      "Epoch 00009: val_loss improved from 44.70992 to 31.88383, saving model to random_split_with_min_max\\Weights-009--31.88383.hdf5\n",
      "185/185 [==============================] - 0s 2ms/step - loss: 48.5316 - accuracy: 0.7486 - val_loss: 31.8838 - val_accuracy: 0.8243\n",
      "Epoch 10/500\n",
      "182/185 [============================>.] - ETA: 0s - loss: 45.2087 - accuracy: 0.7582\n",
      "Epoch 00010: val_loss improved from 31.88383 to 31.77993, saving model to random_split_with_min_max\\Weights-010--31.77993.hdf5\n",
      "185/185 [==============================] - 0s 2ms/step - loss: 44.9898 - accuracy: 0.7602 - val_loss: 31.7799 - val_accuracy: 0.8725\n",
      "Epoch 11/500\n",
      "180/185 [============================>.] - ETA: 0s - loss: 38.6621 - accuracy: 0.8436\n",
      "Epoch 00011: val_loss did not improve from 31.77993\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 38.9406 - accuracy: 0.8436 - val_loss: 47.0831 - val_accuracy: 0.7612\n",
      "Epoch 12/500\n",
      "183/185 [============================>.] - ETA: 0s - loss: 45.9132 - accuracy: 0.8198\n",
      "Epoch 00012: val_loss did not improve from 31.77993\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 45.8599 - accuracy: 0.8199 - val_loss: 41.0813 - val_accuracy: 0.8114\n",
      "Epoch 13/500\n",
      "140/185 [=====================>........] - ETA: 0s - loss: 45.1614 - accuracy: 0.8089\n",
      "Epoch 00013: val_loss did not improve from 31.77993\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 42.6085 - accuracy: 0.8229 - val_loss: 33.5047 - val_accuracy: 0.8324\n",
      "Epoch 14/500\n",
      "140/185 [=====================>........] - ETA: 0s - loss: 37.7845 - accuracy: 0.8598\n",
      "Epoch 00014: val_loss improved from 31.77993 to 26.32083, saving model to random_split_with_min_max\\Weights-014--26.32083.hdf5\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 35.7470 - accuracy: 0.8624 - val_loss: 26.3208 - val_accuracy: 0.8955\n",
      "Epoch 15/500\n",
      "177/185 [===========================>..] - ETA: 0s - loss: 27.8145 - accuracy: 0.8937\n",
      "Epoch 00015: val_loss did not improve from 26.32083\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 32.6128 - accuracy: 0.8911 - val_loss: 56.7764 - val_accuracy: 0.7259\n",
      "Epoch 16/500\n",
      "179/185 [============================>.] - ETA: 0s - loss: 49.0670 - accuracy: 0.7364\n",
      "Epoch 00016: val_loss did not improve from 26.32083\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 48.6542 - accuracy: 0.7407 - val_loss: 33.3360 - val_accuracy: 0.8711\n",
      "Epoch 17/500\n",
      "168/185 [==========================>...] - ETA: 0s - loss: 34.7382 - accuracy: 0.8443\n",
      "Epoch 00017: val_loss did not improve from 26.32083\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 34.8462 - accuracy: 0.8355 - val_loss: 27.7166 - val_accuracy: 0.8480\n",
      "Epoch 18/500\n",
      "143/185 [======================>.......] - ETA: 0s - loss: 41.2557 - accuracy: 0.8671\n",
      "Epoch 00018: val_loss did not improve from 26.32083\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 38.2744 - accuracy: 0.8721 - val_loss: 26.3544 - val_accuracy: 0.8887\n",
      "Epoch 19/500\n",
      "182/185 [============================>.] - ETA: 0s - loss: 30.6034 - accuracy: 0.8886\n",
      "Epoch 00019: val_loss did not improve from 26.32083\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 30.8237 - accuracy: 0.8860 - val_loss: 50.1058 - val_accuracy: 0.6228\n",
      "Epoch 20/500\n",
      "170/185 [==========================>...] - ETA: 0s - loss: 40.6421 - accuracy: 0.7730\n",
      "Epoch 00020: val_loss did not improve from 26.32083\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 41.8244 - accuracy: 0.7610 - val_loss: 50.1387 - val_accuracy: 0.6472\n",
      "Epoch 21/500\n",
      "182/185 [============================>.] - ETA: 0s - loss: 39.0810 - accuracy: 0.8189\n",
      "Epoch 00021: val_loss did not improve from 26.32083\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 39.0442 - accuracy: 0.8202 - val_loss: 30.9439 - val_accuracy: 0.8446\n",
      "Epoch 22/500\n",
      "178/185 [===========================>..] - ETA: 0s - loss: 35.6676 - accuracy: 0.8057\n",
      "Epoch 00022: val_loss improved from 26.32083 to 23.22756, saving model to random_split_with_min_max\\Weights-022--23.22756.hdf5\n",
      "185/185 [==============================] - 0s 2ms/step - loss: 35.3852 - accuracy: 0.8092 - val_loss: 23.2276 - val_accuracy: 0.8874\n",
      "Epoch 23/500\n",
      "184/185 [============================>.] - ETA: 0s - loss: 32.7955 - accuracy: 0.8760\n",
      "Epoch 00023: val_loss improved from 23.22756 to 20.64871, saving model to random_split_with_min_max\\Weights-023--20.64871.hdf5\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 32.7739 - accuracy: 0.8762 - val_loss: 20.6487 - val_accuracy: 0.9064\n",
      "Epoch 24/500\n",
      "142/185 [======================>.......] - ETA: 0s - loss: 21.6590 - accuracy: 0.9243\n",
      "Epoch 00024: val_loss did not improve from 20.64871\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 28.4458 - accuracy: 0.9111 - val_loss: 50.6075 - val_accuracy: 0.7157\n",
      "Epoch 25/500\n",
      "181/185 [============================>.] - ETA: 0s - loss: 33.7193 - accuracy: 0.8439\n",
      "Epoch 00025: val_loss did not improve from 20.64871\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 33.7149 - accuracy: 0.8406 - val_loss: 32.8477 - val_accuracy: 0.6689\n",
      "Epoch 26/500\n",
      "143/185 [======================>.......] - ETA: 0s - loss: 33.6407 - accuracy: 0.8431\n",
      "Epoch 00026: val_loss did not improve from 20.64871\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 34.0316 - accuracy: 0.8431 - val_loss: 30.7050 - val_accuracy: 0.7992\n",
      "Epoch 27/500\n",
      "140/185 [=====================>........] - ETA: 0s - loss: 38.1369 - accuracy: 0.8487\n",
      "Epoch 00027: val_loss did not improve from 20.64871\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 34.8901 - accuracy: 0.8608 - val_loss: 21.7429 - val_accuracy: 0.8982\n",
      "Epoch 28/500\n",
      "138/185 [=====================>........] - ETA: 0s - loss: 23.1722 - accuracy: 0.9151\n",
      "Epoch 00028: val_loss did not improve from 20.64871\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 29.7313 - accuracy: 0.8860 - val_loss: 42.9262 - val_accuracy: 0.7408\n",
      "Epoch 29/500\n",
      "184/185 [============================>.] - ETA: 0s - loss: 40.7420 - accuracy: 0.8198\n",
      "Epoch 00029: val_loss did not improve from 20.64871\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 40.7111 - accuracy: 0.8199 - val_loss: 22.0850 - val_accuracy: 0.9111\n",
      "Epoch 30/500\n",
      "140/185 [=====================>........] - ETA: 0s - loss: 28.4350 - accuracy: 0.8636\n",
      "Epoch 00030: val_loss did not improve from 20.64871\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 28.3577 - accuracy: 0.8684 - val_loss: 24.2205 - val_accuracy: 0.8826\n",
      "Epoch 31/500\n",
      "143/185 [======================>.......] - ETA: 0s - loss: 37.9331 - accuracy: 0.8322\n",
      "Epoch 00031: val_loss did not improve from 20.64871\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 35.5882 - accuracy: 0.8458 - val_loss: 26.3996 - val_accuracy: 0.8725\n",
      "Epoch 32/500\n",
      "141/185 [=====================>........] - ETA: 0s - loss: 33.9481 - accuracy: 0.8172\n",
      "Epoch 00032: val_loss did not improve from 20.64871\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 31.4688 - accuracy: 0.8285 - val_loss: 23.5910 - val_accuracy: 0.8847\n",
      "Epoch 33/500\n",
      "183/185 [============================>.] - ETA: 0s - loss: 29.2761 - accuracy: 0.8753\n",
      "Epoch 00033: val_loss did not improve from 20.64871\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 29.2287 - accuracy: 0.8755 - val_loss: 22.9294 - val_accuracy: 0.9016\n",
      "Epoch 34/500\n",
      "141/185 [=====================>........] - ETA: 0s - loss: 35.3118 - accuracy: 0.8382\n",
      "Epoch 00034: val_loss did not improve from 20.64871\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 32.4995 - accuracy: 0.8513 - val_loss: 24.3916 - val_accuracy: 0.8928\n",
      "Epoch 35/500\n",
      "139/185 [=====================>........] - ETA: 0s - loss: 32.0290 - accuracy: 0.8707\n",
      "Epoch 00035: val_loss improved from 20.64871 to 20.29103, saving model to random_split_with_min_max\\Weights-035--20.29103.hdf5\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 30.0055 - accuracy: 0.8828 - val_loss: 20.2910 - val_accuracy: 0.9152\n",
      "Epoch 36/500\n",
      "144/185 [======================>.......] - ETA: 0s - loss: 23.9813 - accuracy: 0.9125\n",
      "Epoch 00036: val_loss did not improve from 20.29103\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 27.9753 - accuracy: 0.8987 - val_loss: 32.8916 - val_accuracy: 0.8772\n",
      "Epoch 37/500\n",
      "182/185 [============================>.] - ETA: 0s - loss: 42.6807 - accuracy: 0.7957\n",
      "Epoch 00037: val_loss did not improve from 20.29103\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 42.4321 - accuracy: 0.7972 - val_loss: 31.8511 - val_accuracy: 0.8521\n",
      "Epoch 38/500\n",
      "141/185 [=====================>........] - ETA: 0s - loss: 27.9253 - accuracy: 0.9000\n",
      "Epoch 00038: val_loss did not improve from 20.29103\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 28.8011 - accuracy: 0.8930 - val_loss: 25.4425 - val_accuracy: 0.8908\n",
      "Epoch 39/500\n",
      "146/185 [======================>.......] - ETA: 0s - loss: 30.2596 - accuracy: 0.9041\n",
      "Epoch 00039: val_loss improved from 20.29103 to 18.13716, saving model to random_split_with_min_max\\Weights-039--18.13716.hdf5\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 27.7240 - accuracy: 0.9072 - val_loss: 18.1372 - val_accuracy: 0.9369\n",
      "Epoch 40/500\n",
      "142/185 [======================>.......] - ETA: 0s - loss: 27.9021 - accuracy: 0.8864\n",
      "Epoch 00040: val_loss did not improve from 18.13716\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 26.4751 - accuracy: 0.8926 - val_loss: 19.6586 - val_accuracy: 0.9132\n",
      "Epoch 41/500\n",
      "143/185 [======================>.......] - ETA: 0s - loss: 23.0773 - accuracy: 0.9185\n",
      "Epoch 00041: val_loss did not improve from 18.13716\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 22.7618 - accuracy: 0.9227 - val_loss: 18.8232 - val_accuracy: 0.9281\n",
      "Epoch 42/500\n",
      "183/185 [============================>.] - ETA: 0s - loss: 24.7335 - accuracy: 0.9296\n",
      "Epoch 00042: val_loss did not improve from 18.13716\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 24.7120 - accuracy: 0.9293 - val_loss: 24.4626 - val_accuracy: 0.8569\n",
      "Epoch 43/500\n",
      "144/185 [======================>.......] - ETA: 0s - loss: 22.2424 - accuracy: 0.9227\n",
      "Epoch 00043: val_loss did not improve from 18.13716\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 23.2723 - accuracy: 0.9208 - val_loss: 30.9076 - val_accuracy: 0.9186\n",
      "Epoch 44/500\n",
      "140/185 [=====================>........] - ETA: 0s - loss: 25.5719 - accuracy: 0.9049\n",
      "Epoch 00044: val_loss did not improve from 18.13716\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 24.6130 - accuracy: 0.9077 - val_loss: 34.7705 - val_accuracy: 0.8290\n",
      "Epoch 45/500\n",
      "143/185 [======================>.......] - ETA: 0s - loss: 34.9240 - accuracy: 0.8140\n",
      "Epoch 00045: val_loss did not improve from 18.13716\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 31.7701 - accuracy: 0.8382 - val_loss: 20.9777 - val_accuracy: 0.9132\n",
      "Epoch 46/500\n",
      "181/185 [============================>.] - ETA: 0s - loss: 29.6793 - accuracy: 0.8562\n",
      "Epoch 00046: val_loss did not improve from 18.13716\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 29.4597 - accuracy: 0.8569 - val_loss: 20.5482 - val_accuracy: 0.9227\n",
      "Epoch 47/500\n",
      "141/185 [=====================>........] - ETA: 0s - loss: 25.5516 - accuracy: 0.8976\n",
      "Epoch 00047: val_loss did not improve from 18.13716\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 25.1356 - accuracy: 0.9050 - val_loss: 18.8354 - val_accuracy: 0.9118\n",
      "Epoch 48/500\n",
      "180/185 [============================>.] - ETA: 0s - loss: 24.1795 - accuracy: 0.9207\n",
      "Epoch 00048: val_loss did not improve from 18.13716\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 24.0801 - accuracy: 0.9218 - val_loss: 19.0061 - val_accuracy: 0.9199\n",
      "Epoch 49/500\n",
      "183/185 [============================>.] - ETA: 0s - loss: 20.2429 - accuracy: 0.9237\n",
      "Epoch 00049: val_loss did not improve from 18.13716\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 20.2891 - accuracy: 0.9235 - val_loss: 20.7406 - val_accuracy: 0.9091\n",
      "Epoch 50/500\n",
      "179/185 [============================>.] - ETA: 0s - loss: 24.0513 - accuracy: 0.9162\n",
      "Epoch 00050: val_loss did not improve from 18.13716\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 23.9725 - accuracy: 0.9162 - val_loss: 20.2521 - val_accuracy: 0.9091\n",
      "Epoch 51/500\n",
      "168/185 [==========================>...] - ETA: 0s - loss: 20.8336 - accuracy: 0.9319\n",
      "Epoch 00051: val_loss did not improve from 18.13716\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 21.3725 - accuracy: 0.9322 - val_loss: 22.0929 - val_accuracy: 0.9179\n",
      "Epoch 52/500\n",
      "172/185 [==========================>...] - ETA: 0s - loss: 29.4258 - accuracy: 0.9092\n",
      "Epoch 00052: val_loss improved from 18.13716 to 17.90246, saving model to random_split_with_min_max\\Weights-052--17.90246.hdf5\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 28.5085 - accuracy: 0.9110 - val_loss: 17.9025 - val_accuracy: 0.9342\n",
      "Epoch 53/500\n",
      "161/185 [=========================>....] - ETA: 0s - loss: 22.8279 - accuracy: 0.9086\n",
      "Epoch 00053: val_loss did not improve from 17.90246\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 22.6080 - accuracy: 0.9135 - val_loss: 19.1010 - val_accuracy: 0.9274\n",
      "Epoch 54/500\n",
      "183/185 [============================>.] - ETA: 0s - loss: 25.4377 - accuracy: 0.8897\n",
      "Epoch 00054: val_loss did not improve from 17.90246\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 25.3911 - accuracy: 0.8898 - val_loss: 21.8679 - val_accuracy: 0.9227\n",
      "Epoch 55/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144/185 [======================>.......] - ETA: 0s - loss: 20.0438 - accuracy: 0.9338\n",
      "Epoch 00055: val_loss did not improve from 17.90246\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 21.1525 - accuracy: 0.9298 - val_loss: 32.6481 - val_accuracy: 0.7157\n",
      "Epoch 56/500\n",
      "144/185 [======================>.......] - ETA: 0s - loss: 24.8658 - accuracy: 0.8895\n",
      "Epoch 00056: val_loss did not improve from 17.90246\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 25.2012 - accuracy: 0.8984 - val_loss: 26.4275 - val_accuracy: 0.9315\n",
      "Epoch 57/500\n",
      "140/185 [=====================>........] - ETA: 0s - loss: 28.4922 - accuracy: 0.8701\n",
      "Epoch 00057: val_loss did not improve from 17.90246\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 27.4399 - accuracy: 0.8786 - val_loss: 24.8853 - val_accuracy: 0.9166\n",
      "Epoch 58/500\n",
      "142/185 [======================>.......] - ETA: 0s - loss: 28.7624 - accuracy: 0.8825\n",
      "Epoch 00058: val_loss did not improve from 17.90246\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 28.9944 - accuracy: 0.8845 - val_loss: 22.5803 - val_accuracy: 0.8976\n",
      "Epoch 59/500\n",
      "182/185 [============================>.] - ETA: 0s - loss: 21.0685 - accuracy: 0.9286\n",
      "Epoch 00059: val_loss did not improve from 17.90246\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 21.1848 - accuracy: 0.9279 - val_loss: 30.4111 - val_accuracy: 0.8548\n",
      "Epoch 60/500\n",
      "185/185 [==============================] - ETA: 0s - loss: 24.3672 - accuracy: 0.9203\n",
      "Epoch 00060: val_loss did not improve from 17.90246\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 24.3672 - accuracy: 0.9203 - val_loss: 21.5261 - val_accuracy: 0.9138\n",
      "Epoch 61/500\n",
      "145/185 [======================>.......] - ETA: 0s - loss: 20.2958 - accuracy: 0.9226\n",
      "Epoch 00061: val_loss did not improve from 17.90246\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 22.1996 - accuracy: 0.9162 - val_loss: 31.3671 - val_accuracy: 0.8602\n",
      "Epoch 62/500\n",
      "139/185 [=====================>........] - ETA: 0s - loss: 25.5761 - accuracy: 0.9123\n",
      "Epoch 00062: val_loss did not improve from 17.90246\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 28.4464 - accuracy: 0.8933 - val_loss: 26.9597 - val_accuracy: 0.8650\n",
      "Epoch 63/500\n",
      "184/185 [============================>.] - ETA: 0s - loss: 29.0104 - accuracy: 0.8874\n",
      "Epoch 00063: val_loss did not improve from 17.90246\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 28.9827 - accuracy: 0.8876 - val_loss: 27.5763 - val_accuracy: 0.8915\n",
      "Epoch 64/500\n",
      "140/185 [=====================>........] - ETA: 0s - loss: 25.3516 - accuracy: 0.9270\n",
      "Epoch 00064: val_loss did not improve from 17.90246\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 24.6812 - accuracy: 0.9223 - val_loss: 19.5961 - val_accuracy: 0.9342\n",
      "Epoch 65/500\n",
      "143/185 [======================>.......] - ETA: 0s - loss: 18.5702 - accuracy: 0.9360\n",
      "Epoch 00065: val_loss did not improve from 17.90246\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 19.5421 - accuracy: 0.9344 - val_loss: 25.2454 - val_accuracy: 0.9152\n",
      "Epoch 66/500\n",
      "138/185 [=====================>........] - ETA: 0s - loss: 21.8286 - accuracy: 0.9366\n",
      "Epoch 00066: val_loss did not improve from 17.90246\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 22.2485 - accuracy: 0.9350 - val_loss: 26.9034 - val_accuracy: 0.8826\n",
      "Epoch 67/500\n",
      "183/185 [============================>.] - ETA: 0s - loss: 24.9290 - accuracy: 0.9274\n",
      "Epoch 00067: val_loss did not improve from 17.90246\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 24.8540 - accuracy: 0.9274 - val_loss: 18.2561 - val_accuracy: 0.9104\n",
      "Epoch 68/500\n",
      "181/185 [============================>.] - ETA: 0s - loss: 30.0329 - accuracy: 0.8403\n",
      "Epoch 00068: val_loss did not improve from 17.90246\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 29.7694 - accuracy: 0.8416 - val_loss: 20.3485 - val_accuracy: 0.8962\n",
      "Epoch 69/500\n",
      "166/185 [=========================>....] - ETA: 0s - loss: 34.7858 - accuracy: 0.7878\n",
      "Epoch 00069: val_loss did not improve from 17.90246\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 33.2299 - accuracy: 0.8002 - val_loss: 21.5536 - val_accuracy: 0.9125\n",
      "Epoch 70/500\n",
      "180/185 [============================>.] - ETA: 0s - loss: 35.6889 - accuracy: 0.8467\n",
      "Epoch 00070: val_loss did not improve from 17.90246\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 35.5069 - accuracy: 0.8477 - val_loss: 25.4019 - val_accuracy: 0.8691\n",
      "Epoch 71/500\n",
      "143/185 [======================>.......] - ETA: 0s - loss: 27.5282 - accuracy: 0.9065\n",
      "Epoch 00071: val_loss did not improve from 17.90246\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 26.9117 - accuracy: 0.9082 - val_loss: 23.9477 - val_accuracy: 0.9023\n",
      "Epoch 72/500\n",
      "181/185 [============================>.] - ETA: 0s - loss: 23.5702 - accuracy: 0.9251\n",
      "Epoch 00072: val_loss did not improve from 17.90246\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 23.4406 - accuracy: 0.9257 - val_loss: 23.6299 - val_accuracy: 0.9322\n",
      "Epoch 73/500\n",
      "143/185 [======================>.......] - ETA: 0s - loss: 23.0433 - accuracy: 0.9111\n",
      "Epoch 00073: val_loss did not improve from 17.90246\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 22.5049 - accuracy: 0.9103 - val_loss: 18.4165 - val_accuracy: 0.8935\n",
      "Epoch 74/500\n",
      "168/185 [==========================>...] - ETA: 0s - loss: 26.2259 - accuracy: 0.9146\n",
      "Epoch 00074: val_loss did not improve from 17.90246\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 26.1376 - accuracy: 0.9110 - val_loss: 30.9301 - val_accuracy: 0.8976\n",
      "Epoch 75/500\n",
      "140/185 [=====================>........] - ETA: 0s - loss: 26.9013 - accuracy: 0.9163\n",
      "Epoch 00075: val_loss did not improve from 17.90246\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 25.8922 - accuracy: 0.9152 - val_loss: 22.5591 - val_accuracy: 0.9254\n",
      "Epoch 76/500\n",
      "177/185 [===========================>..] - ETA: 0s - loss: 26.0279 - accuracy: 0.8932\n",
      "Epoch 00076: val_loss did not improve from 17.90246\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 25.6493 - accuracy: 0.8954 - val_loss: 18.6368 - val_accuracy: 0.9308\n",
      "Epoch 77/500\n",
      "142/185 [======================>.......] - ETA: 0s - loss: 23.7729 - accuracy: 0.9021\n",
      "Epoch 00077: val_loss improved from 17.90246 to 17.60207, saving model to random_split_with_min_max\\Weights-077--17.60207.hdf5\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 22.7271 - accuracy: 0.9108 - val_loss: 17.6021 - val_accuracy: 0.9294\n",
      "Epoch 78/500\n",
      "142/185 [======================>.......] - ETA: 0s - loss: 16.7380 - accuracy: 0.9467\n",
      "Epoch 00078: val_loss did not improve from 17.60207\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 20.1792 - accuracy: 0.9395 - val_loss: 25.8544 - val_accuracy: 0.9152\n",
      "Epoch 79/500\n",
      "142/185 [======================>.......] - ETA: 0s - loss: 19.4780 - accuracy: 0.9316\n",
      "Epoch 00079: val_loss did not improve from 17.60207\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 20.5975 - accuracy: 0.9323 - val_loss: 19.8621 - val_accuracy: 0.8813\n",
      "Epoch 80/500\n",
      "173/185 [===========================>..] - ETA: 0s - loss: 34.1464 - accuracy: 0.8963\n",
      "Epoch 00080: val_loss did not improve from 17.60207\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 33.1010 - accuracy: 0.8987 - val_loss: 19.4900 - val_accuracy: 0.9267\n",
      "Epoch 81/500\n",
      "184/185 [============================>.] - ETA: 0s - loss: 25.8308 - accuracy: 0.9339\n",
      "Epoch 00081: val_loss did not improve from 17.60207\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 25.9203 - accuracy: 0.9339 - val_loss: 29.0355 - val_accuracy: 0.8786\n",
      "Epoch 82/500\n",
      "144/185 [======================>.......] - ETA: 0s - loss: 26.1830 - accuracy: 0.8876\n",
      "Epoch 00082: val_loss did not improve from 17.60207\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 25.2444 - accuracy: 0.8950 - val_loss: 20.4879 - val_accuracy: 0.9294\n",
      "Epoch 83/500\n",
      "185/185 [==============================] - ETA: 0s - loss: 25.9897 - accuracy: 0.8959\n",
      "Epoch 00083: val_loss did not improve from 17.60207\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 25.9897 - accuracy: 0.8959 - val_loss: 26.0639 - val_accuracy: 0.8480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      "175/185 [===========================>..] - ETA: 0s - loss: 26.2296 - accuracy: 0.9132\n",
      "Epoch 00084: val_loss did not improve from 17.60207\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 25.8228 - accuracy: 0.9140 - val_loss: 18.3707 - val_accuracy: 0.9322\n",
      "Epoch 85/500\n",
      "142/185 [======================>.......] - ETA: 0s - loss: 24.5737 - accuracy: 0.8827\n",
      "Epoch 00085: val_loss improved from 17.60207 to 17.46126, saving model to random_split_with_min_max\\Weights-085--17.46126.hdf5\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 22.9161 - accuracy: 0.8962 - val_loss: 17.4613 - val_accuracy: 0.9355\n",
      "Epoch 86/500\n",
      "183/185 [============================>.] - ETA: 0s - loss: 23.8267 - accuracy: 0.9107\n",
      "Epoch 00086: val_loss did not improve from 17.46126\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 23.7909 - accuracy: 0.9108 - val_loss: 18.0033 - val_accuracy: 0.9288\n",
      "Epoch 87/500\n",
      "184/185 [============================>.] - ETA: 0s - loss: 30.9360 - accuracy: 0.8584\n",
      "Epoch 00087: val_loss did not improve from 17.46126\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 30.9084 - accuracy: 0.8582 - val_loss: 18.2953 - val_accuracy: 0.9213\n",
      "Epoch 88/500\n",
      "142/185 [======================>.......] - ETA: 0s - loss: 27.4010 - accuracy: 0.8882\n",
      "Epoch 00088: val_loss did not improve from 17.46126\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 25.8972 - accuracy: 0.8940 - val_loss: 20.7690 - val_accuracy: 0.9193\n",
      "Epoch 89/500\n",
      "184/185 [============================>.] - ETA: 0s - loss: 20.6781 - accuracy: 0.9229\n",
      "Epoch 00089: val_loss did not improve from 17.46126\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 20.6891 - accuracy: 0.9227 - val_loss: 29.1985 - val_accuracy: 0.7978\n",
      "Epoch 90/500\n",
      "142/185 [======================>.......] - ETA: 0s - loss: 19.9148 - accuracy: 0.9236\n",
      "Epoch 00090: val_loss did not improve from 17.46126\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 20.8137 - accuracy: 0.9255 - val_loss: 23.2156 - val_accuracy: 0.8731\n",
      "Epoch 91/500\n",
      "178/185 [===========================>..] - ETA: 0s - loss: 22.9375 - accuracy: 0.9071\n",
      "Epoch 00091: val_loss did not improve from 17.46126\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 22.7401 - accuracy: 0.9093 - val_loss: 26.6028 - val_accuracy: 0.9315\n",
      "Epoch 92/500\n",
      "182/185 [============================>.] - ETA: 0s - loss: 22.4393 - accuracy: 0.9038\n",
      "Epoch 00092: val_loss improved from 17.46126 to 15.64895, saving model to random_split_with_min_max\\Weights-092--15.64895.hdf5\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 22.3333 - accuracy: 0.9042 - val_loss: 15.6489 - val_accuracy: 0.9328\n",
      "Epoch 93/500\n",
      "143/185 [======================>.......] - ETA: 0s - loss: 16.6050 - accuracy: 0.9419\n",
      "Epoch 00093: val_loss did not improve from 15.64895\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 17.4291 - accuracy: 0.9400 - val_loss: 17.0109 - val_accuracy: 0.9301\n",
      "Epoch 94/500\n",
      "143/185 [======================>.......] - ETA: 0s - loss: 20.0457 - accuracy: 0.9445\n",
      "Epoch 00094: val_loss did not improve from 15.64895\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 20.2716 - accuracy: 0.9376 - val_loss: 18.9920 - val_accuracy: 0.9064\n",
      "Epoch 95/500\n",
      "185/185 [==============================] - ETA: 0s - loss: 28.2802 - accuracy: 0.8451\n",
      "Epoch 00095: val_loss did not improve from 15.64895\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 28.2802 - accuracy: 0.8451 - val_loss: 19.1513 - val_accuracy: 0.9050\n",
      "Epoch 96/500\n",
      "183/185 [============================>.] - ETA: 0s - loss: 23.2334 - accuracy: 0.8912\n",
      "Epoch 00096: val_loss did not improve from 15.64895\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 23.3219 - accuracy: 0.8913 - val_loss: 21.2619 - val_accuracy: 0.9118\n",
      "Epoch 97/500\n",
      "137/185 [=====================>........] - ETA: 0s - loss: 29.4247 - accuracy: 0.8896\n",
      "Epoch 00097: val_loss did not improve from 15.64895\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 29.7277 - accuracy: 0.8894 - val_loss: 38.5870 - val_accuracy: 0.8989\n",
      "Epoch 98/500\n",
      "158/185 [========================>.....] - ETA: 0s - loss: 26.7120 - accuracy: 0.9104\n",
      "Epoch 00098: val_loss did not improve from 15.64895\n",
      "185/185 [==============================] - 0s 2ms/step - loss: 25.5693 - accuracy: 0.9118 - val_loss: 17.5703 - val_accuracy: 0.9050\n",
      "Epoch 99/500\n",
      "169/185 [==========================>...] - ETA: 0s - loss: 21.1246 - accuracy: 0.9173\n",
      "Epoch 00099: val_loss did not improve from 15.64895\n",
      "185/185 [==============================] - 0s 2ms/step - loss: 21.0780 - accuracy: 0.9176 - val_loss: 21.6658 - val_accuracy: 0.9199\n",
      "Epoch 100/500\n",
      "168/185 [==========================>...] - ETA: 0s - loss: 30.3455 - accuracy: 0.8408\n",
      "Epoch 00100: val_loss did not improve from 15.64895\n",
      "185/185 [==============================] - 0s 2ms/step - loss: 29.9230 - accuracy: 0.8446 - val_loss: 21.3402 - val_accuracy: 0.8555\n",
      "Epoch 101/500\n",
      "158/185 [========================>.....] - ETA: 0s - loss: 34.4985 - accuracy: 0.8519\n",
      "Epoch 00101: val_loss did not improve from 15.64895\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 32.9744 - accuracy: 0.8575 - val_loss: 22.5297 - val_accuracy: 0.9125\n",
      "Epoch 102/500\n",
      "166/185 [=========================>....] - ETA: 0s - loss: 20.2867 - accuracy: 0.9155\n",
      "Epoch 00102: val_loss did not improve from 15.64895\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 20.0225 - accuracy: 0.9189 - val_loss: 23.7100 - val_accuracy: 0.9281\n",
      "Epoch 103/500\n",
      "164/185 [=========================>....] - ETA: 0s - loss: 25.5735 - accuracy: 0.9106\n",
      "Epoch 00103: val_loss improved from 15.64895 to 14.70277, saving model to random_split_with_min_max\\Weights-103--14.70277.hdf5\n",
      "185/185 [==============================] - 0s 2ms/step - loss: 24.8225 - accuracy: 0.9143 - val_loss: 14.7028 - val_accuracy: 0.9396\n",
      "Epoch 104/500\n",
      "140/185 [=====================>........] - ETA: 0s - loss: 17.6136 - accuracy: 0.9324\n",
      "Epoch 00104: val_loss did not improve from 14.70277\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 17.6394 - accuracy: 0.9335 - val_loss: 18.1001 - val_accuracy: 0.9342\n",
      "Epoch 105/500\n",
      "177/185 [===========================>..] - ETA: 0s - loss: 16.2477 - accuracy: 0.9428\n",
      "Epoch 00105: val_loss did not improve from 14.70277\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 16.1909 - accuracy: 0.9417 - val_loss: 16.3503 - val_accuracy: 0.9247\n",
      "Epoch 106/500\n",
      "178/185 [===========================>..] - ETA: 0s - loss: 18.8921 - accuracy: 0.9321\n",
      "Epoch 00106: val_loss did not improve from 14.70277\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 18.7908 - accuracy: 0.9315 - val_loss: 20.1855 - val_accuracy: 0.9240\n",
      "Epoch 107/500\n",
      "180/185 [============================>.] - ETA: 0s - loss: 17.4153 - accuracy: 0.9424\n",
      "Epoch 00107: val_loss did not improve from 14.70277\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 17.3653 - accuracy: 0.9422 - val_loss: 21.1530 - val_accuracy: 0.9213\n",
      "Epoch 108/500\n",
      "179/185 [============================>.] - ETA: 0s - loss: 17.4781 - accuracy: 0.9415\n",
      "Epoch 00108: val_loss did not improve from 14.70277\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 17.5755 - accuracy: 0.9411 - val_loss: 22.0669 - val_accuracy: 0.9152\n",
      "Epoch 109/500\n",
      "140/185 [=====================>........] - ETA: 0s - loss: 16.6797 - accuracy: 0.9402\n",
      "Epoch 00109: val_loss improved from 14.70277 to 14.01913, saving model to random_split_with_min_max\\Weights-109--14.01913.hdf5\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 16.0349 - accuracy: 0.9435 - val_loss: 14.0191 - val_accuracy: 0.9444\n",
      "Epoch 110/500\n",
      "155/185 [========================>.....] - ETA: 0s - loss: 16.7281 - accuracy: 0.9492\n",
      "Epoch 00110: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 16.7183 - accuracy: 0.9501 - val_loss: 17.6531 - val_accuracy: 0.9512\n",
      "Epoch 111/500\n",
      "177/185 [===========================>..] - ETA: 0s - loss: 17.0729 - accuracy: 0.9453\n",
      "Epoch 00111: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 17.3565 - accuracy: 0.9444 - val_loss: 35.4616 - val_accuracy: 0.8894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/500\n",
      "175/185 [===========================>..] - ETA: 0s - loss: 21.1594 - accuracy: 0.9257\n",
      "Epoch 00112: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 21.4013 - accuracy: 0.9259 - val_loss: 20.8683 - val_accuracy: 0.9288\n",
      "Epoch 113/500\n",
      "183/185 [============================>.] - ETA: 0s - loss: 19.8446 - accuracy: 0.9307\n",
      "Epoch 00113: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 19.7777 - accuracy: 0.9311 - val_loss: 14.5558 - val_accuracy: 0.9410\n",
      "Epoch 114/500\n",
      "185/185 [==============================] - ETA: 0s - loss: 20.9671 - accuracy: 0.9099\n",
      "Epoch 00114: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 20.9671 - accuracy: 0.9099 - val_loss: 17.7859 - val_accuracy: 0.9471\n",
      "Epoch 115/500\n",
      "185/185 [==============================] - ETA: 0s - loss: 21.0319 - accuracy: 0.9325\n",
      "Epoch 00115: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 21.0319 - accuracy: 0.9325 - val_loss: 22.0142 - val_accuracy: 0.9233\n",
      "Epoch 116/500\n",
      "177/185 [===========================>..] - ETA: 0s - loss: 22.2432 - accuracy: 0.9285\n",
      "Epoch 00116: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 21.9776 - accuracy: 0.9296 - val_loss: 17.8078 - val_accuracy: 0.9417\n",
      "Epoch 117/500\n",
      "143/185 [======================>.......] - ETA: 0s - loss: 19.3771 - accuracy: 0.9421\n",
      "Epoch 00117: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 21.4941 - accuracy: 0.9322 - val_loss: 24.2252 - val_accuracy: 0.9186\n",
      "Epoch 118/500\n",
      "169/185 [==========================>...] - ETA: 0s - loss: 21.1337 - accuracy: 0.9308\n",
      "Epoch 00118: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 21.5469 - accuracy: 0.9293 - val_loss: 21.2455 - val_accuracy: 0.9206\n",
      "Epoch 119/500\n",
      "144/185 [======================>.......] - ETA: 0s - loss: 21.2102 - accuracy: 0.9210\n",
      "Epoch 00119: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 20.5434 - accuracy: 0.9181 - val_loss: 16.7615 - val_accuracy: 0.9383\n",
      "Epoch 120/500\n",
      "180/185 [============================>.] - ETA: 0s - loss: 17.0768 - accuracy: 0.9451\n",
      "Epoch 00120: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 17.1213 - accuracy: 0.9454 - val_loss: 17.0637 - val_accuracy: 0.9349\n",
      "Epoch 121/500\n",
      "142/185 [======================>.......] - ETA: 0s - loss: 16.0390 - accuracy: 0.9408\n",
      "Epoch 00121: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 16.2357 - accuracy: 0.9398 - val_loss: 15.4119 - val_accuracy: 0.9430\n",
      "Epoch 122/500\n",
      "142/185 [======================>.......] - ETA: 0s - loss: 18.0555 - accuracy: 0.9393\n",
      "Epoch 00122: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 17.2338 - accuracy: 0.9427 - val_loss: 17.4421 - val_accuracy: 0.9478\n",
      "Epoch 123/500\n",
      "183/185 [============================>.] - ETA: 0s - loss: 17.2727 - accuracy: 0.9387\n",
      "Epoch 00123: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 17.2344 - accuracy: 0.9391 - val_loss: 15.1758 - val_accuracy: 0.9417\n",
      "Epoch 124/500\n",
      "180/185 [============================>.] - ETA: 0s - loss: 15.3284 - accuracy: 0.9486\n",
      "Epoch 00124: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 15.3427 - accuracy: 0.9481 - val_loss: 30.8412 - val_accuracy: 0.9152\n",
      "Epoch 125/500\n",
      "161/185 [=========================>....] - ETA: 0s - loss: 20.4628 - accuracy: 0.9196\n",
      "Epoch 00125: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 19.8220 - accuracy: 0.9222 - val_loss: 17.5292 - val_accuracy: 0.9512\n",
      "Epoch 126/500\n",
      "172/185 [==========================>...] - ETA: 0s - loss: 16.6817 - accuracy: 0.9471\n",
      "Epoch 00126: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 16.4623 - accuracy: 0.9476 - val_loss: 18.6585 - val_accuracy: 0.9491\n",
      "Epoch 127/500\n",
      "184/185 [============================>.] - ETA: 0s - loss: 17.7074 - accuracy: 0.9312\n",
      "Epoch 00127: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 17.6949 - accuracy: 0.9311 - val_loss: 17.1877 - val_accuracy: 0.9512\n",
      "Epoch 128/500\n",
      "141/185 [=====================>........] - ETA: 0s - loss: 17.5583 - accuracy: 0.9426\n",
      "Epoch 00128: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 19.7574 - accuracy: 0.9355 - val_loss: 29.1765 - val_accuracy: 0.9362\n",
      "Epoch 129/500\n",
      "180/185 [============================>.] - ETA: 0s - loss: 19.6661 - accuracy: 0.9352\n",
      "Epoch 00129: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 19.5693 - accuracy: 0.9350 - val_loss: 19.8696 - val_accuracy: 0.9172\n",
      "Epoch 130/500\n",
      "182/185 [============================>.] - ETA: 0s - loss: 18.9225 - accuracy: 0.9291\n",
      "Epoch 00130: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 18.8259 - accuracy: 0.9293 - val_loss: 16.8080 - val_accuracy: 0.9417\n",
      "Epoch 131/500\n",
      "141/185 [=====================>........] - ETA: 0s - loss: 15.6973 - accuracy: 0.9450\n",
      "Epoch 00131: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 15.3565 - accuracy: 0.9444 - val_loss: 15.5315 - val_accuracy: 0.9518\n",
      "Epoch 132/500\n",
      "181/185 [============================>.] - ETA: 0s - loss: 26.7780 - accuracy: 0.8602\n",
      "Epoch 00132: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 26.6344 - accuracy: 0.8594 - val_loss: 23.1079 - val_accuracy: 0.8053\n",
      "Epoch 133/500\n",
      "179/185 [============================>.] - ETA: 0s - loss: 19.6628 - accuracy: 0.9279\n",
      "Epoch 00133: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 19.6782 - accuracy: 0.9271 - val_loss: 21.6608 - val_accuracy: 0.8202\n",
      "Epoch 134/500\n",
      "183/185 [============================>.] - ETA: 0s - loss: 23.9542 - accuracy: 0.9146\n",
      "Epoch 00134: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 23.8858 - accuracy: 0.9150 - val_loss: 30.1916 - val_accuracy: 0.9172\n",
      "Epoch 135/500\n",
      "183/185 [============================>.] - ETA: 0s - loss: 23.3657 - accuracy: 0.9133\n",
      "Epoch 00135: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 23.3619 - accuracy: 0.9135 - val_loss: 19.1604 - val_accuracy: 0.9125\n",
      "Epoch 136/500\n",
      "180/185 [============================>.] - ETA: 0s - loss: 22.6429 - accuracy: 0.9247\n",
      "Epoch 00136: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 22.5553 - accuracy: 0.9250 - val_loss: 22.8396 - val_accuracy: 0.9322\n",
      "Epoch 137/500\n",
      "184/185 [============================>.] - ETA: 0s - loss: 20.4613 - accuracy: 0.9243\n",
      "Epoch 00137: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 20.4622 - accuracy: 0.9244 - val_loss: 14.8589 - val_accuracy: 0.9437\n",
      "Epoch 138/500\n",
      "142/185 [======================>.......] - ETA: 0s - loss: 16.2208 - accuracy: 0.9368\n",
      "Epoch 00138: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 16.6774 - accuracy: 0.9371 - val_loss: 16.0696 - val_accuracy: 0.9349\n",
      "Epoch 139/500\n",
      "182/185 [============================>.] - ETA: 0s - loss: 18.3550 - accuracy: 0.9360\n",
      "Epoch 00139: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 18.4177 - accuracy: 0.9364 - val_loss: 18.0443 - val_accuracy: 0.9410\n",
      "Epoch 140/500\n",
      "142/185 [======================>.......] - ETA: 0s - loss: 22.4873 - accuracy: 0.9040\n",
      "Epoch 00140: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 21.5477 - accuracy: 0.9132 - val_loss: 21.0004 - val_accuracy: 0.9450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141/500\n",
      "164/185 [=========================>....] - ETA: 0s - loss: 24.2645 - accuracy: 0.9167\n",
      "Epoch 00141: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 24.3693 - accuracy: 0.9035 - val_loss: 24.4855 - val_accuracy: 0.8948\n",
      "Epoch 142/500\n",
      "143/185 [======================>.......] - ETA: 0s - loss: 22.4329 - accuracy: 0.8899\n",
      "Epoch 00142: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 22.6498 - accuracy: 0.8976 - val_loss: 19.4106 - val_accuracy: 0.9071\n",
      "Epoch 143/500\n",
      "141/185 [=====================>........] - ETA: 0s - loss: 20.3623 - accuracy: 0.9138\n",
      "Epoch 00143: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 19.7685 - accuracy: 0.9198 - val_loss: 17.1712 - val_accuracy: 0.9491\n",
      "Epoch 144/500\n",
      "185/185 [==============================] - ETA: 0s - loss: 24.4752 - accuracy: 0.9162\n",
      "Epoch 00144: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 24.4752 - accuracy: 0.9162 - val_loss: 23.7246 - val_accuracy: 0.8996\n",
      "Epoch 145/500\n",
      "182/185 [============================>.] - ETA: 0s - loss: 23.9739 - accuracy: 0.9207\n",
      "Epoch 00145: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 23.9955 - accuracy: 0.9188 - val_loss: 27.7126 - val_accuracy: 0.7768\n",
      "Epoch 146/500\n",
      "141/185 [=====================>........] - ETA: 0s - loss: 20.1112 - accuracy: 0.9195\n",
      "Epoch 00146: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 22.7233 - accuracy: 0.9079 - val_loss: 28.9293 - val_accuracy: 0.7720\n",
      "Epoch 147/500\n",
      "173/185 [===========================>..] - ETA: 0s - loss: 25.4019 - accuracy: 0.9129\n",
      "Epoch 00147: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 25.2591 - accuracy: 0.9133 - val_loss: 20.9694 - val_accuracy: 0.9254\n",
      "Epoch 148/500\n",
      "139/185 [=====================>........] - ETA: 0s - loss: 19.9241 - accuracy: 0.9310\n",
      "Epoch 00148: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 18.8700 - accuracy: 0.9362 - val_loss: 16.6808 - val_accuracy: 0.9450\n",
      "Epoch 149/500\n",
      "174/185 [===========================>..] - ETA: 0s - loss: 21.4034 - accuracy: 0.9303\n",
      "Epoch 00149: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 21.3499 - accuracy: 0.9294 - val_loss: 18.7166 - val_accuracy: 0.9369\n",
      "Epoch 150/500\n",
      "185/185 [==============================] - ETA: 0s - loss: 20.7642 - accuracy: 0.9315\n",
      "Epoch 00150: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 20.7642 - accuracy: 0.9315 - val_loss: 22.7192 - val_accuracy: 0.7442\n",
      "Epoch 151/500\n",
      "185/185 [==============================] - ETA: 0s - loss: 17.0577 - accuracy: 0.9337\n",
      "Epoch 00151: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 17.0577 - accuracy: 0.9337 - val_loss: 16.2719 - val_accuracy: 0.9518\n",
      "Epoch 152/500\n",
      "182/185 [============================>.] - ETA: 0s - loss: 18.9653 - accuracy: 0.9368\n",
      "Epoch 00152: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 18.8925 - accuracy: 0.9367 - val_loss: 17.5957 - val_accuracy: 0.9376\n",
      "Epoch 153/500\n",
      "140/185 [=====================>........] - ETA: 0s - loss: 17.2165 - accuracy: 0.9395\n",
      "Epoch 00153: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 16.6176 - accuracy: 0.9418 - val_loss: 16.4837 - val_accuracy: 0.9478\n",
      "Epoch 154/500\n",
      "179/185 [============================>.] - ETA: 0s - loss: 16.1977 - accuracy: 0.9370\n",
      "Epoch 00154: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 16.1713 - accuracy: 0.9372 - val_loss: 18.0600 - val_accuracy: 0.9478\n",
      "Epoch 155/500\n",
      "185/185 [==============================] - ETA: 0s - loss: 24.3607 - accuracy: 0.9054\n",
      "Epoch 00155: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 24.3607 - accuracy: 0.9054 - val_loss: 39.7168 - val_accuracy: 0.6893\n",
      "Epoch 156/500\n",
      "143/185 [======================>.......] - ETA: 0s - loss: 39.5339 - accuracy: 0.8031\n",
      "Epoch 00156: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 35.2797 - accuracy: 0.8301 - val_loss: 18.1532 - val_accuracy: 0.9383\n",
      "Epoch 157/500\n",
      "174/185 [===========================>..] - ETA: 0s - loss: 22.8691 - accuracy: 0.9237\n",
      "Epoch 00157: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 22.5863 - accuracy: 0.9240 - val_loss: 17.4616 - val_accuracy: 0.9430\n",
      "Epoch 158/500\n",
      "183/185 [============================>.] - ETA: 0s - loss: 21.0230 - accuracy: 0.9305\n",
      "Epoch 00158: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 21.0073 - accuracy: 0.9305 - val_loss: 17.2549 - val_accuracy: 0.9335\n",
      "Epoch 159/500\n",
      "139/185 [=====================>........] - ETA: 0s - loss: 25.0151 - accuracy: 0.8946\n",
      "Epoch 00159: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 25.3140 - accuracy: 0.8842 - val_loss: 21.8132 - val_accuracy: 0.9233\n",
      "Epoch 160/500\n",
      "180/185 [============================>.] - ETA: 0s - loss: 20.9914 - accuracy: 0.9260\n",
      "Epoch 00160: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 21.1841 - accuracy: 0.9244 - val_loss: 33.6545 - val_accuracy: 0.8304\n",
      "Epoch 161/500\n",
      "182/185 [============================>.] - ETA: 0s - loss: 33.8968 - accuracy: 0.9001\n",
      "Epoch 00161: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 33.7373 - accuracy: 0.8999 - val_loss: 15.7081 - val_accuracy: 0.9383\n",
      "Epoch 162/500\n",
      "178/185 [===========================>..] - ETA: 0s - loss: 16.9875 - accuracy: 0.9421\n",
      "Epoch 00162: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 17.2618 - accuracy: 0.9410 - val_loss: 19.9102 - val_accuracy: 0.9193\n",
      "Epoch 163/500\n",
      "143/185 [======================>.......] - ETA: 0s - loss: 26.8731 - accuracy: 0.9205\n",
      "Epoch 00163: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 25.6592 - accuracy: 0.9232 - val_loss: 20.4018 - val_accuracy: 0.9254\n",
      "Epoch 164/500\n",
      "179/185 [============================>.] - ETA: 0s - loss: 24.7812 - accuracy: 0.9242\n",
      "Epoch 00164: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 24.5078 - accuracy: 0.9245 - val_loss: 19.7836 - val_accuracy: 0.9098\n",
      "Epoch 165/500\n",
      "177/185 [===========================>..] - ETA: 0s - loss: 19.3045 - accuracy: 0.9379\n",
      "Epoch 00165: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 19.1494 - accuracy: 0.9383 - val_loss: 16.1827 - val_accuracy: 0.9383\n",
      "Epoch 166/500\n",
      "169/185 [==========================>...] - ETA: 0s - loss: 18.3096 - accuracy: 0.9220\n",
      "Epoch 00166: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 17.9231 - accuracy: 0.9245 - val_loss: 14.3886 - val_accuracy: 0.9498\n",
      "Epoch 167/500\n",
      "140/185 [=====================>........] - ETA: 0s - loss: 17.7236 - accuracy: 0.9444\n",
      "Epoch 00167: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 16.5636 - accuracy: 0.9447 - val_loss: 15.1607 - val_accuracy: 0.9518\n",
      "Epoch 168/500\n",
      "180/185 [============================>.] - ETA: 0s - loss: 14.6074 - accuracy: 0.9413\n",
      "Epoch 00168: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 18.1512 - accuracy: 0.9408 - val_loss: 19.3777 - val_accuracy: 0.9315\n",
      "Epoch 169/500\n",
      "181/185 [============================>.] - ETA: 0s - loss: 21.1699 - accuracy: 0.9178\n",
      "Epoch 00169: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 21.0233 - accuracy: 0.9184 - val_loss: 15.4782 - val_accuracy: 0.9457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/500\n",
      "180/185 [============================>.] - ETA: 0s - loss: 18.7100 - accuracy: 0.9340\n",
      "Epoch 00170: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 18.5786 - accuracy: 0.9349 - val_loss: 16.5129 - val_accuracy: 0.9491\n",
      "Epoch 171/500\n",
      "180/185 [============================>.] - ETA: 0s - loss: 17.6258 - accuracy: 0.9309\n",
      "Epoch 00171: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 17.5806 - accuracy: 0.9308 - val_loss: 19.8899 - val_accuracy: 0.9037\n",
      "Epoch 172/500\n",
      "182/185 [============================>.] - ETA: 0s - loss: 24.5173 - accuracy: 0.9188\n",
      "Epoch 00172: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 24.5670 - accuracy: 0.9181 - val_loss: 18.8056 - val_accuracy: 0.8989\n",
      "Epoch 173/500\n",
      "182/185 [============================>.] - ETA: 0s - loss: 18.8406 - accuracy: 0.9368\n",
      "Epoch 00173: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 18.8638 - accuracy: 0.9367 - val_loss: 15.9759 - val_accuracy: 0.9396\n",
      "Epoch 174/500\n",
      "178/185 [===========================>..] - ETA: 0s - loss: 18.4086 - accuracy: 0.9366\n",
      "Epoch 00174: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 18.2371 - accuracy: 0.9371 - val_loss: 15.8841 - val_accuracy: 0.9376\n",
      "Epoch 175/500\n",
      "139/185 [=====================>........] - ETA: 0s - loss: 15.1469 - accuracy: 0.9503\n",
      "Epoch 00175: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 16.1745 - accuracy: 0.9467 - val_loss: 18.0476 - val_accuracy: 0.9349\n",
      "Epoch 176/500\n",
      "182/185 [============================>.] - ETA: 0s - loss: 22.0119 - accuracy: 0.9493\n",
      "Epoch 00176: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 21.8961 - accuracy: 0.9491 - val_loss: 14.4587 - val_accuracy: 0.9539\n",
      "Epoch 177/500\n",
      "184/185 [============================>.] - ETA: 0s - loss: 16.6672 - accuracy: 0.9465\n",
      "Epoch 00177: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 16.6651 - accuracy: 0.9466 - val_loss: 15.5360 - val_accuracy: 0.9396\n",
      "Epoch 178/500\n",
      "139/185 [=====================>........] - ETA: 0s - loss: 16.3519 - accuracy: 0.9465\n",
      "Epoch 00178: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 15.4809 - accuracy: 0.9498 - val_loss: 17.4204 - val_accuracy: 0.9525\n",
      "Epoch 179/500\n",
      "178/185 [===========================>..] - ETA: 0s - loss: 15.2040 - accuracy: 0.9528\n",
      "Epoch 00179: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 15.1394 - accuracy: 0.9527 - val_loss: 17.4047 - val_accuracy: 0.9532\n",
      "Epoch 180/500\n",
      "141/185 [=====================>........] - ETA: 0s - loss: 16.6847 - accuracy: 0.9479\n",
      "Epoch 00180: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 15.9558 - accuracy: 0.9495 - val_loss: 14.6916 - val_accuracy: 0.9505\n",
      "Epoch 181/500\n",
      "180/185 [============================>.] - ETA: 0s - loss: 14.9189 - accuracy: 0.9536\n",
      "Epoch 00181: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 14.8239 - accuracy: 0.9534 - val_loss: 14.4937 - val_accuracy: 0.9525\n",
      "Epoch 182/500\n",
      "185/185 [==============================] - ETA: 0s - loss: 15.7080 - accuracy: 0.9481\n",
      "Epoch 00182: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 15.7080 - accuracy: 0.9481 - val_loss: 16.7471 - val_accuracy: 0.9355\n",
      "Epoch 183/500\n",
      "177/185 [===========================>..] - ETA: 0s - loss: 16.1891 - accuracy: 0.9507\n",
      "Epoch 00183: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 16.3144 - accuracy: 0.9491 - val_loss: 19.5782 - val_accuracy: 0.9261\n",
      "Epoch 184/500\n",
      "184/185 [============================>.] - ETA: 0s - loss: 19.8167 - accuracy: 0.9373\n",
      "Epoch 00184: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 19.8156 - accuracy: 0.9371 - val_loss: 26.4286 - val_accuracy: 0.8596\n",
      "Epoch 185/500\n",
      "183/185 [============================>.] - ETA: 0s - loss: 24.0772 - accuracy: 0.9066\n",
      "Epoch 00185: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 24.0232 - accuracy: 0.9067 - val_loss: 22.8899 - val_accuracy: 0.8887\n",
      "Epoch 186/500\n",
      "138/185 [=====================>........] - ETA: 0s - loss: 27.7391 - accuracy: 0.8972\n",
      "Epoch 00186: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 26.8359 - accuracy: 0.9020 - val_loss: 20.1420 - val_accuracy: 0.9084\n",
      "Epoch 187/500\n",
      "174/185 [===========================>..] - ETA: 0s - loss: 22.9134 - accuracy: 0.9210\n",
      "Epoch 00187: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 22.9474 - accuracy: 0.9171 - val_loss: 25.4567 - val_accuracy: 0.8779\n",
      "Epoch 188/500\n",
      "145/185 [======================>.......] - ETA: 0s - loss: 21.4616 - accuracy: 0.9183\n",
      "Epoch 00188: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 20.2868 - accuracy: 0.9252 - val_loss: 15.9484 - val_accuracy: 0.9328\n",
      "Epoch 189/500\n",
      "176/185 [===========================>..] - ETA: 0s - loss: 17.2645 - accuracy: 0.9393\n",
      "Epoch 00189: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 17.2733 - accuracy: 0.9391 - val_loss: 20.3124 - val_accuracy: 0.9213\n",
      "Epoch 190/500\n",
      "177/185 [===========================>..] - ETA: 0s - loss: 20.9402 - accuracy: 0.9313\n",
      "Epoch 00190: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 20.7148 - accuracy: 0.9301 - val_loss: 19.8121 - val_accuracy: 0.9193\n",
      "Epoch 191/500\n",
      "178/185 [===========================>..] - ETA: 0s - loss: 16.4500 - accuracy: 0.9342\n",
      "Epoch 00191: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 16.6745 - accuracy: 0.9332 - val_loss: 18.2038 - val_accuracy: 0.9030\n",
      "Epoch 192/500\n",
      "180/185 [============================>.] - ETA: 0s - loss: 28.8448 - accuracy: 0.8887\n",
      "Epoch 00192: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 28.9110 - accuracy: 0.8876 - val_loss: 31.2311 - val_accuracy: 0.8501\n",
      "Epoch 193/500\n",
      "176/185 [===========================>..] - ETA: 0s - loss: 31.1752 - accuracy: 0.8766\n",
      "Epoch 00193: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 30.8272 - accuracy: 0.8775 - val_loss: 24.5991 - val_accuracy: 0.9064\n",
      "Epoch 194/500\n",
      "142/185 [======================>.......] - ETA: 0s - loss: 20.1495 - accuracy: 0.9254\n",
      "Epoch 00194: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 26.3547 - accuracy: 0.9142 - val_loss: 33.1005 - val_accuracy: 0.7395\n",
      "Epoch 195/500\n",
      "181/185 [============================>.] - ETA: 0s - loss: 24.4897 - accuracy: 0.8995\n",
      "Epoch 00195: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 24.4955 - accuracy: 0.8998 - val_loss: 18.4128 - val_accuracy: 0.9206\n",
      "Epoch 196/500\n",
      "176/185 [===========================>..] - ETA: 0s - loss: 18.7083 - accuracy: 0.9354\n",
      "Epoch 00196: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 18.6690 - accuracy: 0.9361 - val_loss: 15.9227 - val_accuracy: 0.9566\n",
      "Epoch 197/500\n",
      "181/185 [============================>.] - ETA: 0s - loss: 17.4794 - accuracy: 0.9420\n",
      "Epoch 00197: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 17.4227 - accuracy: 0.9422 - val_loss: 15.1649 - val_accuracy: 0.9607\n",
      "Epoch 198/500\n",
      "185/185 [==============================] - ETA: 0s - loss: 16.3217 - accuracy: 0.9488\n",
      "Epoch 00198: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 16.3217 - accuracy: 0.9488 - val_loss: 15.6697 - val_accuracy: 0.9593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199/500\n",
      "168/185 [==========================>...] - ETA: 0s - loss: 21.6018 - accuracy: 0.9474\n",
      "Epoch 00199: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 21.3417 - accuracy: 0.9469 - val_loss: 19.7106 - val_accuracy: 0.9410\n",
      "Epoch 200/500\n",
      "185/185 [==============================] - ETA: 0s - loss: 21.9290 - accuracy: 0.9160\n",
      "Epoch 00200: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 21.9290 - accuracy: 0.9160 - val_loss: 17.8298 - val_accuracy: 0.9315\n",
      "Epoch 201/500\n",
      "170/185 [==========================>...] - ETA: 0s - loss: 19.8900 - accuracy: 0.9426\n",
      "Epoch 00201: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 20.0184 - accuracy: 0.9413 - val_loss: 24.6096 - val_accuracy: 0.9138\n",
      "Epoch 202/500\n",
      "147/185 [======================>.......] - ETA: 0s - loss: 19.7092 - accuracy: 0.9392\n",
      "Epoch 00202: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 19.0480 - accuracy: 0.9406 - val_loss: 19.0580 - val_accuracy: 0.9315\n",
      "Epoch 203/500\n",
      "174/185 [===========================>..] - ETA: 0s - loss: 19.4126 - accuracy: 0.9186\n",
      "Epoch 00203: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 19.1986 - accuracy: 0.9213 - val_loss: 18.2438 - val_accuracy: 0.9457\n",
      "Epoch 204/500\n",
      "175/185 [===========================>..] - ETA: 0s - loss: 17.3397 - accuracy: 0.9371\n",
      "Epoch 00204: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 17.6630 - accuracy: 0.9349 - val_loss: 18.9457 - val_accuracy: 0.9294\n",
      "Epoch 205/500\n",
      "181/185 [============================>.] - ETA: 0s - loss: 23.5113 - accuracy: 0.9107\n",
      "Epoch 00205: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 23.4513 - accuracy: 0.9096 - val_loss: 20.2690 - val_accuracy: 0.7822\n",
      "Epoch 206/500\n",
      "140/185 [=====================>........] - ETA: 0s - loss: 16.5788 - accuracy: 0.9201\n",
      "Epoch 00206: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 20.9461 - accuracy: 0.9089 - val_loss: 26.8786 - val_accuracy: 0.7564\n",
      "Epoch 207/500\n",
      "175/185 [===========================>..] - ETA: 0s - loss: 20.6823 - accuracy: 0.9093\n",
      "Epoch 00207: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 20.3833 - accuracy: 0.9118 - val_loss: 16.3688 - val_accuracy: 0.9457\n",
      "Epoch 208/500\n",
      "183/185 [============================>.] - ETA: 0s - loss: 17.7297 - accuracy: 0.9399\n",
      "Epoch 00208: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 17.7195 - accuracy: 0.9401 - val_loss: 18.1706 - val_accuracy: 0.9349\n",
      "Epoch 209/500\n",
      "185/185 [==============================] - ETA: 0s - loss: 24.8590 - accuracy: 0.8874\n",
      "Epoch 00209: val_loss did not improve from 14.01913\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 24.8590 - accuracy: 0.8874 - val_loss: 16.9109 - val_accuracy: 0.9430\n",
      "Epoch 00209: early stopping\n"
     ]
    }
   ],
   "source": [
    "# history1= NN_model1.fit(X_train, Y_train, epochs=500, batch_size=32, validation_split = 0.2, callbacks=callback_list)\n",
    "# #history= NN_model.fit(X_train, Y_train, epochs=7, batch_size=32, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN_model1.save('random_split_with_min_max.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 21.594, Validation loss: 2373.125\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # evaluate the model\n",
    "# train_loss, train_acc = NN_model1.evaluate(X_train, Y_train, verbose=0)\n",
    "# test_loss, test_acc = NN_model1.evaluate(X_test, Y_test, verbose=0)\n",
    "# print('Train loss: %.3f, Validation loss: %.3f' % (train_loss, test_loss))\n",
    "# # plot training history\n",
    "# plt.plot(history1.history['loss'], label='train')\n",
    "# plt.plot(history1.history['val_loss'], label='Validation')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latest=tf.train.latest_checkpoint(checkpoint_dir)\n",
    "# checkpoint_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_pred = NN_model1.predict(X_test)\n",
    "# Y_pred=scaling_y.inverse_transform(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percentage error on test set:  [0.05579034 0.45661266 0.09064317]\n"
     ]
    }
   ],
   "source": [
    "# error= mean_absolute_percentage_error(Y_test, Y_pred, multioutput='raw_values')   \n",
    "# print('Mean absolute percentage error on test set: ', error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Without MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X, Y, \n",
    "        test_size=0.25, \n",
    "        random_state=42)\n",
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               1152      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 166,531\n",
      "Trainable params: 166,531\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NN_model = Sequential()\n",
    "\n",
    "# The Input Layer :\n",
    "NN_model.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n",
    "\n",
    "# The Hidden Layers :\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "\n",
    "# The Output Layer :\n",
    "NN_model.add(Dense(3, kernel_initializer='normal',activation='linear'))\n",
    "# Compile the network :\n",
    "NN_model.compile(loss='mean_absolute_percentage_error', optimizer='adam', metrics=['accuracy'])\n",
    "NN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"random_split/Weights-{epoch:03d}--{val_loss:.5f}.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, verbose=1, monitor='val_loss',save_best_only=True, mode='auto')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # patient early stopping\n",
    "es = EarlyStopping(monitor='val_loss', patience=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_csv=CSVLogger('random_split_loss_logs.csv', separator=',', append=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_list=[checkpoint, es, log_csv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "178/185 [===========================>..] - ETA: 0s - loss: 564.1160 - accuracy: 0.7530\n",
      "Epoch 00001: val_loss improved from inf to 130.90567, saving model to random_split\\Weights-001--130.90567.hdf5\n",
      "185/185 [==============================] - 0s 3ms/step - loss: 547.4772 - accuracy: 0.7576 - val_loss: 130.9057 - val_accuracy: 0.9030\n",
      "Epoch 2/500\n",
      "165/185 [=========================>....] - ETA: 0s - loss: 123.7188 - accuracy: 0.8731\n",
      "Epoch 00002: val_loss improved from 130.90567 to 32.18298, saving model to random_split\\Weights-002--32.18298.hdf5\n",
      "185/185 [==============================] - 0s 2ms/step - loss: 114.5654 - accuracy: 0.8743 - val_loss: 32.1830 - val_accuracy: 0.9084\n",
      "Epoch 3/500\n",
      "153/185 [=======================>......] - ETA: 0s - loss: 40.7454 - accuracy: 0.8985\n",
      "Epoch 00003: val_loss did not improve from 32.18298\n",
      "185/185 [==============================] - 0s 2ms/step - loss: 42.2338 - accuracy: 0.8967 - val_loss: 38.2793 - val_accuracy: 0.8894\n",
      "Epoch 4/500\n",
      "182/185 [============================>.] - ETA: 0s - loss: 34.6452 - accuracy: 0.9083\n",
      "Epoch 00004: val_loss improved from 32.18298 to 31.80699, saving model to random_split\\Weights-004--31.80699.hdf5\n",
      "185/185 [==============================] - 0s 2ms/step - loss: 34.5391 - accuracy: 0.9086 - val_loss: 31.8070 - val_accuracy: 0.9179\n",
      "Epoch 5/500\n",
      "179/185 [============================>.] - ETA: 0s - loss: 25.9961 - accuracy: 0.9225\n",
      "Epoch 00005: val_loss did not improve from 31.80699\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 26.3403 - accuracy: 0.9233 - val_loss: 37.0825 - val_accuracy: 0.9261\n",
      "Epoch 6/500\n",
      "173/185 [===========================>..] - ETA: 0s - loss: 26.4624 - accuracy: 0.9274\n",
      "Epoch 00006: val_loss improved from 31.80699 to 30.03544, saving model to random_split\\Weights-006--30.03544.hdf5\n",
      "185/185 [==============================] - 0s 2ms/step - loss: 26.1094 - accuracy: 0.9284 - val_loss: 30.0354 - val_accuracy: 0.9138\n",
      "Epoch 7/500\n",
      "139/185 [=====================>........] - ETA: 0s - loss: 21.8787 - accuracy: 0.9427\n",
      "Epoch 00007: val_loss improved from 30.03544 to 23.15626, saving model to random_split\\Weights-007--23.15626.hdf5\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 21.9121 - accuracy: 0.9405 - val_loss: 23.1563 - val_accuracy: 0.9322\n",
      "Epoch 8/500\n",
      "176/185 [===========================>..] - ETA: 0s - loss: 18.8791 - accuracy: 0.9442\n",
      "Epoch 00008: val_loss did not improve from 23.15626\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 19.3068 - accuracy: 0.9435 - val_loss: 25.3420 - val_accuracy: 0.9586\n",
      "Epoch 9/500\n",
      "176/185 [===========================>..] - ETA: 0s - loss: 21.7867 - accuracy: 0.9435\n",
      "Epoch 00009: val_loss did not improve from 23.15626\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 22.0532 - accuracy: 0.9430 - val_loss: 26.3870 - val_accuracy: 0.9016\n",
      "Epoch 10/500\n",
      "166/185 [=========================>....] - ETA: 0s - loss: 23.9540 - accuracy: 0.9439\n",
      "Epoch 00010: val_loss improved from 23.15626 to 18.75241, saving model to random_split\\Weights-010--18.75241.hdf5\n",
      "185/185 [==============================] - 0s 2ms/step - loss: 23.5594 - accuracy: 0.9439 - val_loss: 18.7524 - val_accuracy: 0.9525\n",
      "Epoch 11/500\n",
      "175/185 [===========================>..] - ETA: 0s - loss: 19.8205 - accuracy: 0.9495\n",
      "Epoch 00011: val_loss improved from 18.75241 to 16.21133, saving model to random_split\\Weights-011--16.21133.hdf5\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 19.6962 - accuracy: 0.9503 - val_loss: 16.2113 - val_accuracy: 0.9491\n",
      "Epoch 12/500\n",
      "172/185 [==========================>...] - ETA: 0s - loss: 18.5730 - accuracy: 0.9553\n",
      "Epoch 00012: val_loss did not improve from 16.21133\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 18.7797 - accuracy: 0.9539 - val_loss: 18.2635 - val_accuracy: 0.9539\n",
      "Epoch 13/500\n",
      "178/185 [===========================>..] - ETA: 0s - loss: 17.3984 - accuracy: 0.9545\n",
      "Epoch 00013: val_loss did not improve from 16.21133\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 17.3692 - accuracy: 0.9545 - val_loss: 18.8144 - val_accuracy: 0.9342\n",
      "Epoch 14/500\n",
      "183/185 [============================>.] - ETA: 0s - loss: 16.4830 - accuracy: 0.9575\n",
      "Epoch 00014: val_loss did not improve from 16.21133\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 16.4983 - accuracy: 0.9569 - val_loss: 16.8942 - val_accuracy: 0.9545\n",
      "Epoch 15/500\n",
      "181/185 [============================>.] - ETA: 0s - loss: 16.7960 - accuracy: 0.9555\n",
      "Epoch 00015: val_loss improved from 16.21133 to 15.43064, saving model to random_split\\Weights-015--15.43064.hdf5\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 16.8014 - accuracy: 0.9549 - val_loss: 15.4306 - val_accuracy: 0.9654\n",
      "Epoch 16/500\n",
      "178/185 [===========================>..] - ETA: 0s - loss: 17.8526 - accuracy: 0.9540\n",
      "Epoch 00016: val_loss did not improve from 15.43064\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 17.7818 - accuracy: 0.9534 - val_loss: 16.6479 - val_accuracy: 0.9559\n",
      "Epoch 17/500\n",
      "169/185 [==========================>...] - ETA: 0s - loss: 16.3447 - accuracy: 0.9543\n",
      "Epoch 00017: val_loss improved from 15.43064 to 13.98057, saving model to random_split\\Weights-017--13.98057.hdf5\n",
      "185/185 [==============================] - 0s 2ms/step - loss: 16.2244 - accuracy: 0.9551 - val_loss: 13.9806 - val_accuracy: 0.9627\n",
      "Epoch 18/500\n",
      "172/185 [==========================>...] - ETA: 0s - loss: 15.3367 - accuracy: 0.9628\n",
      "Epoch 00018: val_loss did not improve from 13.98057\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 15.3968 - accuracy: 0.9617 - val_loss: 16.0363 - val_accuracy: 0.9654\n",
      "Epoch 19/500\n",
      "178/185 [===========================>..] - ETA: 0s - loss: 15.7937 - accuracy: 0.9589\n",
      "Epoch 00019: val_loss did not improve from 13.98057\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 15.8142 - accuracy: 0.9595 - val_loss: 18.7419 - val_accuracy: 0.9681\n",
      "Epoch 20/500\n",
      "178/185 [===========================>..] - ETA: 0s - loss: 15.3009 - accuracy: 0.9586\n",
      "Epoch 00020: val_loss did not improve from 13.98057\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 15.2473 - accuracy: 0.9586 - val_loss: 15.3527 - val_accuracy: 0.9715\n",
      "Epoch 21/500\n",
      "177/185 [===========================>..] - ETA: 0s - loss: 15.3562 - accuracy: 0.9583\n",
      "Epoch 00021: val_loss did not improve from 13.98057\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 15.3078 - accuracy: 0.9588 - val_loss: 15.8869 - val_accuracy: 0.9729\n",
      "Epoch 22/500\n",
      "181/185 [============================>.] - ETA: 0s - loss: 15.2757 - accuracy: 0.9637\n",
      "Epoch 00022: val_loss did not improve from 13.98057\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 15.2525 - accuracy: 0.9635 - val_loss: 14.3362 - val_accuracy: 0.9593\n",
      "Epoch 23/500\n",
      "183/185 [============================>.] - ETA: 0s - loss: 14.9151 - accuracy: 0.9628\n",
      "Epoch 00023: val_loss improved from 13.98057 to 13.89445, saving model to random_split\\Weights-023--13.89445.hdf5\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 14.9177 - accuracy: 0.9629 - val_loss: 13.8944 - val_accuracy: 0.9661\n",
      "Epoch 24/500\n",
      "149/185 [=======================>......] - ETA: 0s - loss: 15.7068 - accuracy: 0.9608\n",
      "Epoch 00024: val_loss did not improve from 13.89445\n",
      "185/185 [==============================] - 0s 2ms/step - loss: 15.8817 - accuracy: 0.9603 - val_loss: 20.2883 - val_accuracy: 0.9674\n",
      "Epoch 25/500\n",
      "156/185 [========================>.....] - ETA: 0s - loss: 15.2377 - accuracy: 0.9613\n",
      "Epoch 00025: val_loss improved from 13.89445 to 13.80342, saving model to random_split\\Weights-025--13.80342.hdf5\n",
      "185/185 [==============================] - 0s 2ms/step - loss: 15.4671 - accuracy: 0.9617 - val_loss: 13.8034 - val_accuracy: 0.9600\n",
      "Epoch 26/500\n",
      "181/185 [============================>.] - ETA: 0s - loss: 14.7185 - accuracy: 0.9606\n",
      "Epoch 00026: val_loss improved from 13.80342 to 13.35999, saving model to random_split\\Weights-026--13.35999.hdf5\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 14.7176 - accuracy: 0.9610 - val_loss: 13.3600 - val_accuracy: 0.9681\n",
      "Epoch 27/500\n",
      "174/185 [===========================>..] - ETA: 0s - loss: 15.1941 - accuracy: 0.9614\n",
      "Epoch 00027: val_loss did not improve from 13.35999\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 15.1194 - accuracy: 0.9607 - val_loss: 19.0846 - val_accuracy: 0.9695\n",
      "Epoch 28/500\n",
      "176/185 [===========================>..] - ETA: 0s - loss: 18.2023 - accuracy: 0.9585\n",
      "Epoch 00028: val_loss did not improve from 13.35999\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 18.1652 - accuracy: 0.9576 - val_loss: 18.8693 - val_accuracy: 0.9410\n",
      "Epoch 29/500\n",
      "174/185 [===========================>..] - ETA: 0s - loss: 15.7814 - accuracy: 0.9573\n",
      "Epoch 00029: val_loss did not improve from 13.35999\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 15.7158 - accuracy: 0.9579 - val_loss: 14.4039 - val_accuracy: 0.9668\n",
      "Epoch 30/500\n",
      "167/185 [==========================>...] - ETA: 0s - loss: 14.9158 - accuracy: 0.9585\n",
      "Epoch 00030: val_loss did not improve from 13.35999\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 14.8240 - accuracy: 0.9591 - val_loss: 13.8817 - val_accuracy: 0.9627\n",
      "Epoch 31/500\n",
      "165/185 [=========================>....] - ETA: 0s - loss: 14.7326 - accuracy: 0.9629\n",
      "Epoch 00031: val_loss did not improve from 13.35999\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 14.6721 - accuracy: 0.9613 - val_loss: 14.7141 - val_accuracy: 0.9586\n",
      "Epoch 32/500\n",
      "183/185 [============================>.] - ETA: 0s - loss: 14.0452 - accuracy: 0.9635\n",
      "Epoch 00032: val_loss did not improve from 13.35999\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 14.0404 - accuracy: 0.9632 - val_loss: 15.6963 - val_accuracy: 0.9457\n",
      "Epoch 33/500\n",
      "184/185 [============================>.] - ETA: 0s - loss: 14.0960 - accuracy: 0.9582\n",
      "Epoch 00033: val_loss did not improve from 13.35999\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 14.0959 - accuracy: 0.9583 - val_loss: 15.5039 - val_accuracy: 0.9403\n",
      "Epoch 34/500\n",
      "176/185 [===========================>..] - ETA: 0s - loss: 14.4831 - accuracy: 0.9592\n",
      "Epoch 00034: val_loss improved from 13.35999 to 12.71593, saving model to random_split\\Weights-034--12.71593.hdf5\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 14.4738 - accuracy: 0.9598 - val_loss: 12.7159 - val_accuracy: 0.9742\n",
      "Epoch 35/500\n",
      "180/185 [============================>.] - ETA: 0s - loss: 14.4071 - accuracy: 0.9609\n",
      "Epoch 00035: val_loss did not improve from 12.71593\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 14.4121 - accuracy: 0.9607 - val_loss: 14.8023 - val_accuracy: 0.9688\n",
      "Epoch 36/500\n",
      "181/185 [============================>.] - ETA: 0s - loss: 13.7492 - accuracy: 0.9584\n",
      "Epoch 00036: val_loss did not improve from 12.71593\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 13.7187 - accuracy: 0.9583 - val_loss: 12.8248 - val_accuracy: 0.9620\n",
      "Epoch 37/500\n",
      "184/185 [============================>.] - ETA: 0s - loss: 13.4614 - accuracy: 0.9609\n",
      "Epoch 00037: val_loss improved from 12.71593 to 12.41998, saving model to random_split\\Weights-037--12.41998.hdf5\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 13.4530 - accuracy: 0.9610 - val_loss: 12.4200 - val_accuracy: 0.9654\n",
      "Epoch 38/500\n",
      "156/185 [========================>.....] - ETA: 0s - loss: 12.4441 - accuracy: 0.9597\n",
      "Epoch 00038: val_loss did not improve from 12.41998\n",
      "185/185 [==============================] - 0s 2ms/step - loss: 12.6592 - accuracy: 0.9615 - val_loss: 15.2063 - val_accuracy: 0.9498\n",
      "Epoch 39/500\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 13.5767 - accuracy: 0.9605\n",
      "Epoch 00039: val_loss did not improve from 12.41998\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 13.6225 - accuracy: 0.9607 - val_loss: 16.0395 - val_accuracy: 0.9478\n",
      "Epoch 40/500\n",
      "155/185 [========================>.....] - ETA: 0s - loss: 12.9932 - accuracy: 0.9603\n",
      "Epoch 00040: val_loss did not improve from 12.41998\n",
      "185/185 [==============================] - 0s 2ms/step - loss: 12.9557 - accuracy: 0.9598 - val_loss: 15.5858 - val_accuracy: 0.9701\n",
      "Epoch 41/500\n",
      "159/185 [========================>.....] - ETA: 0s - loss: 12.1881 - accuracy: 0.9617\n",
      "Epoch 00041: val_loss improved from 12.41998 to 10.37445, saving model to random_split\\Weights-041--10.37445.hdf5\n",
      "185/185 [==============================] - 0s 2ms/step - loss: 11.9572 - accuracy: 0.9618 - val_loss: 10.3745 - val_accuracy: 0.9701\n",
      "Epoch 42/500\n",
      "179/185 [============================>.] - ETA: 0s - loss: 11.8589 - accuracy: 0.9628\n",
      "Epoch 00042: val_loss did not improve from 10.37445\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 11.8186 - accuracy: 0.9632 - val_loss: 10.6530 - val_accuracy: 0.9681\n",
      "Epoch 43/500\n",
      "179/185 [============================>.] - ETA: 0s - loss: 11.1163 - accuracy: 0.9625\n",
      "Epoch 00043: val_loss did not improve from 10.37445\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 11.1932 - accuracy: 0.9627 - val_loss: 10.6545 - val_accuracy: 0.9552\n",
      "Epoch 44/500\n",
      "183/185 [============================>.] - ETA: 0s - loss: 11.0006 - accuracy: 0.9667\n",
      "Epoch 00044: val_loss did not improve from 10.37445\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 11.0094 - accuracy: 0.9668 - val_loss: 12.1229 - val_accuracy: 0.9701\n",
      "Epoch 45/500\n",
      "139/185 [=====================>........] - ETA: 0s - loss: 10.2291 - accuracy: 0.9667\n",
      "Epoch 00045: val_loss improved from 10.37445 to 10.27973, saving model to random_split\\Weights-045--10.27973.hdf5\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 10.5143 - accuracy: 0.9662 - val_loss: 10.2797 - val_accuracy: 0.9573\n",
      "Epoch 46/500\n",
      "181/185 [============================>.] - ETA: 0s - loss: 10.3333 - accuracy: 0.9656\n",
      "Epoch 00046: val_loss improved from 10.27973 to 9.53519, saving model to random_split\\Weights-046--9.53519.hdf5\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 10.3263 - accuracy: 0.9659 - val_loss: 9.5352 - val_accuracy: 0.9613\n",
      "Epoch 47/500\n",
      "179/185 [============================>.] - ETA: 0s - loss: 9.7573 - accuracy: 0.9688\n",
      "Epoch 00047: val_loss improved from 9.53519 to 9.21469, saving model to random_split\\Weights-047--9.21469.hdf5\n",
      "185/185 [==============================] - 0s 2ms/step - loss: 9.7297 - accuracy: 0.9690 - val_loss: 9.2147 - val_accuracy: 0.9593\n",
      "Epoch 48/500\n",
      "181/185 [============================>.] - ETA: 0s - loss: 8.5808 - accuracy: 0.9691\n",
      "Epoch 00048: val_loss improved from 9.21469 to 7.62954, saving model to random_split\\Weights-048--7.62954.hdf5\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 8.5747 - accuracy: 0.9690 - val_loss: 7.6295 - val_accuracy: 0.9661\n",
      "Epoch 49/500\n",
      "143/185 [======================>.......] - ETA: 0s - loss: 8.5614 - accuracy: 0.9729\n",
      "Epoch 00049: val_loss did not improve from 7.62954\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 8.3541 - accuracy: 0.9713 - val_loss: 9.8852 - val_accuracy: 0.9742\n",
      "Epoch 50/500\n",
      "183/185 [============================>.] - ETA: 0s - loss: 8.1574 - accuracy: 0.9728\n",
      "Epoch 00050: val_loss improved from 7.62954 to 6.33414, saving model to random_split\\Weights-050--6.33414.hdf5\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 8.1563 - accuracy: 0.9730 - val_loss: 6.3341 - val_accuracy: 0.9763\n",
      "Epoch 51/500\n",
      "140/185 [=====================>........] - ETA: 0s - loss: 8.3936 - accuracy: 0.9734\n",
      "Epoch 00051: val_loss did not improve from 6.33414\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 8.3925 - accuracy: 0.9735 - val_loss: 7.4688 - val_accuracy: 0.9749\n",
      "Epoch 52/500\n",
      "182/185 [============================>.] - ETA: 0s - loss: 8.1569 - accuracy: 0.9725\n",
      "Epoch 00052: val_loss did not improve from 6.33414\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 8.1581 - accuracy: 0.9725 - val_loss: 7.5130 - val_accuracy: 0.9701\n",
      "Epoch 53/500\n",
      "141/185 [=====================>........] - ETA: 0s - loss: 8.2773 - accuracy: 0.9732\n",
      "Epoch 00053: val_loss did not improve from 6.33414\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 7.9033 - accuracy: 0.9739 - val_loss: 6.9176 - val_accuracy: 0.9796\n",
      "Epoch 54/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181/185 [============================>.] - ETA: 0s - loss: 7.8690 - accuracy: 0.9755\n",
      "Epoch 00054: val_loss did not improve from 6.33414\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 7.8431 - accuracy: 0.9754 - val_loss: 6.7820 - val_accuracy: 0.9763\n",
      "Epoch 55/500\n",
      "183/185 [============================>.] - ETA: 0s - loss: 7.3583 - accuracy: 0.9717\n",
      "Epoch 00055: val_loss did not improve from 6.33414\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 7.3456 - accuracy: 0.9718 - val_loss: 8.6559 - val_accuracy: 0.9776\n",
      "Epoch 56/500\n",
      "185/185 [==============================] - ETA: 0s - loss: 7.6317 - accuracy: 0.9735\n",
      "Epoch 00056: val_loss improved from 6.33414 to 5.56985, saving model to random_split\\Weights-056--5.56985.hdf5\n",
      "185/185 [==============================] - 0s 2ms/step - loss: 7.6317 - accuracy: 0.9735 - val_loss: 5.5699 - val_accuracy: 0.9824\n",
      "Epoch 57/500\n",
      "143/185 [======================>.......] - ETA: 0s - loss: 7.0148 - accuracy: 0.9775\n",
      "Epoch 00057: val_loss did not improve from 5.56985\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 6.9401 - accuracy: 0.9774 - val_loss: 6.8204 - val_accuracy: 0.9824\n",
      "Epoch 58/500\n",
      "183/185 [============================>.] - ETA: 0s - loss: 7.4669 - accuracy: 0.9764\n",
      "Epoch 00058: val_loss did not improve from 5.56985\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 7.4701 - accuracy: 0.9761 - val_loss: 11.5039 - val_accuracy: 0.9695\n",
      "Epoch 59/500\n",
      "143/185 [======================>.......] - ETA: 0s - loss: 7.6902 - accuracy: 0.9716\n",
      "Epoch 00059: val_loss did not improve from 5.56985\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 7.6887 - accuracy: 0.9739 - val_loss: 7.7269 - val_accuracy: 0.9742\n",
      "Epoch 60/500\n",
      "172/185 [==========================>...] - ETA: 0s - loss: 6.6714 - accuracy: 0.9767 ETA: 0s - loss: 6.9663 - accuracy: \n",
      "Epoch 00060: val_loss did not improve from 5.56985\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 6.5993 - accuracy: 0.9778 - val_loss: 6.2149 - val_accuracy: 0.9864\n",
      "Epoch 61/500\n",
      "184/185 [============================>.] - ETA: 0s - loss: 6.7574 - accuracy: 0.9767\n",
      "Epoch 00061: val_loss did not improve from 5.56985\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 6.7539 - accuracy: 0.9768 - val_loss: 11.5736 - val_accuracy: 0.9756\n",
      "Epoch 62/500\n",
      "181/185 [============================>.] - ETA: 0s - loss: 6.8331 - accuracy: 0.9789\n",
      "Epoch 00062: val_loss did not improve from 5.56985\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 6.8027 - accuracy: 0.9793 - val_loss: 9.6008 - val_accuracy: 0.9647\n",
      "Epoch 63/500\n",
      "141/185 [=====================>........] - ETA: 0s - loss: 6.9582 - accuracy: 0.9738\n",
      "Epoch 00063: val_loss improved from 5.56985 to 5.47244, saving model to random_split\\Weights-063--5.47244.hdf5\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 6.6973 - accuracy: 0.9744 - val_loss: 5.4724 - val_accuracy: 0.9851\n",
      "Epoch 64/500\n",
      "184/185 [============================>.] - ETA: 0s - loss: 6.6764 - accuracy: 0.9779\n",
      "Epoch 00064: val_loss did not improve from 5.47244\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 6.6729 - accuracy: 0.9780 - val_loss: 8.7643 - val_accuracy: 0.9837\n",
      "Epoch 65/500\n",
      "143/185 [======================>.......] - ETA: 0s - loss: 6.5116 - accuracy: 0.9806\n",
      "Epoch 00065: val_loss did not improve from 5.47244\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 6.4834 - accuracy: 0.9808 - val_loss: 10.1807 - val_accuracy: 0.9688\n",
      "Epoch 66/500\n",
      "140/185 [=====================>........] - ETA: 0s - loss: 6.6495 - accuracy: 0.9792\n",
      "Epoch 00066: val_loss did not improve from 5.47244\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 6.6792 - accuracy: 0.9776 - val_loss: 7.7203 - val_accuracy: 0.9817\n",
      "Epoch 67/500\n",
      "138/185 [=====================>........] - ETA: 0s - loss: 6.3960 - accuracy: 0.9785\n",
      "Epoch 00067: val_loss did not improve from 5.47244\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 6.2529 - accuracy: 0.9790 - val_loss: 6.9039 - val_accuracy: 0.9858\n",
      "Epoch 68/500\n",
      "143/185 [======================>.......] - ETA: 0s - loss: 6.4433 - accuracy: 0.9812\n",
      "Epoch 00068: val_loss improved from 5.47244 to 5.20625, saving model to random_split\\Weights-068--5.20625.hdf5\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 6.3306 - accuracy: 0.9819 - val_loss: 5.2062 - val_accuracy: 0.9912\n",
      "Epoch 69/500\n",
      "142/185 [======================>.......] - ETA: 0s - loss: 6.0823 - accuracy: 0.9791\n",
      "Epoch 00069: val_loss did not improve from 5.20625\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 6.3240 - accuracy: 0.9776 - val_loss: 7.6574 - val_accuracy: 0.9607\n",
      "Epoch 70/500\n",
      "141/185 [=====================>........] - ETA: 0s - loss: 6.5231 - accuracy: 0.9801\n",
      "Epoch 00070: val_loss did not improve from 5.20625\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 6.4554 - accuracy: 0.9790 - val_loss: 5.7482 - val_accuracy: 0.9891\n",
      "Epoch 71/500\n",
      "178/185 [===========================>..] - ETA: 0s - loss: 6.2014 - accuracy: 0.9802\n",
      "Epoch 00071: val_loss did not improve from 5.20625\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 6.1985 - accuracy: 0.9800 - val_loss: 9.0475 - val_accuracy: 0.9674\n",
      "Epoch 72/500\n",
      "173/185 [===========================>..] - ETA: 0s - loss: 5.8860 - accuracy: 0.9818\n",
      "Epoch 00072: val_loss did not improve from 5.20625\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.9050 - accuracy: 0.9813 - val_loss: 6.3807 - val_accuracy: 0.9878\n",
      "Epoch 73/500\n",
      "185/185 [==============================] - ETA: 0s - loss: 6.4882 - accuracy: 0.9803\n",
      "Epoch 00073: val_loss did not improve from 5.20625\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 6.4882 - accuracy: 0.9803 - val_loss: 7.4403 - val_accuracy: 0.9769\n",
      "Epoch 74/500\n",
      "139/185 [=====================>........] - ETA: 0s - loss: 5.8722 - accuracy: 0.9782\n",
      "Epoch 00074: val_loss did not improve from 5.20625\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.7644 - accuracy: 0.9791 - val_loss: 10.8620 - val_accuracy: 0.9634\n",
      "Epoch 75/500\n",
      "184/185 [============================>.] - ETA: 0s - loss: 6.0306 - accuracy: 0.9805\n",
      "Epoch 00075: val_loss did not improve from 5.20625\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 6.0277 - accuracy: 0.9805 - val_loss: 6.5954 - val_accuracy: 0.9803\n",
      "Epoch 76/500\n",
      "178/185 [===========================>..] - ETA: 0s - loss: 5.9425 - accuracy: 0.9830\n",
      "Epoch 00076: val_loss did not improve from 5.20625\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.9984 - accuracy: 0.9827 - val_loss: 7.7731 - val_accuracy: 0.9763\n",
      "Epoch 77/500\n",
      "179/185 [============================>.] - ETA: 0s - loss: 5.7116 - accuracy: 0.9801\n",
      "Epoch 00077: val_loss did not improve from 5.20625\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.6665 - accuracy: 0.9805 - val_loss: 5.9769 - val_accuracy: 0.9864\n",
      "Epoch 78/500\n",
      "183/185 [============================>.] - ETA: 0s - loss: 5.6119 - accuracy: 0.9810\n",
      "Epoch 00078: val_loss did not improve from 5.20625\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.6073 - accuracy: 0.9810 - val_loss: 6.1946 - val_accuracy: 0.9776\n",
      "Epoch 79/500\n",
      "183/185 [============================>.] - ETA: 0s - loss: 5.5885 - accuracy: 0.9822\n",
      "Epoch 00079: val_loss improved from 5.20625 to 4.83207, saving model to random_split\\Weights-079--4.83207.hdf5\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.5751 - accuracy: 0.9820 - val_loss: 4.8321 - val_accuracy: 0.9830\n",
      "Epoch 80/500\n",
      "185/185 [==============================] - ETA: 0s - loss: 6.4120 - accuracy: 0.9802\n",
      "Epoch 00080: val_loss did not improve from 4.83207\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 6.4120 - accuracy: 0.9802 - val_loss: 6.0830 - val_accuracy: 0.9858\n",
      "Epoch 81/500\n",
      "177/185 [===========================>..] - ETA: 0s - loss: 5.8519 - accuracy: 0.9797\n",
      "Epoch 00081: val_loss did not improve from 4.83207\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.8480 - accuracy: 0.9791 - val_loss: 4.9893 - val_accuracy: 0.9844\n",
      "Epoch 82/500\n",
      "139/185 [=====================>........] - ETA: 0s - loss: 5.7505 - accuracy: 0.9809\n",
      "Epoch 00082: val_loss did not improve from 4.83207\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.9589 - accuracy: 0.9819 - val_loss: 6.3946 - val_accuracy: 0.9817\n",
      "Epoch 83/500\n",
      "141/185 [=====================>........] - ETA: 0s - loss: 6.5752 - accuracy: 0.9778\n",
      "Epoch 00083: val_loss did not improve from 4.83207\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 6.1165 - accuracy: 0.9798 - val_loss: 5.2236 - val_accuracy: 0.9878\n",
      "Epoch 84/500\n",
      "184/185 [============================>.] - ETA: 0s - loss: 5.4630 - accuracy: 0.9815\n",
      "Epoch 00084: val_loss did not improve from 4.83207\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.4637 - accuracy: 0.9815 - val_loss: 6.8205 - val_accuracy: 0.9817\n",
      "Epoch 85/500\n",
      "180/185 [============================>.] - ETA: 0s - loss: 5.4238 - accuracy: 0.9812\n",
      "Epoch 00085: val_loss did not improve from 4.83207\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.4222 - accuracy: 0.9813 - val_loss: 6.4724 - val_accuracy: 0.9803\n",
      "Epoch 86/500\n",
      "182/185 [============================>.] - ETA: 0s - loss: 6.0163 - accuracy: 0.9796\n",
      "Epoch 00086: val_loss did not improve from 4.83207\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.9972 - accuracy: 0.9796 - val_loss: 8.5601 - val_accuracy: 0.9796\n",
      "Epoch 87/500\n",
      "184/185 [============================>.] - ETA: 0s - loss: 5.8409 - accuracy: 0.9791\n",
      "Epoch 00087: val_loss did not improve from 4.83207\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.8393 - accuracy: 0.9791 - val_loss: 6.5288 - val_accuracy: 0.9803\n",
      "Epoch 88/500\n",
      "168/185 [==========================>...] - ETA: 0s - loss: 6.0698 - accuracy: 0.9777\n",
      "Epoch 00088: val_loss did not improve from 4.83207\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 6.1191 - accuracy: 0.9776 - val_loss: 5.1511 - val_accuracy: 0.9851\n",
      "Epoch 89/500\n",
      "184/185 [============================>.] - ETA: 0s - loss: 5.4588 - accuracy: 0.9806\n",
      "Epoch 00089: val_loss did not improve from 4.83207\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.4564 - accuracy: 0.9807 - val_loss: 5.8378 - val_accuracy: 0.9824\n",
      "Epoch 90/500\n",
      "181/185 [============================>.] - ETA: 0s - loss: 5.6778 - accuracy: 0.9784\n",
      "Epoch 00090: val_loss did not improve from 4.83207\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.6782 - accuracy: 0.9781 - val_loss: 7.6043 - val_accuracy: 0.9837\n",
      "Epoch 91/500\n",
      "185/185 [==============================] - ETA: 0s - loss: 5.5427 - accuracy: 0.9834\n",
      "Epoch 00091: val_loss did not improve from 4.83207\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.5427 - accuracy: 0.9834 - val_loss: 7.7438 - val_accuracy: 0.9749\n",
      "Epoch 92/500\n",
      "183/185 [============================>.] - ETA: 0s - loss: 6.6609 - accuracy: 0.9793\n",
      "Epoch 00092: val_loss did not improve from 4.83207\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 6.6526 - accuracy: 0.9793 - val_loss: 5.2612 - val_accuracy: 0.9871\n",
      "Epoch 93/500\n",
      "185/185 [==============================] - ETA: 0s - loss: 5.1124 - accuracy: 0.9837\n",
      "Epoch 00093: val_loss did not improve from 4.83207\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.1124 - accuracy: 0.9837 - val_loss: 5.4664 - val_accuracy: 0.9912\n",
      "Epoch 94/500\n",
      "185/185 [==============================] - ETA: 0s - loss: 5.6392 - accuracy: 0.9820\n",
      "Epoch 00094: val_loss did not improve from 4.83207\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.6392 - accuracy: 0.9820 - val_loss: 9.5089 - val_accuracy: 0.9790\n",
      "Epoch 95/500\n",
      "177/185 [===========================>..] - ETA: 0s - loss: 5.5235 - accuracy: 0.9808\n",
      "Epoch 00095: val_loss did not improve from 4.83207\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.5632 - accuracy: 0.9808 - val_loss: 5.4726 - val_accuracy: 0.9858\n",
      "Epoch 96/500\n",
      "168/185 [==========================>...] - ETA: 0s - loss: 5.5364 - accuracy: 0.9834\n",
      "Epoch 00096: val_loss did not improve from 4.83207\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.5344 - accuracy: 0.9834 - val_loss: 5.3057 - val_accuracy: 0.9837\n",
      "Epoch 97/500\n",
      "152/185 [=======================>......] - ETA: 0s - loss: 5.3789 - accuracy: 0.9831\n",
      "Epoch 00097: val_loss did not improve from 4.83207\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.3498 - accuracy: 0.9832 - val_loss: 11.4963 - val_accuracy: 0.9661\n",
      "Epoch 98/500\n",
      "182/185 [============================>.] - ETA: 0s - loss: 5.8933 - accuracy: 0.9818\n",
      "Epoch 00098: val_loss did not improve from 4.83207\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.8844 - accuracy: 0.9820 - val_loss: 5.1813 - val_accuracy: 0.9885\n",
      "Epoch 99/500\n",
      "140/185 [=====================>........] - ETA: 0s - loss: 5.4049 - accuracy: 0.9839\n",
      "Epoch 00099: val_loss did not improve from 4.83207\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.4332 - accuracy: 0.9817 - val_loss: 6.0482 - val_accuracy: 0.9851\n",
      "Epoch 100/500\n",
      "141/185 [=====================>........] - ETA: 0s - loss: 5.1674 - accuracy: 0.9825\n",
      "Epoch 00100: val_loss did not improve from 4.83207\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.3198 - accuracy: 0.9815 - val_loss: 5.1493 - val_accuracy: 0.9885\n",
      "Epoch 101/500\n",
      "185/185 [==============================] - ETA: 0s - loss: 5.4950 - accuracy: 0.9829\n",
      "Epoch 00101: val_loss did not improve from 4.83207\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.4950 - accuracy: 0.9829 - val_loss: 5.8179 - val_accuracy: 0.9742\n",
      "Epoch 102/500\n",
      "177/185 [===========================>..] - ETA: 0s - loss: 5.2064 - accuracy: 0.9815\n",
      "Epoch 00102: val_loss did not improve from 4.83207\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.2091 - accuracy: 0.9820 - val_loss: 6.5719 - val_accuracy: 0.9742\n",
      "Epoch 103/500\n",
      "185/185 [==============================] - ETA: 0s - loss: 5.6456 - accuracy: 0.9812\n",
      "Epoch 00103: val_loss did not improve from 4.83207\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.6456 - accuracy: 0.9812 - val_loss: 5.2780 - val_accuracy: 0.9885\n",
      "Epoch 104/500\n",
      "141/185 [=====================>........] - ETA: 0s - loss: 4.9979 - accuracy: 0.9827\n",
      "Epoch 00104: val_loss did not improve from 4.83207\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.3463 - accuracy: 0.9805 - val_loss: 6.1199 - val_accuracy: 0.9844\n",
      "Epoch 105/500\n",
      "180/185 [============================>.] - ETA: 0s - loss: 4.9399 - accuracy: 0.9832\n",
      "Epoch 00105: val_loss did not improve from 4.83207\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.9511 - accuracy: 0.9835 - val_loss: 5.4549 - val_accuracy: 0.9871\n",
      "Epoch 106/500\n",
      "177/185 [===========================>..] - ETA: 0s - loss: 4.7805 - accuracy: 0.9839\n",
      "Epoch 00106: val_loss did not improve from 4.83207\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.8144 - accuracy: 0.9844 - val_loss: 8.0996 - val_accuracy: 0.9824\n",
      "Epoch 107/500\n",
      "139/185 [=====================>........] - ETA: 0s - loss: 5.6178 - accuracy: 0.9818\n",
      "Epoch 00107: val_loss did not improve from 4.83207\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.4599 - accuracy: 0.9827 - val_loss: 5.0105 - val_accuracy: 0.9878\n",
      "Epoch 108/500\n",
      "142/185 [======================>.......] - ETA: 0s - loss: 5.0065 - accuracy: 0.9848\n",
      "Epoch 00108: val_loss did not improve from 4.83207\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.9638 - accuracy: 0.9844 - val_loss: 6.1455 - val_accuracy: 0.9783\n",
      "Epoch 109/500\n",
      "139/185 [=====================>........] - ETA: 0s - loss: 4.9595 - accuracy: 0.9820\n",
      "Epoch 00109: val_loss did not improve from 4.83207\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.9462 - accuracy: 0.9837 - val_loss: 4.8484 - val_accuracy: 0.9885\n",
      "Epoch 110/500\n",
      "183/185 [============================>.] - ETA: 0s - loss: 5.2850 - accuracy: 0.9848\n",
      "Epoch 00110: val_loss did not improve from 4.83207\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.2768 - accuracy: 0.9849 - val_loss: 5.7637 - val_accuracy: 0.9763\n",
      "Epoch 111/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181/185 [============================>.] - ETA: 0s - loss: 4.9524 - accuracy: 0.9846\n",
      "Epoch 00111: val_loss did not improve from 4.83207\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.9283 - accuracy: 0.9844 - val_loss: 5.7516 - val_accuracy: 0.9844\n",
      "Epoch 112/500\n",
      "184/185 [============================>.] - ETA: 0s - loss: 5.2050 - accuracy: 0.9845\n",
      "Epoch 00112: val_loss did not improve from 4.83207\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.2021 - accuracy: 0.9846 - val_loss: 5.3544 - val_accuracy: 0.9817\n",
      "Epoch 113/500\n",
      "178/185 [===========================>..] - ETA: 0s - loss: 5.6417 - accuracy: 0.9821\n",
      "Epoch 00113: val_loss did not improve from 4.83207\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.6567 - accuracy: 0.9822 - val_loss: 6.1838 - val_accuracy: 0.9742\n",
      "Epoch 114/500\n",
      "142/185 [======================>.......] - ETA: 0s - loss: 5.0110 - accuracy: 0.9844\n",
      "Epoch 00114: val_loss improved from 4.83207 to 4.64275, saving model to random_split\\Weights-114--4.64275.hdf5\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.0410 - accuracy: 0.9847 - val_loss: 4.6427 - val_accuracy: 0.9864\n",
      "Epoch 115/500\n",
      "183/185 [============================>.] - ETA: 0s - loss: 5.3463 - accuracy: 0.9822\n",
      "Epoch 00115: val_loss did not improve from 4.64275\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.3499 - accuracy: 0.9824 - val_loss: 5.5768 - val_accuracy: 0.9769\n",
      "Epoch 116/500\n",
      "140/185 [=====================>........] - ETA: 0s - loss: 5.4495 - accuracy: 0.9812\n",
      "Epoch 00116: val_loss did not improve from 4.64275\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.4814 - accuracy: 0.9822 - val_loss: 6.0937 - val_accuracy: 0.9803\n",
      "Epoch 117/500\n",
      "141/185 [=====================>........] - ETA: 0s - loss: 5.4540 - accuracy: 0.9834\n",
      "Epoch 00117: val_loss improved from 4.64275 to 4.02962, saving model to random_split\\Weights-117--4.02962.hdf5\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.3018 - accuracy: 0.9841 - val_loss: 4.0296 - val_accuracy: 0.9796\n",
      "Epoch 118/500\n",
      "172/185 [==========================>...] - ETA: 0s - loss: 5.1418 - accuracy: 0.9836\n",
      "Epoch 00118: val_loss did not improve from 4.02962\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.0857 - accuracy: 0.9841 - val_loss: 4.4323 - val_accuracy: 0.9905\n",
      "Epoch 119/500\n",
      "185/185 [==============================] - ETA: 0s - loss: 5.1706 - accuracy: 0.9825\n",
      "Epoch 00119: val_loss did not improve from 4.02962\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.1706 - accuracy: 0.9825 - val_loss: 6.2070 - val_accuracy: 0.9803\n",
      "Epoch 120/500\n",
      "184/185 [============================>.] - ETA: 0s - loss: 5.0903 - accuracy: 0.9834\n",
      "Epoch 00120: val_loss did not improve from 4.02962\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.0868 - accuracy: 0.9834 - val_loss: 6.1694 - val_accuracy: 0.9851\n",
      "Epoch 121/500\n",
      "180/185 [============================>.] - ETA: 0s - loss: 5.1068 - accuracy: 0.9856\n",
      "Epoch 00121: val_loss improved from 4.02962 to 3.75531, saving model to random_split\\Weights-121--3.75531.hdf5\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.0908 - accuracy: 0.9856 - val_loss: 3.7553 - val_accuracy: 0.9858\n",
      "Epoch 122/500\n",
      "176/185 [===========================>..] - ETA: 0s - loss: 4.7619 - accuracy: 0.9812\n",
      "Epoch 00122: val_loss did not improve from 3.75531\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.7648 - accuracy: 0.9810 - val_loss: 5.4990 - val_accuracy: 0.9919\n",
      "Epoch 123/500\n",
      "182/185 [============================>.] - ETA: 0s - loss: 4.7027 - accuracy: 0.9851\n",
      "Epoch 00123: val_loss did not improve from 3.75531\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.6994 - accuracy: 0.9851 - val_loss: 5.6218 - val_accuracy: 0.9858\n",
      "Epoch 124/500\n",
      "182/185 [============================>.] - ETA: 0s - loss: 5.1610 - accuracy: 0.9835\n",
      "Epoch 00124: val_loss did not improve from 3.75531\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.1670 - accuracy: 0.9834 - val_loss: 5.6786 - val_accuracy: 0.9837\n",
      "Epoch 125/500\n",
      "141/185 [=====================>........] - ETA: 0s - loss: 4.3958 - accuracy: 0.9856\n",
      "Epoch 00125: val_loss did not improve from 3.75531\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.5739 - accuracy: 0.9847 - val_loss: 3.8465 - val_accuracy: 0.9898\n",
      "Epoch 126/500\n",
      "183/185 [============================>.] - ETA: 0s - loss: 4.8725 - accuracy: 0.9848\n",
      "Epoch 00126: val_loss did not improve from 3.75531\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.8746 - accuracy: 0.9849 - val_loss: 5.2056 - val_accuracy: 0.9871\n",
      "Epoch 127/500\n",
      "140/185 [=====================>........] - ETA: 0s - loss: 4.4008 - accuracy: 0.9879\n",
      "Epoch 00127: val_loss did not improve from 3.75531\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.6757 - accuracy: 0.9871 - val_loss: 4.8761 - val_accuracy: 0.9858\n",
      "Epoch 128/500\n",
      "174/185 [===========================>..] - ETA: 0s - loss: 5.1434 - accuracy: 0.9851\n",
      "Epoch 00128: val_loss did not improve from 3.75531\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.0899 - accuracy: 0.9849 - val_loss: 4.7933 - val_accuracy: 0.9912\n",
      "Epoch 129/500\n",
      "147/185 [======================>.......] - ETA: 0s - loss: 4.8645 - accuracy: 0.9843\n",
      "Epoch 00129: val_loss did not improve from 3.75531\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.7539 - accuracy: 0.9844 - val_loss: 6.5685 - val_accuracy: 0.9776\n",
      "Epoch 130/500\n",
      "137/185 [=====================>........] - ETA: 0s - loss: 4.8649 - accuracy: 0.9865\n",
      "Epoch 00130: val_loss did not improve from 3.75531\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.8723 - accuracy: 0.9861 - val_loss: 4.4928 - val_accuracy: 0.9803\n",
      "Epoch 131/500\n",
      "178/185 [===========================>..] - ETA: 0s - loss: 4.6469 - accuracy: 0.9849\n",
      "Epoch 00131: val_loss did not improve from 3.75531\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.6354 - accuracy: 0.9849 - val_loss: 6.2989 - val_accuracy: 0.9749\n",
      "Epoch 132/500\n",
      "185/185 [==============================] - ETA: 0s - loss: 5.0560 - accuracy: 0.9846\n",
      "Epoch 00132: val_loss did not improve from 3.75531\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.0560 - accuracy: 0.9846 - val_loss: 4.7297 - val_accuracy: 0.9803\n",
      "Epoch 133/500\n",
      "140/185 [=====================>........] - ETA: 0s - loss: 4.8596 - accuracy: 0.9799\n",
      "Epoch 00133: val_loss did not improve from 3.75531\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.9394 - accuracy: 0.9812 - val_loss: 5.9347 - val_accuracy: 0.9898\n",
      "Epoch 134/500\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 5.3622 - accuracy: 0.9861\n",
      "Epoch 00134: val_loss did not improve from 3.75531\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.3965 - accuracy: 0.9866 - val_loss: 4.5132 - val_accuracy: 0.9885\n",
      "Epoch 135/500\n",
      "169/185 [==========================>...] - ETA: 0s - loss: 5.0159 - accuracy: 0.9830\n",
      "Epoch 00135: val_loss did not improve from 3.75531\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.9433 - accuracy: 0.9832 - val_loss: 5.9224 - val_accuracy: 0.9837\n",
      "Epoch 136/500\n",
      "183/185 [============================>.] - ETA: 0s - loss: 4.7408 - accuracy: 0.9829\n",
      "Epoch 00136: val_loss did not improve from 3.75531\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.7299 - accuracy: 0.9830 - val_loss: 5.4129 - val_accuracy: 0.9796\n",
      "Epoch 137/500\n",
      "141/185 [=====================>........] - ETA: 0s - loss: 4.5684 - accuracy: 0.9832\n",
      "Epoch 00137: val_loss did not improve from 3.75531\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.7112 - accuracy: 0.9832 - val_loss: 8.0120 - val_accuracy: 0.9769\n",
      "Epoch 138/500\n",
      "171/185 [==========================>...] - ETA: 0s - loss: 4.6163 - accuracy: 0.9815\n",
      "Epoch 00138: val_loss did not improve from 3.75531\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.5737 - accuracy: 0.9815 - val_loss: 4.8511 - val_accuracy: 0.9905\n",
      "Epoch 139/500\n",
      "180/185 [============================>.] - ETA: 0s - loss: 4.7273 - accuracy: 0.9833\n",
      "Epoch 00139: val_loss did not improve from 3.75531\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.7383 - accuracy: 0.9834 - val_loss: 5.1901 - val_accuracy: 0.9898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140/500\n",
      "185/185 [==============================] - ETA: 0s - loss: 4.4730 - accuracy: 0.9844\n",
      "Epoch 00140: val_loss did not improve from 3.75531\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.4730 - accuracy: 0.9844 - val_loss: 6.7018 - val_accuracy: 0.9871\n",
      "Epoch 141/500\n",
      "141/185 [=====================>........] - ETA: 0s - loss: 5.3753 - accuracy: 0.9814\n",
      "Epoch 00141: val_loss did not improve from 3.75531\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.5012 - accuracy: 0.9815 - val_loss: 5.9097 - val_accuracy: 0.9803\n",
      "Epoch 142/500\n",
      "140/185 [=====================>........] - ETA: 0s - loss: 4.6230 - accuracy: 0.9839\n",
      "Epoch 00142: val_loss did not improve from 3.75531\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.7786 - accuracy: 0.9822 - val_loss: 4.5463 - val_accuracy: 0.9871\n",
      "Epoch 143/500\n",
      "178/185 [===========================>..] - ETA: 0s - loss: 4.5827 - accuracy: 0.9847\n",
      "Epoch 00143: val_loss did not improve from 3.75531\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.6411 - accuracy: 0.9844 - val_loss: 7.4952 - val_accuracy: 0.9817\n",
      "Epoch 144/500\n",
      "183/185 [============================>.] - ETA: 0s - loss: 5.1062 - accuracy: 0.9793\n",
      "Epoch 00144: val_loss did not improve from 3.75531\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.1055 - accuracy: 0.9791 - val_loss: 5.3927 - val_accuracy: 0.9858\n",
      "Epoch 145/500\n",
      "141/185 [=====================>........] - ETA: 0s - loss: 5.1483 - accuracy: 0.9825\n",
      "Epoch 00145: val_loss did not improve from 3.75531\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.0472 - accuracy: 0.9825 - val_loss: 4.6892 - val_accuracy: 0.9891\n",
      "Epoch 146/500\n",
      "178/185 [===========================>..] - ETA: 0s - loss: 4.6248 - accuracy: 0.9842\n",
      "Epoch 00146: val_loss improved from 3.75531 to 3.29350, saving model to random_split\\Weights-146--3.29350.hdf5\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.6221 - accuracy: 0.9842 - val_loss: 3.2935 - val_accuracy: 0.9932\n",
      "Epoch 147/500\n",
      "163/185 [=========================>....] - ETA: 0s - loss: 4.7942 - accuracy: 0.9845\n",
      "Epoch 00147: val_loss did not improve from 3.29350\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.7050 - accuracy: 0.9842 - val_loss: 4.8023 - val_accuracy: 0.9817\n",
      "Epoch 148/500\n",
      "143/185 [======================>.......] - ETA: 0s - loss: 4.7400 - accuracy: 0.9847\n",
      "Epoch 00148: val_loss did not improve from 3.29350\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.6954 - accuracy: 0.9837 - val_loss: 5.2428 - val_accuracy: 0.9776\n",
      "Epoch 149/500\n",
      "156/185 [========================>.....] - ETA: 0s - loss: 4.5784 - accuracy: 0.9854\n",
      "Epoch 00149: val_loss did not improve from 3.29350\n",
      "185/185 [==============================] - 0s 2ms/step - loss: 4.4837 - accuracy: 0.9846 - val_loss: 3.4047 - val_accuracy: 0.9898\n",
      "Epoch 150/500\n",
      "153/185 [=======================>......] - ETA: 0s - loss: 4.6613 - accuracy: 0.9839\n",
      "Epoch 00150: val_loss did not improve from 3.29350\n",
      "185/185 [==============================] - 0s 2ms/step - loss: 4.5382 - accuracy: 0.9846 - val_loss: 5.9396 - val_accuracy: 0.9878\n",
      "Epoch 151/500\n",
      "181/185 [============================>.] - ETA: 0s - loss: 5.1328 - accuracy: 0.9848\n",
      "Epoch 00151: val_loss did not improve from 3.29350\n",
      "185/185 [==============================] - 0s 2ms/step - loss: 5.1237 - accuracy: 0.9846 - val_loss: 4.0022 - val_accuracy: 0.9925\n",
      "Epoch 152/500\n",
      "175/185 [===========================>..] - ETA: 0s - loss: 4.6052 - accuracy: 0.9861\n",
      "Epoch 00152: val_loss did not improve from 3.29350\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.5859 - accuracy: 0.9861 - val_loss: 6.9329 - val_accuracy: 0.9790\n",
      "Epoch 153/500\n",
      "168/185 [==========================>...] - ETA: 0s - loss: 4.4075 - accuracy: 0.9827\n",
      "Epoch 00153: val_loss did not improve from 3.29350\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.5498 - accuracy: 0.9832 - val_loss: 3.8053 - val_accuracy: 0.9905\n",
      "Epoch 154/500\n",
      "159/185 [========================>.....] - ETA: 0s - loss: 4.4838 - accuracy: 0.9855\n",
      "Epoch 00154: val_loss did not improve from 3.29350\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.4926 - accuracy: 0.9852 - val_loss: 5.7126 - val_accuracy: 0.9695\n",
      "Epoch 155/500\n",
      "178/185 [===========================>..] - ETA: 0s - loss: 4.4555 - accuracy: 0.9835\n",
      "Epoch 00155: val_loss did not improve from 3.29350\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.4345 - accuracy: 0.9835 - val_loss: 4.5980 - val_accuracy: 0.9898\n",
      "Epoch 156/500\n",
      "172/185 [==========================>...] - ETA: 0s - loss: 4.2020 - accuracy: 0.9869\n",
      "Epoch 00156: val_loss did not improve from 3.29350\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.2045 - accuracy: 0.9871 - val_loss: 3.6422 - val_accuracy: 0.9919\n",
      "Epoch 157/500\n",
      "157/185 [========================>.....] - ETA: 0s - loss: 4.7117 - accuracy: 0.9823\n",
      "Epoch 00157: val_loss did not improve from 3.29350\n",
      "185/185 [==============================] - 0s 2ms/step - loss: 4.6575 - accuracy: 0.9834 - val_loss: 5.0690 - val_accuracy: 0.9919\n",
      "Epoch 158/500\n",
      "184/185 [============================>.] - ETA: 0s - loss: 4.9207 - accuracy: 0.9864\n",
      "Epoch 00158: val_loss did not improve from 3.29350\n",
      "185/185 [==============================] - 0s 2ms/step - loss: 4.9189 - accuracy: 0.9864 - val_loss: 4.4024 - val_accuracy: 0.9885\n",
      "Epoch 159/500\n",
      "152/185 [=======================>......] - ETA: 0s - loss: 4.0366 - accuracy: 0.9866\n",
      "Epoch 00159: val_loss did not improve from 3.29350\n",
      "185/185 [==============================] - 0s 2ms/step - loss: 4.2841 - accuracy: 0.9849 - val_loss: 4.6478 - val_accuracy: 0.9837\n",
      "Epoch 160/500\n",
      "176/185 [===========================>..] - ETA: 0s - loss: 5.5934 - accuracy: 0.9821\n",
      "Epoch 00160: val_loss did not improve from 3.29350\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.6400 - accuracy: 0.9819 - val_loss: 5.7289 - val_accuracy: 0.9776\n",
      "Epoch 161/500\n",
      "173/185 [===========================>..] - ETA: 0s - loss: 4.0844 - accuracy: 0.9825\n",
      "Epoch 00161: val_loss did not improve from 3.29350\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.1363 - accuracy: 0.9825 - val_loss: 4.8889 - val_accuracy: 0.9837\n",
      "Epoch 162/500\n",
      "175/185 [===========================>..] - ETA: 0s - loss: 4.9061 - accuracy: 0.9846\n",
      "Epoch 00162: val_loss did not improve from 3.29350\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.8664 - accuracy: 0.9846 - val_loss: 4.6563 - val_accuracy: 0.9905\n",
      "Epoch 163/500\n",
      "166/185 [=========================>....] - ETA: 0s - loss: 4.3897 - accuracy: 0.9866\n",
      "Epoch 00163: val_loss did not improve from 3.29350\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.3092 - accuracy: 0.9861 - val_loss: 3.8029 - val_accuracy: 0.9851\n",
      "Epoch 164/500\n",
      "176/185 [===========================>..] - ETA: 0s - loss: 4.6910 - accuracy: 0.9863\n",
      "Epoch 00164: val_loss did not improve from 3.29350\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.7539 - accuracy: 0.9863 - val_loss: 7.8429 - val_accuracy: 0.9830\n",
      "Epoch 165/500\n",
      "177/185 [===========================>..] - ETA: 0s - loss: 5.2192 - accuracy: 0.9836\n",
      "Epoch 00165: val_loss did not improve from 3.29350\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.1695 - accuracy: 0.9841 - val_loss: 3.6537 - val_accuracy: 0.9905\n",
      "Epoch 166/500\n",
      "166/185 [=========================>....] - ETA: 0s - loss: 4.8553 - accuracy: 0.9821\n",
      "Epoch 00166: val_loss did not improve from 3.29350\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.9039 - accuracy: 0.9815 - val_loss: 4.2328 - val_accuracy: 0.9871\n",
      "Epoch 167/500\n",
      "177/185 [===========================>..] - ETA: 0s - loss: 4.4086 - accuracy: 0.9843\n",
      "Epoch 00167: val_loss did not improve from 3.29350\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.4719 - accuracy: 0.9844 - val_loss: 4.9826 - val_accuracy: 0.9844\n",
      "Epoch 168/500\n",
      "177/185 [===========================>..] - ETA: 0s - loss: 4.6963 - accuracy: 0.9841\n",
      "Epoch 00168: val_loss did not improve from 3.29350\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.6825 - accuracy: 0.9846 - val_loss: 7.8141 - val_accuracy: 0.9817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169/500\n",
      "180/185 [============================>.] - ETA: 0s - loss: 4.6615 - accuracy: 0.9849\n",
      "Epoch 00169: val_loss did not improve from 3.29350\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.7129 - accuracy: 0.9852 - val_loss: 8.9744 - val_accuracy: 0.9661\n",
      "Epoch 170/500\n",
      "170/185 [==========================>...] - ETA: 0s - loss: 5.3892 - accuracy: 0.9847\n",
      "Epoch 00170: val_loss did not improve from 3.29350\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.3092 - accuracy: 0.9835 - val_loss: 4.9993 - val_accuracy: 0.9858\n",
      "Epoch 171/500\n",
      "181/185 [============================>.] - ETA: 0s - loss: 4.5839 - accuracy: 0.9860\n",
      "Epoch 00171: val_loss did not improve from 3.29350\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.5863 - accuracy: 0.9861 - val_loss: 4.9710 - val_accuracy: 0.9912\n",
      "Epoch 172/500\n",
      "182/185 [============================>.] - ETA: 0s - loss: 4.4750 - accuracy: 0.9828\n",
      "Epoch 00172: val_loss did not improve from 3.29350\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.4586 - accuracy: 0.9830 - val_loss: 4.0566 - val_accuracy: 0.9878\n",
      "Epoch 173/500\n",
      "166/185 [=========================>....] - ETA: 0s - loss: 4.3433 - accuracy: 0.9848\n",
      "Epoch 00173: val_loss did not improve from 3.29350\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.4385 - accuracy: 0.9841 - val_loss: 5.9028 - val_accuracy: 0.9756\n",
      "Epoch 174/500\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 4.7583 - accuracy: 0.9836\n",
      "Epoch 00174: val_loss did not improve from 3.29350\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.7734 - accuracy: 0.9842 - val_loss: 4.7112 - val_accuracy: 0.9878\n",
      "Epoch 175/500\n",
      "184/185 [============================>.] - ETA: 0s - loss: 4.4767 - accuracy: 0.9852\n",
      "Epoch 00175: val_loss did not improve from 3.29350\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.4735 - accuracy: 0.9852 - val_loss: 4.6559 - val_accuracy: 0.9783\n",
      "Epoch 176/500\n",
      "176/185 [===========================>..] - ETA: 0s - loss: 4.7255 - accuracy: 0.9860\n",
      "Epoch 00176: val_loss did not improve from 3.29350\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.6972 - accuracy: 0.9863 - val_loss: 5.5619 - val_accuracy: 0.9898\n",
      "Epoch 177/500\n",
      "176/185 [===========================>..] - ETA: 0s - loss: 4.5194 - accuracy: 0.9851\n",
      "Epoch 00177: val_loss did not improve from 3.29350\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.5275 - accuracy: 0.9851 - val_loss: 5.5846 - val_accuracy: 0.9776\n",
      "Epoch 178/500\n",
      "175/185 [===========================>..] - ETA: 0s - loss: 4.9756 - accuracy: 0.9843\n",
      "Epoch 00178: val_loss did not improve from 3.29350\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 5.0024 - accuracy: 0.9832 - val_loss: 5.9730 - val_accuracy: 0.9864\n",
      "Epoch 179/500\n",
      "184/185 [============================>.] - ETA: 0s - loss: 4.6496 - accuracy: 0.9832\n",
      "Epoch 00179: val_loss did not improve from 3.29350\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.6536 - accuracy: 0.9832 - val_loss: 5.8792 - val_accuracy: 0.9912\n",
      "Epoch 180/500\n",
      "182/185 [============================>.] - ETA: 0s - loss: 4.5612 - accuracy: 0.9851\n",
      "Epoch 00180: val_loss did not improve from 3.29350\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.5841 - accuracy: 0.9851 - val_loss: 4.3412 - val_accuracy: 0.9898\n",
      "Epoch 181/500\n",
      "173/185 [===========================>..] - ETA: 0s - loss: 4.1631 - accuracy: 0.9854\n",
      "Epoch 00181: val_loss did not improve from 3.29350\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.1459 - accuracy: 0.9852 - val_loss: 4.9311 - val_accuracy: 0.9932\n",
      "Epoch 182/500\n",
      "157/185 [========================>.....] - ETA: 0s - loss: 4.3809 - accuracy: 0.9861\n",
      "Epoch 00182: val_loss did not improve from 3.29350\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.3322 - accuracy: 0.9851 - val_loss: 4.8634 - val_accuracy: 0.9898\n",
      "Epoch 183/500\n",
      "178/185 [===========================>..] - ETA: 0s - loss: 4.3277 - accuracy: 0.9853\n",
      "Epoch 00183: val_loss did not improve from 3.29350\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.3300 - accuracy: 0.9844 - val_loss: 4.8331 - val_accuracy: 0.9885\n",
      "Epoch 184/500\n",
      "180/185 [============================>.] - ETA: 0s - loss: 4.0535 - accuracy: 0.9870\n",
      "Epoch 00184: val_loss did not improve from 3.29350\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.0536 - accuracy: 0.9869 - val_loss: 4.6658 - val_accuracy: 0.9885\n",
      "Epoch 185/500\n",
      "169/185 [==========================>...] - ETA: 0s - loss: 4.1125 - accuracy: 0.9863\n",
      "Epoch 00185: val_loss did not improve from 3.29350\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.1126 - accuracy: 0.9858 - val_loss: 4.0468 - val_accuracy: 0.9858\n",
      "Epoch 186/500\n",
      "163/185 [=========================>....] - ETA: 0s - loss: 4.7856 - accuracy: 0.9845\n",
      "Epoch 00186: val_loss did not improve from 3.29350\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.6962 - accuracy: 0.9849 - val_loss: 4.2366 - val_accuracy: 0.9810\n",
      "Epoch 187/500\n",
      "179/185 [============================>.] - ETA: 0s - loss: 4.4877 - accuracy: 0.9855\n",
      "Epoch 00187: val_loss did not improve from 3.29350\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.5200 - accuracy: 0.9858 - val_loss: 5.1262 - val_accuracy: 0.9817\n",
      "Epoch 188/500\n",
      "172/185 [==========================>...] - ETA: 0s - loss: 4.0438 - accuracy: 0.9871\n",
      "Epoch 00188: val_loss did not improve from 3.29350\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.0460 - accuracy: 0.9871 - val_loss: 8.0261 - val_accuracy: 0.9735\n",
      "Epoch 189/500\n",
      "170/185 [==========================>...] - ETA: 0s - loss: 4.7725 - accuracy: 0.9873\n",
      "Epoch 00189: val_loss did not improve from 3.29350\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.6726 - accuracy: 0.9873 - val_loss: 6.0332 - val_accuracy: 0.9783\n",
      "Epoch 190/500\n",
      "167/185 [==========================>...] - ETA: 0s - loss: 4.6143 - accuracy: 0.9835\n",
      "Epoch 00190: val_loss improved from 3.29350 to 3.14561, saving model to random_split\\Weights-190--3.14561.hdf5\n",
      "185/185 [==============================] - 0s 2ms/step - loss: 4.5222 - accuracy: 0.9841 - val_loss: 3.1456 - val_accuracy: 0.9912\n",
      "Epoch 191/500\n",
      "173/185 [===========================>..] - ETA: 0s - loss: 4.3936 - accuracy: 0.9827\n",
      "Epoch 00191: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.3591 - accuracy: 0.9829 - val_loss: 4.8633 - val_accuracy: 0.9905\n",
      "Epoch 192/500\n",
      "182/185 [============================>.] - ETA: 0s - loss: 4.4120 - accuracy: 0.9845\n",
      "Epoch 00192: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.4041 - accuracy: 0.9847 - val_loss: 3.9214 - val_accuracy: 0.9878\n",
      "Epoch 193/500\n",
      "175/185 [===========================>..] - ETA: 0s - loss: 4.5442 - accuracy: 0.9848\n",
      "Epoch 00193: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.5616 - accuracy: 0.9847 - val_loss: 6.3364 - val_accuracy: 0.9803\n",
      "Epoch 194/500\n",
      "174/185 [===========================>..] - ETA: 0s - loss: 4.2316 - accuracy: 0.9837\n",
      "Epoch 00194: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.2186 - accuracy: 0.9834 - val_loss: 3.8534 - val_accuracy: 0.9932\n",
      "Epoch 195/500\n",
      "167/185 [==========================>...] - ETA: 0s - loss: 4.2468 - accuracy: 0.9878\n",
      "Epoch 00195: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.3814 - accuracy: 0.9869 - val_loss: 5.4496 - val_accuracy: 0.9885\n",
      "Epoch 196/500\n",
      "178/185 [===========================>..] - ETA: 0s - loss: 4.3264 - accuracy: 0.9856\n",
      "Epoch 00196: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.3104 - accuracy: 0.9856 - val_loss: 4.6025 - val_accuracy: 0.9837\n",
      "Epoch 197/500\n",
      "180/185 [============================>.] - ETA: 0s - loss: 4.7658 - accuracy: 0.9812\n",
      "Epoch 00197: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.7576 - accuracy: 0.9812 - val_loss: 6.6147 - val_accuracy: 0.9824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 198/500\n",
      "159/185 [========================>.....] - ETA: 0s - loss: 4.1269 - accuracy: 0.9874\n",
      "Epoch 00198: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.3121 - accuracy: 0.9868 - val_loss: 6.4235 - val_accuracy: 0.9885\n",
      "Epoch 199/500\n",
      "182/185 [============================>.] - ETA: 0s - loss: 4.3983 - accuracy: 0.9851\n",
      "Epoch 00199: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.3887 - accuracy: 0.9847 - val_loss: 3.4636 - val_accuracy: 0.9891\n",
      "Epoch 200/500\n",
      "172/185 [==========================>...] - ETA: 0s - loss: 3.9352 - accuracy: 0.9864\n",
      "Epoch 00200: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.8954 - accuracy: 0.9863 - val_loss: 3.7549 - val_accuracy: 0.9919\n",
      "Epoch 201/500\n",
      "166/185 [=========================>....] - ETA: 0s - loss: 4.3357 - accuracy: 0.9859\n",
      "Epoch 00201: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.4741 - accuracy: 0.9852 - val_loss: 6.0388 - val_accuracy: 0.9939\n",
      "Epoch 202/500\n",
      "175/185 [===========================>..] - ETA: 0s - loss: 4.7464 - accuracy: 0.9857\n",
      "Epoch 00202: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.6814 - accuracy: 0.9861 - val_loss: 6.0306 - val_accuracy: 0.9885\n",
      "Epoch 203/500\n",
      "176/185 [===========================>..] - ETA: 0s - loss: 4.4560 - accuracy: 0.9844\n",
      "Epoch 00203: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.4716 - accuracy: 0.9846 - val_loss: 6.9224 - val_accuracy: 0.9796\n",
      "Epoch 204/500\n",
      "170/185 [==========================>...] - ETA: 0s - loss: 4.9173 - accuracy: 0.9862\n",
      "Epoch 00204: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.9757 - accuracy: 0.9864 - val_loss: 4.5786 - val_accuracy: 0.9905\n",
      "Epoch 205/500\n",
      "159/185 [========================>.....] - ETA: 0s - loss: 4.2743 - accuracy: 0.9866\n",
      "Epoch 00205: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.2534 - accuracy: 0.9852 - val_loss: 5.8890 - val_accuracy: 0.9844\n",
      "Epoch 206/500\n",
      "179/185 [============================>.] - ETA: 0s - loss: 4.5085 - accuracy: 0.9855\n",
      "Epoch 00206: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.5060 - accuracy: 0.9856 - val_loss: 5.7521 - val_accuracy: 0.9858\n",
      "Epoch 207/500\n",
      "172/185 [==========================>...] - ETA: 0s - loss: 3.7134 - accuracy: 0.9860\n",
      "Epoch 00207: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.7334 - accuracy: 0.9856 - val_loss: 3.5610 - val_accuracy: 0.9946\n",
      "Epoch 208/500\n",
      "180/185 [============================>.] - ETA: 0s - loss: 3.9790 - accuracy: 0.9866\n",
      "Epoch 00208: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.0240 - accuracy: 0.9863 - val_loss: 5.0442 - val_accuracy: 0.9912\n",
      "Epoch 209/500\n",
      "175/185 [===========================>..] - ETA: 0s - loss: 4.0871 - accuracy: 0.9862\n",
      "Epoch 00209: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.1529 - accuracy: 0.9863 - val_loss: 5.3172 - val_accuracy: 0.9864\n",
      "Epoch 210/500\n",
      "176/185 [===========================>..] - ETA: 0s - loss: 4.2664 - accuracy: 0.9846\n",
      "Epoch 00210: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.3257 - accuracy: 0.9842 - val_loss: 3.6663 - val_accuracy: 0.9912\n",
      "Epoch 211/500\n",
      "166/185 [=========================>....] - ETA: 0s - loss: 4.4854 - accuracy: 0.9846\n",
      "Epoch 00211: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.4262 - accuracy: 0.9847 - val_loss: 3.6264 - val_accuracy: 0.9905\n",
      "Epoch 212/500\n",
      "182/185 [============================>.] - ETA: 0s - loss: 3.9395 - accuracy: 0.9873\n",
      "Epoch 00212: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.9321 - accuracy: 0.9871 - val_loss: 4.2475 - val_accuracy: 0.9776\n",
      "Epoch 213/500\n",
      "178/185 [===========================>..] - ETA: 0s - loss: 4.5991 - accuracy: 0.9828\n",
      "Epoch 00213: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.5869 - accuracy: 0.9829 - val_loss: 6.5602 - val_accuracy: 0.9749\n",
      "Epoch 214/500\n",
      "172/185 [==========================>...] - ETA: 0s - loss: 4.0615 - accuracy: 0.9864\n",
      "Epoch 00214: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.0925 - accuracy: 0.9863 - val_loss: 4.8708 - val_accuracy: 0.9898\n",
      "Epoch 215/500\n",
      "172/185 [==========================>...] - ETA: 0s - loss: 4.4945 - accuracy: 0.9836\n",
      "Epoch 00215: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.4937 - accuracy: 0.9834 - val_loss: 8.2242 - val_accuracy: 0.9749\n",
      "Epoch 216/500\n",
      "181/185 [============================>.] - ETA: 0s - loss: 4.4508 - accuracy: 0.9839\n",
      "Epoch 00216: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.4602 - accuracy: 0.9841 - val_loss: 5.3493 - val_accuracy: 0.9844\n",
      "Epoch 217/500\n",
      "170/185 [==========================>...] - ETA: 0s - loss: 4.7956 - accuracy: 0.9833\n",
      "Epoch 00217: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.7059 - accuracy: 0.9835 - val_loss: 4.6276 - val_accuracy: 0.9885\n",
      "Epoch 218/500\n",
      "180/185 [============================>.] - ETA: 0s - loss: 3.9247 - accuracy: 0.9896\n",
      "Epoch 00218: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.9180 - accuracy: 0.9898 - val_loss: 3.8800 - val_accuracy: 0.9912\n",
      "Epoch 219/500\n",
      "174/185 [===========================>..] - ETA: 0s - loss: 4.5616 - accuracy: 0.9824\n",
      "Epoch 00219: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.5139 - accuracy: 0.9827 - val_loss: 3.8251 - val_accuracy: 0.9953\n",
      "Epoch 220/500\n",
      "182/185 [============================>.] - ETA: 0s - loss: 3.7028 - accuracy: 0.9883\n",
      "Epoch 00220: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.6959 - accuracy: 0.9885 - val_loss: 3.2369 - val_accuracy: 0.9939\n",
      "Epoch 221/500\n",
      "170/185 [==========================>...] - ETA: 0s - loss: 4.5640 - accuracy: 0.9825\n",
      "Epoch 00221: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.4784 - accuracy: 0.9825 - val_loss: 4.7611 - val_accuracy: 0.9878\n",
      "Epoch 222/500\n",
      "178/185 [===========================>..] - ETA: 0s - loss: 4.1517 - accuracy: 0.9842\n",
      "Epoch 00222: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.1727 - accuracy: 0.9844 - val_loss: 5.9164 - val_accuracy: 0.9885\n",
      "Epoch 223/500\n",
      "173/185 [===========================>..] - ETA: 0s - loss: 4.5421 - accuracy: 0.9872\n",
      "Epoch 00223: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.5491 - accuracy: 0.9868 - val_loss: 4.2761 - val_accuracy: 0.9905\n",
      "Epoch 224/500\n",
      "180/185 [============================>.] - ETA: 0s - loss: 4.0861 - accuracy: 0.9866\n",
      "Epoch 00224: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.0732 - accuracy: 0.9864 - val_loss: 4.8890 - val_accuracy: 0.9891\n",
      "Epoch 225/500\n",
      "169/185 [==========================>...] - ETA: 0s - loss: 4.7551 - accuracy: 0.9848\n",
      "Epoch 00225: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.6440 - accuracy: 0.9856 - val_loss: 5.7186 - val_accuracy: 0.9885\n",
      "Epoch 226/500\n",
      "177/185 [===========================>..] - ETA: 0s - loss: 4.4699 - accuracy: 0.9846\n",
      "Epoch 00226: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.4753 - accuracy: 0.9847 - val_loss: 6.6263 - val_accuracy: 0.9837\n",
      "Epoch 227/500\n",
      "168/185 [==========================>...] - ETA: 0s - loss: 4.6667 - accuracy: 0.9829\n",
      "Epoch 00227: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.5495 - accuracy: 0.9832 - val_loss: 4.2145 - val_accuracy: 0.9837\n",
      "Epoch 228/500\n",
      "180/185 [============================>.] - ETA: 0s - loss: 4.6443 - accuracy: 0.9849\n",
      "Epoch 00228: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.6259 - accuracy: 0.9849 - val_loss: 4.0122 - val_accuracy: 0.9912\n",
      "Epoch 229/500\n",
      "179/185 [============================>.] - ETA: 0s - loss: 4.5188 - accuracy: 0.9843\n",
      "Epoch 00229: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.4880 - accuracy: 0.9842 - val_loss: 3.8060 - val_accuracy: 0.9830\n",
      "Epoch 230/500\n",
      "172/185 [==========================>...] - ETA: 0s - loss: 3.9509 - accuracy: 0.9860\n",
      "Epoch 00230: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.0493 - accuracy: 0.9854 - val_loss: 7.0157 - val_accuracy: 0.9756\n",
      "Epoch 231/500\n",
      "184/185 [============================>.] - ETA: 0s - loss: 4.3666 - accuracy: 0.9857\n",
      "Epoch 00231: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.3650 - accuracy: 0.9858 - val_loss: 8.0131 - val_accuracy: 0.9722\n",
      "Epoch 232/500\n",
      "173/185 [===========================>..] - ETA: 0s - loss: 4.2511 - accuracy: 0.9857\n",
      "Epoch 00232: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.2316 - accuracy: 0.9851 - val_loss: 3.7608 - val_accuracy: 0.9898\n",
      "Epoch 233/500\n",
      "162/185 [=========================>....] - ETA: 0s - loss: 4.1576 - accuracy: 0.9851\n",
      "Epoch 00233: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.1504 - accuracy: 0.9852 - val_loss: 4.1069 - val_accuracy: 0.9925\n",
      "Epoch 234/500\n",
      "168/185 [==========================>...] - ETA: 0s - loss: 4.1680 - accuracy: 0.9844\n",
      "Epoch 00234: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.2035 - accuracy: 0.9851 - val_loss: 4.9199 - val_accuracy: 0.9817\n",
      "Epoch 235/500\n",
      "180/185 [============================>.] - ETA: 0s - loss: 4.3603 - accuracy: 0.9861\n",
      "Epoch 00235: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.3447 - accuracy: 0.9861 - val_loss: 3.7377 - val_accuracy: 0.9837\n",
      "Epoch 236/500\n",
      "171/185 [==========================>...] - ETA: 0s - loss: 4.5569 - accuracy: 0.9859\n",
      "Epoch 00236: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.5325 - accuracy: 0.9859 - val_loss: 3.6670 - val_accuracy: 0.9871\n",
      "Epoch 237/500\n",
      "172/185 [==========================>...] - ETA: 0s - loss: 3.9330 - accuracy: 0.9878\n",
      "Epoch 00237: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.0328 - accuracy: 0.9883 - val_loss: 5.1890 - val_accuracy: 0.9817\n",
      "Epoch 238/500\n",
      "171/185 [==========================>...] - ETA: 0s - loss: 4.0399 - accuracy: 0.9850\n",
      "Epoch 00238: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.9978 - accuracy: 0.9847 - val_loss: 4.0113 - val_accuracy: 0.9919\n",
      "Epoch 239/500\n",
      "176/185 [===========================>..] - ETA: 0s - loss: 4.0604 - accuracy: 0.9886\n",
      "Epoch 00239: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.0419 - accuracy: 0.9890 - val_loss: 4.3383 - val_accuracy: 0.9925\n",
      "Epoch 240/500\n",
      "180/185 [============================>.] - ETA: 0s - loss: 4.5743 - accuracy: 0.9842\n",
      "Epoch 00240: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.6044 - accuracy: 0.9844 - val_loss: 6.1176 - val_accuracy: 0.9796\n",
      "Epoch 241/500\n",
      "168/185 [==========================>...] - ETA: 0s - loss: 4.1747 - accuracy: 0.9874\n",
      "Epoch 00241: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.2106 - accuracy: 0.9873 - val_loss: 4.0450 - val_accuracy: 0.9837\n",
      "Epoch 242/500\n",
      "181/185 [============================>.] - ETA: 0s - loss: 4.2888 - accuracy: 0.9869\n",
      "Epoch 00242: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.3072 - accuracy: 0.9869 - val_loss: 4.0159 - val_accuracy: 0.9844\n",
      "Epoch 243/500\n",
      "181/185 [============================>.] - ETA: 0s - loss: 4.7857 - accuracy: 0.9858\n",
      "Epoch 00243: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.7650 - accuracy: 0.9861 - val_loss: 4.3648 - val_accuracy: 0.9912\n",
      "Epoch 244/500\n",
      "164/185 [=========================>....] - ETA: 0s - loss: 4.1202 - accuracy: 0.9874\n",
      "Epoch 00244: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.1034 - accuracy: 0.9871 - val_loss: 4.1972 - val_accuracy: 0.9864\n",
      "Epoch 245/500\n",
      "173/185 [===========================>..] - ETA: 0s - loss: 4.0461 - accuracy: 0.9855\n",
      "Epoch 00245: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.1159 - accuracy: 0.9863 - val_loss: 4.7244 - val_accuracy: 0.9871\n",
      "Epoch 246/500\n",
      "175/185 [===========================>..] - ETA: 0s - loss: 4.1584 - accuracy: 0.9864\n",
      "Epoch 00246: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.1589 - accuracy: 0.9866 - val_loss: 3.3806 - val_accuracy: 0.9919\n",
      "Epoch 247/500\n",
      "181/185 [============================>.] - ETA: 0s - loss: 4.5469 - accuracy: 0.9852\n",
      "Epoch 00247: val_loss did not improve from 3.14561\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.5254 - accuracy: 0.9849 - val_loss: 4.2710 - val_accuracy: 0.9939\n",
      "Epoch 248/500\n",
      "178/185 [===========================>..] - ETA: 0s - loss: 3.7486 - accuracy: 0.9865\n",
      "Epoch 00248: val_loss improved from 3.14561 to 3.11158, saving model to random_split\\Weights-248--3.11158.hdf5\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.7277 - accuracy: 0.9863 - val_loss: 3.1116 - val_accuracy: 0.9891\n",
      "Epoch 249/500\n",
      "165/185 [=========================>....] - ETA: 0s - loss: 4.0930 - accuracy: 0.9848\n",
      "Epoch 00249: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.1215 - accuracy: 0.9854 - val_loss: 4.0855 - val_accuracy: 0.9871\n",
      "Epoch 250/500\n",
      "177/185 [===========================>..] - ETA: 0s - loss: 3.8031 - accuracy: 0.9855\n",
      "Epoch 00250: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.7861 - accuracy: 0.9858 - val_loss: 4.0750 - val_accuracy: 0.9891\n",
      "Epoch 251/500\n",
      "181/185 [============================>.] - ETA: 0s - loss: 4.1980 - accuracy: 0.9891\n",
      "Epoch 00251: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.1846 - accuracy: 0.9891 - val_loss: 3.6762 - val_accuracy: 0.9905\n",
      "Epoch 252/500\n",
      "172/185 [==========================>...] - ETA: 0s - loss: 4.3598 - accuracy: 0.9867\n",
      "Epoch 00252: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.2886 - accuracy: 0.9869 - val_loss: 3.6855 - val_accuracy: 0.9912\n",
      "Epoch 253/500\n",
      "174/185 [===========================>..] - ETA: 0s - loss: 4.2647 - accuracy: 0.9860\n",
      "Epoch 00253: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.3662 - accuracy: 0.9856 - val_loss: 5.6686 - val_accuracy: 0.9783\n",
      "Epoch 254/500\n",
      "180/185 [============================>.] - ETA: 0s - loss: 4.2297 - accuracy: 0.9858\n",
      "Epoch 00254: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.2381 - accuracy: 0.9859 - val_loss: 3.9442 - val_accuracy: 0.9932\n",
      "Epoch 255/500\n",
      "175/185 [===========================>..] - ETA: 0s - loss: 4.0639 - accuracy: 0.9895\n",
      "Epoch 00255: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.0396 - accuracy: 0.9891 - val_loss: 3.2629 - val_accuracy: 0.9905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 256/500\n",
      "176/185 [===========================>..] - ETA: 0s - loss: 4.0443 - accuracy: 0.9872\n",
      "Epoch 00256: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.0059 - accuracy: 0.9873 - val_loss: 3.2499 - val_accuracy: 0.9939\n",
      "Epoch 257/500\n",
      "169/185 [==========================>...] - ETA: 0s - loss: 4.2938 - accuracy: 0.9854\n",
      "Epoch 00257: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.3247 - accuracy: 0.9854 - val_loss: 5.1563 - val_accuracy: 0.9871\n",
      "Epoch 258/500\n",
      "178/185 [===========================>..] - ETA: 0s - loss: 4.0896 - accuracy: 0.9874\n",
      "Epoch 00258: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.0942 - accuracy: 0.9871 - val_loss: 3.6791 - val_accuracy: 0.9925\n",
      "Epoch 259/500\n",
      "181/185 [============================>.] - ETA: 0s - loss: 4.1240 - accuracy: 0.9864\n",
      "Epoch 00259: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.1082 - accuracy: 0.9863 - val_loss: 5.6144 - val_accuracy: 0.9803\n",
      "Epoch 260/500\n",
      "174/185 [===========================>..] - ETA: 0s - loss: 4.4176 - accuracy: 0.9858\n",
      "Epoch 00260: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.4053 - accuracy: 0.9849 - val_loss: 4.8677 - val_accuracy: 0.9844\n",
      "Epoch 261/500\n",
      "173/185 [===========================>..] - ETA: 0s - loss: 3.9541 - accuracy: 0.9874\n",
      "Epoch 00261: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.9330 - accuracy: 0.9871 - val_loss: 5.4041 - val_accuracy: 0.9824\n",
      "Epoch 262/500\n",
      "178/185 [===========================>..] - ETA: 0s - loss: 3.9445 - accuracy: 0.9872\n",
      "Epoch 00262: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.9267 - accuracy: 0.9876 - val_loss: 3.1738 - val_accuracy: 0.9871\n",
      "Epoch 263/500\n",
      "178/185 [===========================>..] - ETA: 0s - loss: 3.9772 - accuracy: 0.9868\n",
      "Epoch 00263: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.9822 - accuracy: 0.9868 - val_loss: 5.0653 - val_accuracy: 0.9925\n",
      "Epoch 264/500\n",
      "183/185 [============================>.] - ETA: 0s - loss: 4.6701 - accuracy: 0.9836\n",
      "Epoch 00264: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.6778 - accuracy: 0.9835 - val_loss: 7.3327 - val_accuracy: 0.9803\n",
      "Epoch 265/500\n",
      "161/185 [=========================>....] - ETA: 0s - loss: 4.5905 - accuracy: 0.9819\n",
      "Epoch 00265: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.6683 - accuracy: 0.9817 - val_loss: 8.1121 - val_accuracy: 0.9824\n",
      "Epoch 266/500\n",
      "154/185 [=======================>......] - ETA: 0s - loss: 4.3086 - accuracy: 0.9836 ETA: 0s - loss: 4.4526 - accuracy: 0.\n",
      "Epoch 00266: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 2ms/step - loss: 4.2498 - accuracy: 0.9851 - val_loss: 4.8154 - val_accuracy: 0.9844\n",
      "Epoch 267/500\n",
      "171/185 [==========================>...] - ETA: 0s - loss: 4.3999 - accuracy: 0.9861\n",
      "Epoch 00267: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.3707 - accuracy: 0.9856 - val_loss: 4.1744 - val_accuracy: 0.9871\n",
      "Epoch 268/500\n",
      "179/185 [============================>.] - ETA: 0s - loss: 4.4651 - accuracy: 0.9843\n",
      "Epoch 00268: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.4588 - accuracy: 0.9846 - val_loss: 4.5615 - val_accuracy: 0.9891\n",
      "Epoch 269/500\n",
      "170/185 [==========================>...] - ETA: 0s - loss: 4.0666 - accuracy: 0.9860\n",
      "Epoch 00269: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.0758 - accuracy: 0.9856 - val_loss: 3.4165 - val_accuracy: 0.9912\n",
      "Epoch 270/500\n",
      "178/185 [===========================>..] - ETA: 0s - loss: 3.8525 - accuracy: 0.9868\n",
      "Epoch 00270: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.8603 - accuracy: 0.9868 - val_loss: 4.6071 - val_accuracy: 0.9810\n",
      "Epoch 271/500\n",
      "178/185 [===========================>..] - ETA: 0s - loss: 4.3334 - accuracy: 0.9861\n",
      "Epoch 00271: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.3697 - accuracy: 0.9861 - val_loss: 6.7466 - val_accuracy: 0.9891\n",
      "Epoch 272/500\n",
      "175/185 [===========================>..] - ETA: 0s - loss: 4.2239 - accuracy: 0.9884\n",
      "Epoch 00272: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.2197 - accuracy: 0.9881 - val_loss: 4.6116 - val_accuracy: 0.9871\n",
      "Epoch 273/500\n",
      "149/185 [=======================>......] - ETA: 0s - loss: 3.9521 - accuracy: 0.9874\n",
      "Epoch 00273: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 2ms/step - loss: 3.8885 - accuracy: 0.9878 - val_loss: 4.5844 - val_accuracy: 0.9919\n",
      "Epoch 274/500\n",
      "169/185 [==========================>...] - ETA: 0s - loss: 4.1041 - accuracy: 0.9865\n",
      "Epoch 00274: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.0889 - accuracy: 0.9869 - val_loss: 4.8783 - val_accuracy: 0.9783\n",
      "Epoch 275/500\n",
      "175/185 [===========================>..] - ETA: 0s - loss: 4.2562 - accuracy: 0.9836\n",
      "Epoch 00275: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.2803 - accuracy: 0.9837 - val_loss: 5.6971 - val_accuracy: 0.9851\n",
      "Epoch 276/500\n",
      "177/185 [===========================>..] - ETA: 0s - loss: 4.4061 - accuracy: 0.9876\n",
      "Epoch 00276: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.3822 - accuracy: 0.9876 - val_loss: 3.7706 - val_accuracy: 0.9844\n",
      "Epoch 277/500\n",
      "167/185 [==========================>...] - ETA: 0s - loss: 4.0483 - accuracy: 0.9876\n",
      "Epoch 00277: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.0521 - accuracy: 0.9878 - val_loss: 4.4627 - val_accuracy: 0.9912\n",
      "Epoch 278/500\n",
      "184/185 [============================>.] - ETA: 0s - loss: 4.2006 - accuracy: 0.9878\n",
      "Epoch 00278: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.2041 - accuracy: 0.9878 - val_loss: 3.9673 - val_accuracy: 0.9891\n",
      "Epoch 279/500\n",
      "176/185 [===========================>..] - ETA: 0s - loss: 4.3355 - accuracy: 0.9853\n",
      "Epoch 00279: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.2929 - accuracy: 0.9854 - val_loss: 7.2331 - val_accuracy: 0.9810\n",
      "Epoch 280/500\n",
      "180/185 [============================>.] - ETA: 0s - loss: 4.0633 - accuracy: 0.9859\n",
      "Epoch 00280: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.0628 - accuracy: 0.9858 - val_loss: 4.2394 - val_accuracy: 0.9891\n",
      "Epoch 281/500\n",
      "168/185 [==========================>...] - ETA: 0s - loss: 4.0624 - accuracy: 0.9874\n",
      "Epoch 00281: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.0366 - accuracy: 0.9874 - val_loss: 3.4310 - val_accuracy: 0.9919\n",
      "Epoch 282/500\n",
      "181/185 [============================>.] - ETA: 0s - loss: 3.7646 - accuracy: 0.9871\n",
      "Epoch 00282: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.7990 - accuracy: 0.9873 - val_loss: 5.1352 - val_accuracy: 0.9864\n",
      "Epoch 283/500\n",
      "176/185 [===========================>..] - ETA: 0s - loss: 4.2528 - accuracy: 0.9846\n",
      "Epoch 00283: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.2796 - accuracy: 0.9847 - val_loss: 4.4748 - val_accuracy: 0.9810\n",
      "Epoch 284/500\n",
      "174/185 [===========================>..] - ETA: 0s - loss: 4.1607 - accuracy: 0.9856\n",
      "Epoch 00284: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.2860 - accuracy: 0.9856 - val_loss: 6.7592 - val_accuracy: 0.9905\n",
      "Epoch 285/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173/185 [===========================>..] - ETA: 0s - loss: 4.0113 - accuracy: 0.9884\n",
      "Epoch 00285: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.9444 - accuracy: 0.9886 - val_loss: 3.5247 - val_accuracy: 0.9919\n",
      "Epoch 286/500\n",
      "174/185 [===========================>..] - ETA: 0s - loss: 4.1141 - accuracy: 0.9871\n",
      "Epoch 00286: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.1480 - accuracy: 0.9866 - val_loss: 4.5558 - val_accuracy: 0.9885\n",
      "Epoch 287/500\n",
      "181/185 [============================>.] - ETA: 0s - loss: 3.9872 - accuracy: 0.9850\n",
      "Epoch 00287: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.9890 - accuracy: 0.9846 - val_loss: 4.1913 - val_accuracy: 0.9885\n",
      "Epoch 288/500\n",
      "171/185 [==========================>...] - ETA: 0s - loss: 4.3163 - accuracy: 0.9836\n",
      "Epoch 00288: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.4877 - accuracy: 0.9832 - val_loss: 7.1195 - val_accuracy: 0.9545\n",
      "Epoch 289/500\n",
      "168/185 [==========================>...] - ETA: 0s - loss: 4.2950 - accuracy: 0.9844\n",
      "Epoch 00289: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.2276 - accuracy: 0.9844 - val_loss: 4.4418 - val_accuracy: 0.9864\n",
      "Epoch 290/500\n",
      "180/185 [============================>.] - ETA: 0s - loss: 3.8687 - accuracy: 0.9885\n",
      "Epoch 00290: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.8564 - accuracy: 0.9883 - val_loss: 3.8721 - val_accuracy: 0.9925\n",
      "Epoch 291/500\n",
      "174/185 [===========================>..] - ETA: 0s - loss: 3.9811 - accuracy: 0.9865\n",
      "Epoch 00291: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.9493 - accuracy: 0.9868 - val_loss: 4.4909 - val_accuracy: 0.9891\n",
      "Epoch 292/500\n",
      "176/185 [===========================>..] - ETA: 0s - loss: 4.3515 - accuracy: 0.9870\n",
      "Epoch 00292: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.3237 - accuracy: 0.9871 - val_loss: 3.7074 - val_accuracy: 0.9925\n",
      "Epoch 293/500\n",
      "168/185 [==========================>...] - ETA: 0s - loss: 4.6078 - accuracy: 0.9829\n",
      "Epoch 00293: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.5723 - accuracy: 0.9830 - val_loss: 5.1468 - val_accuracy: 0.9824\n",
      "Epoch 294/500\n",
      "175/185 [===========================>..] - ETA: 0s - loss: 3.7081 - accuracy: 0.9868\n",
      "Epoch 00294: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.7453 - accuracy: 0.9868 - val_loss: 3.6926 - val_accuracy: 0.9919\n",
      "Epoch 295/500\n",
      "179/185 [============================>.] - ETA: 0s - loss: 4.0116 - accuracy: 0.9878\n",
      "Epoch 00295: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.9926 - accuracy: 0.9878 - val_loss: 3.4062 - val_accuracy: 0.9891\n",
      "Epoch 296/500\n",
      "176/185 [===========================>..] - ETA: 0s - loss: 3.7947 - accuracy: 0.9874\n",
      "Epoch 00296: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.8047 - accuracy: 0.9878 - val_loss: 6.9769 - val_accuracy: 0.9796\n",
      "Epoch 297/500\n",
      "166/185 [=========================>....] - ETA: 0s - loss: 4.2579 - accuracy: 0.9853\n",
      "Epoch 00297: val_loss did not improve from 3.11158\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.2980 - accuracy: 0.9852 - val_loss: 6.1704 - val_accuracy: 0.9824\n",
      "Epoch 298/500\n",
      "182/185 [============================>.] - ETA: 0s - loss: 3.9135 - accuracy: 0.9852\n",
      "Epoch 00298: val_loss improved from 3.11158 to 2.78399, saving model to random_split\\Weights-298--2.78399.hdf5\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.9077 - accuracy: 0.9852 - val_loss: 2.7840 - val_accuracy: 0.9905\n",
      "Epoch 299/500\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 4.0732 - accuracy: 0.9857\n",
      "Epoch 00299: val_loss did not improve from 2.78399\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.9483 - accuracy: 0.9856 - val_loss: 7.0801 - val_accuracy: 0.9613\n",
      "Epoch 300/500\n",
      "182/185 [============================>.] - ETA: 0s - loss: 4.0632 - accuracy: 0.9859\n",
      "Epoch 00300: val_loss did not improve from 2.78399\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.0677 - accuracy: 0.9858 - val_loss: 5.3250 - val_accuracy: 0.9858\n",
      "Epoch 301/500\n",
      "172/185 [==========================>...] - ETA: 0s - loss: 3.6423 - accuracy: 0.9864\n",
      "Epoch 00301: val_loss did not improve from 2.78399\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.6874 - accuracy: 0.9864 - val_loss: 3.6607 - val_accuracy: 0.9830\n",
      "Epoch 302/500\n",
      "172/185 [==========================>...] - ETA: 0s - loss: 3.6872 - accuracy: 0.9875\n",
      "Epoch 00302: val_loss did not improve from 2.78399\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.6764 - accuracy: 0.9871 - val_loss: 3.0233 - val_accuracy: 0.9919\n",
      "Epoch 303/500\n",
      "180/185 [============================>.] - ETA: 0s - loss: 3.9399 - accuracy: 0.9861\n",
      "Epoch 00303: val_loss did not improve from 2.78399\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.9517 - accuracy: 0.9863 - val_loss: 5.4763 - val_accuracy: 0.9919\n",
      "Epoch 304/500\n",
      "181/185 [============================>.] - ETA: 0s - loss: 3.6818 - accuracy: 0.9877\n",
      "Epoch 00304: val_loss did not improve from 2.78399\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.6794 - accuracy: 0.9878 - val_loss: 3.2742 - val_accuracy: 0.9905\n",
      "Epoch 305/500\n",
      "176/185 [===========================>..] - ETA: 0s - loss: 3.7741 - accuracy: 0.9885\n",
      "Epoch 00305: val_loss did not improve from 2.78399\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.7778 - accuracy: 0.9876 - val_loss: 5.5764 - val_accuracy: 0.9837\n",
      "Epoch 306/500\n",
      "182/185 [============================>.] - ETA: 0s - loss: 3.8148 - accuracy: 0.9868\n",
      "Epoch 00306: val_loss did not improve from 2.78399\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.8138 - accuracy: 0.9868 - val_loss: 4.3112 - val_accuracy: 0.9898\n",
      "Epoch 307/500\n",
      "181/185 [============================>.] - ETA: 0s - loss: 4.1014 - accuracy: 0.9869\n",
      "Epoch 00307: val_loss did not improve from 2.78399\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.0907 - accuracy: 0.9871 - val_loss: 4.2342 - val_accuracy: 0.9885\n",
      "Epoch 308/500\n",
      "177/185 [===========================>..] - ETA: 0s - loss: 3.8959 - accuracy: 0.9861\n",
      "Epoch 00308: val_loss did not improve from 2.78399\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.8659 - accuracy: 0.9863 - val_loss: 3.0540 - val_accuracy: 0.9891\n",
      "Epoch 309/500\n",
      "168/185 [==========================>...] - ETA: 0s - loss: 3.5535 - accuracy: 0.9888\n",
      "Epoch 00309: val_loss did not improve from 2.78399\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.6111 - accuracy: 0.9886 - val_loss: 4.4339 - val_accuracy: 0.9919\n",
      "Epoch 310/500\n",
      "181/185 [============================>.] - ETA: 0s - loss: 4.3675 - accuracy: 0.9850\n",
      "Epoch 00310: val_loss did not improve from 2.78399\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.3677 - accuracy: 0.9846 - val_loss: 5.5500 - val_accuracy: 0.9858\n",
      "Epoch 311/500\n",
      "181/185 [============================>.] - ETA: 0s - loss: 4.1297 - accuracy: 0.9871\n",
      "Epoch 00311: val_loss did not improve from 2.78399\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.1261 - accuracy: 0.9871 - val_loss: 3.6752 - val_accuracy: 0.9898\n",
      "Epoch 312/500\n",
      "179/185 [============================>.] - ETA: 0s - loss: 3.7263 - accuracy: 0.9871\n",
      "Epoch 00312: val_loss did not improve from 2.78399\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.7464 - accuracy: 0.9869 - val_loss: 4.9013 - val_accuracy: 0.9891\n",
      "Epoch 313/500\n",
      "174/185 [===========================>..] - ETA: 0s - loss: 3.7239 - accuracy: 0.9874\n",
      "Epoch 00313: val_loss improved from 2.78399 to 2.65493, saving model to random_split\\Weights-313--2.65493.hdf5\n",
      "185/185 [==============================] - 0s 2ms/step - loss: 3.6835 - accuracy: 0.9871 - val_loss: 2.6549 - val_accuracy: 0.9932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 314/500\n",
      "172/185 [==========================>...] - ETA: 0s - loss: 4.1936 - accuracy: 0.9847\n",
      "Epoch 00314: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.1636 - accuracy: 0.9849 - val_loss: 5.4995 - val_accuracy: 0.9796\n",
      "Epoch 315/500\n",
      "180/185 [============================>.] - ETA: 0s - loss: 3.9980 - accuracy: 0.9859\n",
      "Epoch 00315: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.9753 - accuracy: 0.9861 - val_loss: 3.5164 - val_accuracy: 0.9905\n",
      "Epoch 316/500\n",
      "179/185 [============================>.] - ETA: 0s - loss: 3.7594 - accuracy: 0.9869\n",
      "Epoch 00316: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.7550 - accuracy: 0.9868 - val_loss: 3.7676 - val_accuracy: 0.9905\n",
      "Epoch 317/500\n",
      "183/185 [============================>.] - ETA: 0s - loss: 3.8333 - accuracy: 0.9863\n",
      "Epoch 00317: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.8321 - accuracy: 0.9864 - val_loss: 3.3208 - val_accuracy: 0.9885\n",
      "Epoch 318/500\n",
      "180/185 [============================>.] - ETA: 0s - loss: 4.1072 - accuracy: 0.9866\n",
      "Epoch 00318: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.1012 - accuracy: 0.9863 - val_loss: 4.1513 - val_accuracy: 0.9858\n",
      "Epoch 319/500\n",
      "179/185 [============================>.] - ETA: 0s - loss: 4.0704 - accuracy: 0.9862\n",
      "Epoch 00319: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.0714 - accuracy: 0.9864 - val_loss: 3.9510 - val_accuracy: 0.9824\n",
      "Epoch 320/500\n",
      "176/185 [===========================>..] - ETA: 0s - loss: 3.6324 - accuracy: 0.9885\n",
      "Epoch 00320: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.6569 - accuracy: 0.9883 - val_loss: 3.7580 - val_accuracy: 0.9891\n",
      "Epoch 321/500\n",
      "180/185 [============================>.] - ETA: 0s - loss: 4.0871 - accuracy: 0.9872\n",
      "Epoch 00321: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.0781 - accuracy: 0.9868 - val_loss: 4.1140 - val_accuracy: 0.9851\n",
      "Epoch 322/500\n",
      "173/185 [===========================>..] - ETA: 0s - loss: 4.1543 - accuracy: 0.9877\n",
      "Epoch 00322: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.2012 - accuracy: 0.9874 - val_loss: 6.0174 - val_accuracy: 0.9803\n",
      "Epoch 323/500\n",
      "182/185 [============================>.] - ETA: 0s - loss: 4.2239 - accuracy: 0.9863\n",
      "Epoch 00323: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.2491 - accuracy: 0.9861 - val_loss: 5.6931 - val_accuracy: 0.9864\n",
      "Epoch 324/500\n",
      "182/185 [============================>.] - ETA: 0s - loss: 4.2678 - accuracy: 0.9864\n",
      "Epoch 00324: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.2716 - accuracy: 0.9864 - val_loss: 4.8058 - val_accuracy: 0.9946\n",
      "Epoch 325/500\n",
      "173/185 [===========================>..] - ETA: 0s - loss: 4.0991 - accuracy: 0.9852\n",
      "Epoch 00325: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.0736 - accuracy: 0.9858 - val_loss: 3.1522 - val_accuracy: 0.9912\n",
      "Epoch 326/500\n",
      "163/185 [=========================>....] - ETA: 0s - loss: 4.1861 - accuracy: 0.9866\n",
      "Epoch 00326: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.3806 - accuracy: 0.9869 - val_loss: 6.3172 - val_accuracy: 0.9796\n",
      "Epoch 327/500\n",
      "169/185 [==========================>...] - ETA: 0s - loss: 3.7656 - accuracy: 0.9871\n",
      "Epoch 00327: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.8085 - accuracy: 0.9861 - val_loss: 4.4057 - val_accuracy: 0.9858\n",
      "Epoch 328/500\n",
      "184/185 [============================>.] - ETA: 0s - loss: 3.9558 - accuracy: 0.9868\n",
      "Epoch 00328: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.9558 - accuracy: 0.9868 - val_loss: 3.5388 - val_accuracy: 0.9891\n",
      "Epoch 329/500\n",
      "177/185 [===========================>..] - ETA: 0s - loss: 4.1733 - accuracy: 0.9868\n",
      "Epoch 00329: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.1934 - accuracy: 0.9866 - val_loss: 5.2839 - val_accuracy: 0.9830\n",
      "Epoch 330/500\n",
      "179/185 [============================>.] - ETA: 0s - loss: 3.9039 - accuracy: 0.9871\n",
      "Epoch 00330: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.8800 - accuracy: 0.9871 - val_loss: 3.9393 - val_accuracy: 0.9885\n",
      "Epoch 331/500\n",
      "182/185 [============================>.] - ETA: 0s - loss: 4.1629 - accuracy: 0.9861\n",
      "Epoch 00331: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.1544 - accuracy: 0.9861 - val_loss: 4.1748 - val_accuracy: 0.9953\n",
      "Epoch 332/500\n",
      "179/185 [============================>.] - ETA: 0s - loss: 3.7298 - accuracy: 0.9885\n",
      "Epoch 00332: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.7281 - accuracy: 0.9883 - val_loss: 5.2309 - val_accuracy: 0.9844\n",
      "Epoch 333/500\n",
      "170/185 [==========================>...] - ETA: 0s - loss: 3.7843 - accuracy: 0.9897\n",
      "Epoch 00333: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.7630 - accuracy: 0.9893 - val_loss: 3.4531 - val_accuracy: 0.9919\n",
      "Epoch 334/500\n",
      "179/185 [============================>.] - ETA: 0s - loss: 3.7085 - accuracy: 0.9888\n",
      "Epoch 00334: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.7082 - accuracy: 0.9891 - val_loss: 4.7462 - val_accuracy: 0.9912\n",
      "Epoch 335/500\n",
      "171/185 [==========================>...] - ETA: 0s - loss: 3.5903 - accuracy: 0.9881\n",
      "Epoch 00335: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.5478 - accuracy: 0.9881 - val_loss: 2.6571 - val_accuracy: 0.9939\n",
      "Epoch 336/500\n",
      "182/185 [============================>.] - ETA: 0s - loss: 3.8120 - accuracy: 0.9875\n",
      "Epoch 00336: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.8005 - accuracy: 0.9874 - val_loss: 3.2257 - val_accuracy: 0.9837\n",
      "Epoch 337/500\n",
      "165/185 [=========================>....] - ETA: 0s - loss: 3.5607 - accuracy: 0.9854\n",
      "Epoch 00337: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.7396 - accuracy: 0.9852 - val_loss: 6.2373 - val_accuracy: 0.9790\n",
      "Epoch 338/500\n",
      "180/185 [============================>.] - ETA: 0s - loss: 4.1310 - accuracy: 0.9859\n",
      "Epoch 00338: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.1542 - accuracy: 0.9863 - val_loss: 4.3760 - val_accuracy: 0.9844\n",
      "Epoch 339/500\n",
      "178/185 [===========================>..] - ETA: 0s - loss: 4.3068 - accuracy: 0.9867\n",
      "Epoch 00339: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.2845 - accuracy: 0.9868 - val_loss: 4.2336 - val_accuracy: 0.9844\n",
      "Epoch 340/500\n",
      "181/185 [============================>.] - ETA: 0s - loss: 3.7103 - accuracy: 0.9872\n",
      "Epoch 00340: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.6953 - accuracy: 0.9873 - val_loss: 4.4783 - val_accuracy: 0.9851\n",
      "Epoch 341/500\n",
      "171/185 [==========================>...] - ETA: 0s - loss: 3.8564 - accuracy: 0.9867\n",
      "Epoch 00341: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.8669 - accuracy: 0.9866 - val_loss: 5.5205 - val_accuracy: 0.9919\n",
      "Epoch 342/500\n",
      "182/185 [============================>.] - ETA: 0s - loss: 3.6033 - accuracy: 0.9871\n",
      "Epoch 00342: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.6060 - accuracy: 0.9866 - val_loss: 4.6647 - val_accuracy: 0.9871\n",
      "Epoch 343/500\n",
      "175/185 [===========================>..] - ETA: 0s - loss: 4.0801 - accuracy: 0.9870\n",
      "Epoch 00343: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.0227 - accuracy: 0.9871 - val_loss: 3.1206 - val_accuracy: 0.9912\n",
      "Epoch 344/500\n",
      "183/185 [============================>.] - ETA: 0s - loss: 4.0726 - accuracy: 0.9891\n",
      "Epoch 00344: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.0660 - accuracy: 0.9891 - val_loss: 4.6124 - val_accuracy: 0.9851\n",
      "Epoch 345/500\n",
      "174/185 [===========================>..] - ETA: 0s - loss: 3.9319 - accuracy: 0.9892\n",
      "Epoch 00345: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.9262 - accuracy: 0.9890 - val_loss: 4.0192 - val_accuracy: 0.9885\n",
      "Epoch 346/500\n",
      "180/185 [============================>.] - ETA: 0s - loss: 3.4418 - accuracy: 0.9901\n",
      "Epoch 00346: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.4396 - accuracy: 0.9902 - val_loss: 7.3613 - val_accuracy: 0.9708\n",
      "Epoch 347/500\n",
      "176/185 [===========================>..] - ETA: 0s - loss: 3.7167 - accuracy: 0.9893\n",
      "Epoch 00347: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.7464 - accuracy: 0.9891 - val_loss: 5.0603 - val_accuracy: 0.9885\n",
      "Epoch 348/500\n",
      "185/185 [==============================] - ETA: 0s - loss: 3.8158 - accuracy: 0.9871\n",
      "Epoch 00348: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.8158 - accuracy: 0.9871 - val_loss: 4.0780 - val_accuracy: 0.9898\n",
      "Epoch 349/500\n",
      "171/185 [==========================>...] - ETA: 0s - loss: 4.2079 - accuracy: 0.9874\n",
      "Epoch 00349: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.1366 - accuracy: 0.9878 - val_loss: 3.5397 - val_accuracy: 0.9912\n",
      "Epoch 350/500\n",
      "180/185 [============================>.] - ETA: 0s - loss: 4.0715 - accuracy: 0.9877\n",
      "Epoch 00350: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.0476 - accuracy: 0.9880 - val_loss: 4.2748 - val_accuracy: 0.9871\n",
      "Epoch 351/500\n",
      "171/185 [==========================>...] - ETA: 0s - loss: 3.6276 - accuracy: 0.9876\n",
      "Epoch 00351: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.5918 - accuracy: 0.9880 - val_loss: 3.9715 - val_accuracy: 0.9905\n",
      "Epoch 352/500\n",
      "181/185 [============================>.] - ETA: 0s - loss: 3.8087 - accuracy: 0.9865\n",
      "Epoch 00352: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.7932 - accuracy: 0.9868 - val_loss: 3.6169 - val_accuracy: 0.9851\n",
      "Epoch 353/500\n",
      "180/185 [============================>.] - ETA: 0s - loss: 4.3592 - accuracy: 0.9835\n",
      "Epoch 00353: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.3408 - accuracy: 0.9837 - val_loss: 4.2930 - val_accuracy: 0.9925\n",
      "Epoch 354/500\n",
      "179/185 [============================>.] - ETA: 0s - loss: 3.7110 - accuracy: 0.9876\n",
      "Epoch 00354: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.7325 - accuracy: 0.9874 - val_loss: 3.9548 - val_accuracy: 0.9891\n",
      "Epoch 355/500\n",
      "180/185 [============================>.] - ETA: 0s - loss: 4.3409 - accuracy: 0.9851\n",
      "Epoch 00355: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.3236 - accuracy: 0.9851 - val_loss: 3.8765 - val_accuracy: 0.9885\n",
      "Epoch 356/500\n",
      "139/185 [=====================>........] - ETA: 0s - loss: 3.7038 - accuracy: 0.9876\n",
      "Epoch 00356: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.9718 - accuracy: 0.9869 - val_loss: 4.6325 - val_accuracy: 0.9919\n",
      "Epoch 357/500\n",
      "174/185 [===========================>..] - ETA: 0s - loss: 4.1746 - accuracy: 0.9899\n",
      "Epoch 00357: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.1097 - accuracy: 0.9897 - val_loss: 3.8921 - val_accuracy: 0.9932\n",
      "Epoch 358/500\n",
      "179/185 [============================>.] - ETA: 0s - loss: 3.8233 - accuracy: 0.9866\n",
      "Epoch 00358: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.8492 - accuracy: 0.9864 - val_loss: 7.2044 - val_accuracy: 0.9763\n",
      "Epoch 359/500\n",
      "177/185 [===========================>..] - ETA: 0s - loss: 4.3922 - accuracy: 0.9868\n",
      "Epoch 00359: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.3367 - accuracy: 0.9861 - val_loss: 3.7404 - val_accuracy: 0.9932\n",
      "Epoch 360/500\n",
      "185/185 [==============================] - ETA: 0s - loss: 4.4403 - accuracy: 0.9856\n",
      "Epoch 00360: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.4403 - accuracy: 0.9856 - val_loss: 3.5439 - val_accuracy: 0.9919\n",
      "Epoch 361/500\n",
      "177/185 [===========================>..] - ETA: 0s - loss: 3.4415 - accuracy: 0.9891\n",
      "Epoch 00361: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.4528 - accuracy: 0.9885 - val_loss: 3.9591 - val_accuracy: 0.9932\n",
      "Epoch 362/500\n",
      "165/185 [=========================>....] - ETA: 0s - loss: 4.2054 - accuracy: 0.9892\n",
      "Epoch 00362: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.2623 - accuracy: 0.9885 - val_loss: 4.9047 - val_accuracy: 0.9824\n",
      "Epoch 363/500\n",
      "182/185 [============================>.] - ETA: 0s - loss: 4.3148 - accuracy: 0.9878\n",
      "Epoch 00363: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.3172 - accuracy: 0.9880 - val_loss: 4.9357 - val_accuracy: 0.9817\n",
      "Epoch 364/500\n",
      "178/185 [===========================>..] - ETA: 0s - loss: 3.9405 - accuracy: 0.9879\n",
      "Epoch 00364: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.9265 - accuracy: 0.9876 - val_loss: 4.2162 - val_accuracy: 0.9912\n",
      "Epoch 365/500\n",
      "179/185 [============================>.] - ETA: 0s - loss: 4.4007 - accuracy: 0.9829\n",
      "Epoch 00365: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.4009 - accuracy: 0.9825 - val_loss: 3.8742 - val_accuracy: 0.9830\n",
      "Epoch 366/500\n",
      "181/185 [============================>.] - ETA: 0s - loss: 3.9650 - accuracy: 0.9852\n",
      "Epoch 00366: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.9710 - accuracy: 0.9854 - val_loss: 2.9017 - val_accuracy: 0.9959\n",
      "Epoch 367/500\n",
      "179/185 [============================>.] - ETA: 0s - loss: 3.9644 - accuracy: 0.9871\n",
      "Epoch 00367: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.9606 - accuracy: 0.9869 - val_loss: 2.7761 - val_accuracy: 0.9932\n",
      "Epoch 368/500\n",
      "175/185 [===========================>..] - ETA: 0s - loss: 3.3694 - accuracy: 0.9911\n",
      "Epoch 00368: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.3583 - accuracy: 0.9907 - val_loss: 3.8825 - val_accuracy: 0.9898\n",
      "Epoch 369/500\n",
      "175/185 [===========================>..] - ETA: 0s - loss: 3.9058 - accuracy: 0.9896\n",
      "Epoch 00369: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.9118 - accuracy: 0.9898 - val_loss: 4.1548 - val_accuracy: 0.9885\n",
      "Epoch 370/500\n",
      "170/185 [==========================>...] - ETA: 0s - loss: 3.6105 - accuracy: 0.9882\n",
      "Epoch 00370: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.6078 - accuracy: 0.9881 - val_loss: 3.2324 - val_accuracy: 0.9953\n",
      "Epoch 371/500\n",
      "179/185 [============================>.] - ETA: 0s - loss: 3.7631 - accuracy: 0.9864\n",
      "Epoch 00371: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.7660 - accuracy: 0.9868 - val_loss: 3.4603 - val_accuracy: 0.9925\n",
      "Epoch 372/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177/185 [===========================>..] - ETA: 0s - loss: 4.4968 - accuracy: 0.9853\n",
      "Epoch 00372: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.5091 - accuracy: 0.9854 - val_loss: 5.3521 - val_accuracy: 0.9885\n",
      "Epoch 373/500\n",
      "175/185 [===========================>..] - ETA: 0s - loss: 3.8625 - accuracy: 0.9895\n",
      "Epoch 00373: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.9096 - accuracy: 0.9891 - val_loss: 5.0449 - val_accuracy: 0.9837\n",
      "Epoch 374/500\n",
      "176/185 [===========================>..] - ETA: 0s - loss: 3.7184 - accuracy: 0.9885\n",
      "Epoch 00374: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.7167 - accuracy: 0.9886 - val_loss: 4.2043 - val_accuracy: 0.9864\n",
      "Epoch 375/500\n",
      "164/185 [=========================>....] - ETA: 0s - loss: 3.6958 - accuracy: 0.9870\n",
      "Epoch 00375: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.7275 - accuracy: 0.9873 - val_loss: 4.0450 - val_accuracy: 0.9912\n",
      "Epoch 376/500\n",
      "175/185 [===========================>..] - ETA: 0s - loss: 3.8435 - accuracy: 0.9893\n",
      "Epoch 00376: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.8243 - accuracy: 0.9891 - val_loss: 4.3602 - val_accuracy: 0.9891\n",
      "Epoch 377/500\n",
      "182/185 [============================>.] - ETA: 0s - loss: 3.6090 - accuracy: 0.9895\n",
      "Epoch 00377: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.5993 - accuracy: 0.9897 - val_loss: 3.3713 - val_accuracy: 0.9932\n",
      "Epoch 378/500\n",
      "175/185 [===========================>..] - ETA: 0s - loss: 3.8312 - accuracy: 0.9880\n",
      "Epoch 00378: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.8150 - accuracy: 0.9885 - val_loss: 3.8078 - val_accuracy: 0.9919\n",
      "Epoch 379/500\n",
      "177/185 [===========================>..] - ETA: 0s - loss: 3.7468 - accuracy: 0.9878\n",
      "Epoch 00379: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.7387 - accuracy: 0.9873 - val_loss: 3.6727 - val_accuracy: 0.9932\n",
      "Epoch 380/500\n",
      "178/185 [===========================>..] - ETA: 0s - loss: 4.2109 - accuracy: 0.9856\n",
      "Epoch 00380: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.2026 - accuracy: 0.9858 - val_loss: 4.4779 - val_accuracy: 0.9891\n",
      "Epoch 381/500\n",
      "182/185 [============================>.] - ETA: 0s - loss: 3.6227 - accuracy: 0.9882\n",
      "Epoch 00381: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.6338 - accuracy: 0.9881 - val_loss: 4.6045 - val_accuracy: 0.9966\n",
      "Epoch 382/500\n",
      "178/185 [===========================>..] - ETA: 0s - loss: 3.5711 - accuracy: 0.9896\n",
      "Epoch 00382: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.5864 - accuracy: 0.9891 - val_loss: 5.1464 - val_accuracy: 0.9858\n",
      "Epoch 383/500\n",
      "181/185 [============================>.] - ETA: 0s - loss: 4.2231 - accuracy: 0.9862\n",
      "Epoch 00383: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.1933 - accuracy: 0.9863 - val_loss: 3.7399 - val_accuracy: 0.9905\n",
      "Epoch 384/500\n",
      "178/185 [===========================>..] - ETA: 0s - loss: 3.3759 - accuracy: 0.9863\n",
      "Epoch 00384: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.4053 - accuracy: 0.9863 - val_loss: 4.8621 - val_accuracy: 0.9878\n",
      "Epoch 385/500\n",
      "179/185 [============================>.] - ETA: 0s - loss: 4.4593 - accuracy: 0.9867\n",
      "Epoch 00385: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.4516 - accuracy: 0.9859 - val_loss: 3.2120 - val_accuracy: 0.9885\n",
      "Epoch 386/500\n",
      "164/185 [=========================>....] - ETA: 0s - loss: 3.8671 - accuracy: 0.9867\n",
      "Epoch 00386: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.0776 - accuracy: 0.9859 - val_loss: 7.7340 - val_accuracy: 0.9885\n",
      "Epoch 387/500\n",
      "160/185 [========================>.....] - ETA: 0s - loss: 3.7661 - accuracy: 0.9875\n",
      "Epoch 00387: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.8922 - accuracy: 0.9864 - val_loss: 4.6416 - val_accuracy: 0.9864\n",
      "Epoch 388/500\n",
      "179/185 [============================>.] - ETA: 0s - loss: 3.6881 - accuracy: 0.9866\n",
      "Epoch 00388: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.6912 - accuracy: 0.9866 - val_loss: 3.2689 - val_accuracy: 0.9912\n",
      "Epoch 389/500\n",
      "183/185 [============================>.] - ETA: 0s - loss: 3.8353 - accuracy: 0.9903\n",
      "Epoch 00389: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.8387 - accuracy: 0.9900 - val_loss: 7.1560 - val_accuracy: 0.9803\n",
      "Epoch 390/500\n",
      "181/185 [============================>.] - ETA: 0s - loss: 3.7024 - accuracy: 0.9867\n",
      "Epoch 00390: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.6931 - accuracy: 0.9866 - val_loss: 3.9304 - val_accuracy: 0.9919\n",
      "Epoch 391/500\n",
      "180/185 [============================>.] - ETA: 0s - loss: 3.6647 - accuracy: 0.9882\n",
      "Epoch 00391: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.6598 - accuracy: 0.9878 - val_loss: 3.7919 - val_accuracy: 0.9925\n",
      "Epoch 392/500\n",
      "169/185 [==========================>...] - ETA: 0s - loss: 3.9790 - accuracy: 0.9841\n",
      "Epoch 00392: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.9189 - accuracy: 0.9847 - val_loss: 3.5880 - val_accuracy: 0.9912\n",
      "Epoch 393/500\n",
      "181/185 [============================>.] - ETA: 0s - loss: 3.9055 - accuracy: 0.9874\n",
      "Epoch 00393: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.8943 - accuracy: 0.9871 - val_loss: 4.5551 - val_accuracy: 0.9871\n",
      "Epoch 394/500\n",
      "171/185 [==========================>...] - ETA: 0s - loss: 4.2011 - accuracy: 0.9830\n",
      "Epoch 00394: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.1473 - accuracy: 0.9837 - val_loss: 4.3037 - val_accuracy: 0.9837\n",
      "Epoch 395/500\n",
      "182/185 [============================>.] - ETA: 0s - loss: 4.2356 - accuracy: 0.9857\n",
      "Epoch 00395: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.2352 - accuracy: 0.9858 - val_loss: 5.3467 - val_accuracy: 0.9844\n",
      "Epoch 396/500\n",
      "170/185 [==========================>...] - ETA: 0s - loss: 3.2379 - accuracy: 0.9862\n",
      "Epoch 00396: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.2780 - accuracy: 0.9859 - val_loss: 3.6240 - val_accuracy: 0.9912\n",
      "Epoch 397/500\n",
      "179/185 [============================>.] - ETA: 0s - loss: 4.3122 - accuracy: 0.9825\n",
      "Epoch 00397: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.3133 - accuracy: 0.9825 - val_loss: 4.2733 - val_accuracy: 0.9891\n",
      "Epoch 398/500\n",
      "181/185 [============================>.] - ETA: 0s - loss: 3.7288 - accuracy: 0.9864\n",
      "Epoch 00398: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.7249 - accuracy: 0.9861 - val_loss: 2.9838 - val_accuracy: 0.9939\n",
      "Epoch 399/500\n",
      "183/185 [============================>.] - ETA: 0s - loss: 3.5144 - accuracy: 0.9887\n",
      "Epoch 00399: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.5091 - accuracy: 0.9888 - val_loss: 4.1201 - val_accuracy: 0.9905\n",
      "Epoch 400/500\n",
      "180/185 [============================>.] - ETA: 0s - loss: 4.3777 - accuracy: 0.9844\n",
      "Epoch 00400: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.3443 - accuracy: 0.9847 - val_loss: 3.5553 - val_accuracy: 0.9919\n",
      "Epoch 401/500\n",
      "181/185 [============================>.] - ETA: 0s - loss: 3.8397 - accuracy: 0.9867\n",
      "Epoch 00401: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.8445 - accuracy: 0.9866 - val_loss: 5.9138 - val_accuracy: 0.9803\n",
      "Epoch 402/500\n",
      "178/185 [===========================>..] - ETA: 0s - loss: 3.8928 - accuracy: 0.9872\n",
      "Epoch 00402: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.8939 - accuracy: 0.9874 - val_loss: 3.1875 - val_accuracy: 0.9912\n",
      "Epoch 403/500\n",
      "183/185 [============================>.] - ETA: 0s - loss: 3.5812 - accuracy: 0.9867\n",
      "Epoch 00403: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.5998 - accuracy: 0.9866 - val_loss: 4.6059 - val_accuracy: 0.9946\n",
      "Epoch 404/500\n",
      "181/185 [============================>.] - ETA: 0s - loss: 4.0770 - accuracy: 0.9860\n",
      "Epoch 00404: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 4.0558 - accuracy: 0.9854 - val_loss: 4.6104 - val_accuracy: 0.9905\n",
      "Epoch 405/500\n",
      "181/185 [============================>.] - ETA: 0s - loss: 3.5673 - accuracy: 0.9879\n",
      "Epoch 00405: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.5642 - accuracy: 0.9881 - val_loss: 3.3050 - val_accuracy: 0.9946\n",
      "Epoch 406/500\n",
      "181/185 [============================>.] - ETA: 0s - loss: 3.7643 - accuracy: 0.9869\n",
      "Epoch 00406: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.7639 - accuracy: 0.9869 - val_loss: 4.2573 - val_accuracy: 0.9912\n",
      "Epoch 407/500\n",
      "179/185 [============================>.] - ETA: 0s - loss: 3.7680 - accuracy: 0.9871\n",
      "Epoch 00407: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.7578 - accuracy: 0.9873 - val_loss: 3.8231 - val_accuracy: 0.9878\n",
      "Epoch 408/500\n",
      "183/185 [============================>.] - ETA: 0s - loss: 3.6760 - accuracy: 0.9865\n",
      "Epoch 00408: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.6834 - accuracy: 0.9864 - val_loss: 6.1897 - val_accuracy: 0.9824\n",
      "Epoch 409/500\n",
      "178/185 [===========================>..] - ETA: 0s - loss: 3.7999 - accuracy: 0.9875\n",
      "Epoch 00409: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.7647 - accuracy: 0.9878 - val_loss: 2.9061 - val_accuracy: 0.9932\n",
      "Epoch 410/500\n",
      "179/185 [============================>.] - ETA: 0s - loss: 3.6336 - accuracy: 0.9878\n",
      "Epoch 00410: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.6230 - accuracy: 0.9881 - val_loss: 4.4890 - val_accuracy: 0.9891\n",
      "Epoch 411/500\n",
      "176/185 [===========================>..] - ETA: 0s - loss: 3.6091 - accuracy: 0.9847\n",
      "Epoch 00411: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.6054 - accuracy: 0.9847 - val_loss: 3.6395 - val_accuracy: 0.9837\n",
      "Epoch 412/500\n",
      "176/185 [===========================>..] - ETA: 0s - loss: 3.6636 - accuracy: 0.9881\n",
      "Epoch 00412: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.6363 - accuracy: 0.9883 - val_loss: 4.5159 - val_accuracy: 0.9912\n",
      "Epoch 413/500\n",
      "170/185 [==========================>...] - ETA: 0s - loss: 3.8809 - accuracy: 0.9873\n",
      "Epoch 00413: val_loss did not improve from 2.65493\n",
      "185/185 [==============================] - 0s 1ms/step - loss: 3.8428 - accuracy: 0.9876 - val_loss: 2.8832 - val_accuracy: 0.9953\n",
      "Epoch 00413: early stopping\n"
     ]
    }
   ],
   "source": [
    "history= NN_model.fit(X_train, Y_train, epochs=500, batch_size=32, validation_split = 0.2, callbacks=callback_list)\n",
    "#history= NN_model.fit(X_train, Y_train, epochs=7, batch_size=32, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_model.save('random_split_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 2.802, Validation loss: 2.990\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluate the model\n",
    "train_loss, train_acc = NN_model.evaluate(X_train, Y_train, verbose=0)\n",
    "test_loss, test_acc = NN_model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Train loss: %.3f, Validation loss: %.3f' % (train_loss, test_loss))\n",
    "# plot training history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latest=tf.train.latest_checkpoint(checkpoint_dir)\n",
    "# checkpoint_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = NN_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percentage error on test set:  [0.02029656 0.03380397 0.03561135]\n"
     ]
    }
   ],
   "source": [
    "error= mean_absolute_percentage_error(Y_test, Y_pred, multioutput='raw_values')   \n",
    "print('Mean absolute percentage error on test set: ', error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leave fractal dimesnion: =2.2 as test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2195, 36)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set=df1[(df1['fractal_dimension']<=2.1) | (df1['fractal_dimension']>2.2)]\n",
    "test_set=df1[df1['fractal_dimension']==2.2]\n",
    "test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = train_set.iloc[:,25:28]\n",
    "X_train = train_set.iloc[:,:8]\n",
    "Y_test = test_set.iloc[:,25:28]\n",
    "X_test = test_set.iloc[:,:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model1 = load_model('random_split_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               1152      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 166,531\n",
      "Trainable params: 166,531\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model1 = Sequential()\n",
    "# # The Input Layer :\n",
    "# model1.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n",
    "\n",
    "# # The Hidden Layers :\n",
    "# model1.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "# model1.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "# model1.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "\n",
    "# # The Output Layer :\n",
    "# model1.add(Dense(3, kernel_initializer='normal',activation='linear'))\n",
    "# #model1.add(Dense(3, kernel_initializer='normal',activation='relu'))\n",
    "# # Compile the network :\n",
    "# model1.compile(loss='mean_absolute_percentage_error', optimizer='adam', metrics=['accuracy'])\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"fractal_dimension_2_2/Weights-{epoch:03d}--{val_loss:.5f}.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, verbose=1, monitor='val_loss',save_best_only=True, mode='auto')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # patient early stopping\n",
    "es = EarlyStopping(monitor='val_loss', patience=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_csv=CSVLogger('fractal_dimension_2_2_loss_logs.csv', separator=',', append=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_list=[checkpoint, es, log_csv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "182/191 [===========================>..] - ETA: 0s - loss: 4.1667 - accuracy: 0.9832\n",
      "Epoch 00001: val_loss improved from inf to 4.52571, saving model to fractal_dimension_2_2\\Weights-001--4.52571.hdf5\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 4.1414 - accuracy: 0.9835 - val_loss: 4.5257 - val_accuracy: 0.9915\n",
      "Epoch 2/500\n",
      "182/191 [===========================>..] - ETA: 0s - loss: 3.8505 - accuracy: 0.9875\n",
      "Epoch 00002: val_loss improved from 4.52571 to 4.52006, saving model to fractal_dimension_2_2\\Weights-002--4.52006.hdf5\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 3.8364 - accuracy: 0.9874 - val_loss: 4.5201 - val_accuracy: 0.9948\n",
      "Epoch 3/500\n",
      "181/191 [===========================>..] - ETA: 0s - loss: 3.9581 - accuracy: 0.9843\n",
      "Epoch 00003: val_loss did not improve from 4.52006\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 3.9681 - accuracy: 0.9849 - val_loss: 5.8031 - val_accuracy: 0.9908\n",
      "Epoch 4/500\n",
      "178/191 [==========================>...] - ETA: 0s - loss: 4.1263 - accuracy: 0.9872\n",
      "Epoch 00004: val_loss improved from 4.52006 to 3.70026, saving model to fractal_dimension_2_2\\Weights-004--3.70026.hdf5\n",
      "191/191 [==============================] - 0s 2ms/step - loss: 4.1291 - accuracy: 0.9876 - val_loss: 3.7003 - val_accuracy: 0.9948\n",
      "Epoch 5/500\n",
      "179/191 [===========================>..] - ETA: 0s - loss: 3.7392 - accuracy: 0.9860\n",
      "Epoch 00005: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 3.7749 - accuracy: 0.9859 - val_loss: 4.7926 - val_accuracy: 0.9902\n",
      "Epoch 6/500\n",
      "145/191 [=====================>........] - ETA: 0s - loss: 3.7571 - accuracy: 0.9856\n",
      "Epoch 00006: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 3.6832 - accuracy: 0.9862 - val_loss: 5.3622 - val_accuracy: 0.9869\n",
      "Epoch 7/500\n",
      "189/191 [============================>.] - ETA: 0s - loss: 4.2886 - accuracy: 0.9838\n",
      "Epoch 00007: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 4.2927 - accuracy: 0.9839 - val_loss: 5.8564 - val_accuracy: 0.9843\n",
      "Epoch 8/500\n",
      "187/191 [============================>.] - ETA: 0s - loss: 3.2955 - accuracy: 0.9875\n",
      "Epoch 00008: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 3.3059 - accuracy: 0.9877 - val_loss: 5.5895 - val_accuracy: 0.9876\n",
      "Epoch 9/500\n",
      "146/191 [=====================>........] - ETA: 0s - loss: 3.5908 - accuracy: 0.9889\n",
      "Epoch 00009: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 3.7029 - accuracy: 0.9877 - val_loss: 4.6973 - val_accuracy: 0.9928\n",
      "Epoch 10/500\n",
      "191/191 [==============================] - ETA: 0s - loss: 4.6247 - accuracy: 0.9849\n",
      "Epoch 00010: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 4.6247 - accuracy: 0.9849 - val_loss: 6.7330 - val_accuracy: 0.9849\n",
      "Epoch 11/500\n",
      "145/191 [=====================>........] - ETA: 0s - loss: 3.6276 - accuracy: 0.9866\n",
      "Epoch 00011: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 3.5456 - accuracy: 0.9874 - val_loss: 5.6511 - val_accuracy: 0.9882\n",
      "Epoch 12/500\n",
      "181/191 [===========================>..] - ETA: 0s - loss: 3.5802 - accuracy: 0.9864\n",
      "Epoch 00012: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 3.5618 - accuracy: 0.9864 - val_loss: 4.8289 - val_accuracy: 0.9843\n",
      "Epoch 13/500\n",
      "191/191 [==============================] - ETA: 0s - loss: 4.4692 - accuracy: 0.9853\n",
      "Epoch 00013: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 4.4692 - accuracy: 0.9853 - val_loss: 6.1215 - val_accuracy: 0.9869\n",
      "Epoch 14/500\n",
      "143/191 [=====================>........] - ETA: 0s - loss: 3.8783 - accuracy: 0.9858\n",
      "Epoch 00014: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 3.8536 - accuracy: 0.9864 - val_loss: 4.7769 - val_accuracy: 0.9869\n",
      "Epoch 15/500\n",
      "185/191 [============================>.] - ETA: 0s - loss: 3.8319 - accuracy: 0.9855\n",
      "Epoch 00015: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 3.8156 - accuracy: 0.9859 - val_loss: 5.0745 - val_accuracy: 0.9849\n",
      "Epoch 16/500\n",
      "189/191 [============================>.] - ETA: 0s - loss: 4.3355 - accuracy: 0.9869\n",
      "Epoch 00016: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 4.3393 - accuracy: 0.9871 - val_loss: 6.1619 - val_accuracy: 0.9869\n",
      "Epoch 17/500\n",
      "187/191 [============================>.] - ETA: 0s - loss: 4.1358 - accuracy: 0.9843\n",
      "Epoch 00017: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 4.1396 - accuracy: 0.9843 - val_loss: 7.0603 - val_accuracy: 0.9764\n",
      "Epoch 18/500\n",
      "191/191 [==============================] - ETA: 0s - loss: 3.8547 - accuracy: 0.9856\n",
      "Epoch 00018: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 3.8547 - accuracy: 0.9856 - val_loss: 6.6439 - val_accuracy: 0.9862\n",
      "Epoch 19/500\n",
      "187/191 [============================>.] - ETA: 0s - loss: 3.9132 - accuracy: 0.9878\n",
      "Epoch 00019: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 3.9033 - accuracy: 0.9876 - val_loss: 7.7133 - val_accuracy: 0.9777\n",
      "Epoch 20/500\n",
      "181/191 [===========================>..] - ETA: 0s - loss: 4.0618 - accuracy: 0.9864\n",
      "Epoch 00020: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 4.0389 - accuracy: 0.9869 - val_loss: 7.0036 - val_accuracy: 0.9902\n",
      "Epoch 21/500\n",
      "181/191 [===========================>..] - ETA: 0s - loss: 3.5610 - accuracy: 0.9874\n",
      "Epoch 00021: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 3.5407 - accuracy: 0.9876 - val_loss: 5.9972 - val_accuracy: 0.9777\n",
      "Epoch 22/500\n",
      "172/191 [==========================>...] - ETA: 0s - loss: 4.1427 - accuracy: 0.9853\n",
      "Epoch 00022: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 4.0903 - accuracy: 0.9857 - val_loss: 6.6399 - val_accuracy: 0.9843\n",
      "Epoch 23/500\n",
      "183/191 [===========================>..] - ETA: 0s - loss: 3.7248 - accuracy: 0.9846\n",
      "Epoch 00023: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 3.7219 - accuracy: 0.9844 - val_loss: 6.9333 - val_accuracy: 0.9856\n",
      "Epoch 24/500\n",
      "191/191 [==============================] - ETA: 0s - loss: 3.6629 - accuracy: 0.9866\n",
      "Epoch 00024: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 3.6629 - accuracy: 0.9866 - val_loss: 6.7677 - val_accuracy: 0.9889\n",
      "Epoch 25/500\n",
      "181/191 [===========================>..] - ETA: 0s - loss: 3.6884 - accuracy: 0.9862\n",
      "Epoch 00025: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 3.6800 - accuracy: 0.9862 - val_loss: 7.4755 - val_accuracy: 0.9876\n",
      "Epoch 26/500\n",
      "181/191 [===========================>..] - ETA: 0s - loss: 3.7417 - accuracy: 0.9841\n",
      "Epoch 00026: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 3.7065 - accuracy: 0.9836 - val_loss: 6.4399 - val_accuracy: 0.9843\n",
      "Epoch 27/500\n",
      "146/191 [=====================>........] - ETA: 0s - loss: 3.5657 - accuracy: 0.9874\n",
      "Epoch 00027: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 3.5277 - accuracy: 0.9862 - val_loss: 5.9555 - val_accuracy: 0.9804\n",
      "Epoch 28/500\n",
      "182/191 [===========================>..] - ETA: 0s - loss: 3.6847 - accuracy: 0.9856\n",
      "Epoch 00028: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 3.6855 - accuracy: 0.9857 - val_loss: 7.2072 - val_accuracy: 0.9895\n",
      "Epoch 29/500\n",
      "184/191 [===========================>..] - ETA: 0s - loss: 3.9051 - accuracy: 0.9840\n",
      "Epoch 00029: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 3.8737 - accuracy: 0.9841 - val_loss: 7.2199 - val_accuracy: 0.9810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/500\n",
      "186/191 [============================>.] - ETA: 0s - loss: 3.5557 - accuracy: 0.9872\n",
      "Epoch 00030: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 3.5539 - accuracy: 0.9876 - val_loss: 7.1763 - val_accuracy: 0.9843\n",
      "Epoch 31/500\n",
      "182/191 [===========================>..] - ETA: 0s - loss: 3.6593 - accuracy: 0.9868\n",
      "Epoch 00031: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 3.6863 - accuracy: 0.9872 - val_loss: 7.9027 - val_accuracy: 0.9869\n",
      "Epoch 32/500\n",
      "144/191 [=====================>........] - ETA: 0s - loss: 3.4259 - accuracy: 0.9881\n",
      "Epoch 00032: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 3.4656 - accuracy: 0.9859 - val_loss: 7.3592 - val_accuracy: 0.9849\n",
      "Epoch 33/500\n",
      "185/191 [============================>.] - ETA: 0s - loss: 3.5254 - accuracy: 0.9880\n",
      "Epoch 00033: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 3.5178 - accuracy: 0.9882 - val_loss: 8.6096 - val_accuracy: 0.9797\n",
      "Epoch 34/500\n",
      "179/191 [===========================>..] - ETA: 0s - loss: 4.1838 - accuracy: 0.9855\n",
      "Epoch 00034: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 4.2302 - accuracy: 0.9846 - val_loss: 7.0376 - val_accuracy: 0.9856\n",
      "Epoch 35/500\n",
      "183/191 [===========================>..] - ETA: 0s - loss: 3.7978 - accuracy: 0.9858\n",
      "Epoch 00035: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 3.8052 - accuracy: 0.9856 - val_loss: 7.4374 - val_accuracy: 0.9725\n",
      "Epoch 36/500\n",
      "189/191 [============================>.] - ETA: 0s - loss: 3.6167 - accuracy: 0.9845\n",
      "Epoch 00036: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 3.6106 - accuracy: 0.9846 - val_loss: 6.8698 - val_accuracy: 0.9843\n",
      "Epoch 37/500\n",
      "145/191 [=====================>........] - ETA: 0s - loss: 3.7817 - accuracy: 0.9853\n",
      "Epoch 00037: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 4.0048 - accuracy: 0.9853 - val_loss: 7.2067 - val_accuracy: 0.9790\n",
      "Epoch 38/500\n",
      "185/191 [============================>.] - ETA: 0s - loss: 3.6456 - accuracy: 0.9865\n",
      "Epoch 00038: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 3.6320 - accuracy: 0.9867 - val_loss: 8.1491 - val_accuracy: 0.9876\n",
      "Epoch 39/500\n",
      "144/191 [=====================>........] - ETA: 0s - loss: 3.5799 - accuracy: 0.9857\n",
      "Epoch 00039: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 3.5488 - accuracy: 0.9862 - val_loss: 7.8050 - val_accuracy: 0.9869\n",
      "Epoch 40/500\n",
      "187/191 [============================>.] - ETA: 0s - loss: 3.5337 - accuracy: 0.9875\n",
      "Epoch 00040: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 3.5641 - accuracy: 0.9872 - val_loss: 7.9858 - val_accuracy: 0.9777\n",
      "Epoch 41/500\n",
      "189/191 [============================>.] - ETA: 0s - loss: 3.4414 - accuracy: 0.9861\n",
      "Epoch 00041: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 3.4374 - accuracy: 0.9862 - val_loss: 7.3665 - val_accuracy: 0.9843\n",
      "Epoch 42/500\n",
      "188/191 [============================>.] - ETA: 0s - loss: 3.6246 - accuracy: 0.9882\n",
      "Epoch 00042: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 3.6198 - accuracy: 0.9884 - val_loss: 7.5567 - val_accuracy: 0.9784\n",
      "Epoch 43/500\n",
      "188/191 [============================>.] - ETA: 0s - loss: 4.1434 - accuracy: 0.9862\n",
      "Epoch 00043: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 4.1558 - accuracy: 0.9861 - val_loss: 6.9461 - val_accuracy: 0.9849\n",
      "Epoch 44/500\n",
      "182/191 [===========================>..] - ETA: 0s - loss: 3.7884 - accuracy: 0.9864\n",
      "Epoch 00044: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 3.7902 - accuracy: 0.9861 - val_loss: 6.6814 - val_accuracy: 0.9862\n",
      "Epoch 45/500\n",
      "187/191 [============================>.] - ETA: 0s - loss: 3.7917 - accuracy: 0.9871\n",
      "Epoch 00045: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 3.7963 - accuracy: 0.9871 - val_loss: 7.1158 - val_accuracy: 0.9856\n",
      "Epoch 46/500\n",
      "187/191 [============================>.] - ETA: 0s - loss: 3.7966 - accuracy: 0.9851\n",
      "Epoch 00046: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 3.7984 - accuracy: 0.9848 - val_loss: 8.0107 - val_accuracy: 0.9895\n",
      "Epoch 47/500\n",
      "185/191 [============================>.] - ETA: 0s - loss: 3.5377 - accuracy: 0.9880\n",
      "Epoch 00047: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 3.5257 - accuracy: 0.9879 - val_loss: 7.1682 - val_accuracy: 0.9836\n",
      "Epoch 48/500\n",
      "178/191 [==========================>...] - ETA: 0s - loss: 3.7387 - accuracy: 0.9870\n",
      "Epoch 00048: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 3.6672 - accuracy: 0.9871 - val_loss: 6.7067 - val_accuracy: 0.9823\n",
      "Epoch 49/500\n",
      "146/191 [=====================>........] - ETA: 0s - loss: 3.5744 - accuracy: 0.9874\n",
      "Epoch 00049: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 3.6292 - accuracy: 0.9867 - val_loss: 7.4544 - val_accuracy: 0.9849\n",
      "Epoch 50/500\n",
      "180/191 [===========================>..] - ETA: 0s - loss: 3.7378 - accuracy: 0.9875\n",
      "Epoch 00050: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 3.7139 - accuracy: 0.9872 - val_loss: 7.0083 - val_accuracy: 0.9869\n",
      "Epoch 51/500\n",
      "189/191 [============================>.] - ETA: 0s - loss: 3.4298 - accuracy: 0.9883\n",
      "Epoch 00051: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 3.4236 - accuracy: 0.9884 - val_loss: 7.6294 - val_accuracy: 0.9869\n",
      "Epoch 52/500\n",
      "189/191 [============================>.] - ETA: 0s - loss: 3.7393 - accuracy: 0.9858\n",
      "Epoch 00052: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 3.7314 - accuracy: 0.9857 - val_loss: 8.4846 - val_accuracy: 0.9804\n",
      "Epoch 53/500\n",
      "188/191 [============================>.] - ETA: 0s - loss: 3.6897 - accuracy: 0.9854\n",
      "Epoch 00053: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 3.6904 - accuracy: 0.9854 - val_loss: 8.3051 - val_accuracy: 0.9856\n",
      "Epoch 54/500\n",
      "189/191 [============================>.] - ETA: 0s - loss: 3.6633 - accuracy: 0.9881\n",
      "Epoch 00054: val_loss did not improve from 3.70026\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 3.6583 - accuracy: 0.9882 - val_loss: 7.7142 - val_accuracy: 0.9804\n",
      "Epoch 00054: early stopping\n"
     ]
    }
   ],
   "source": [
    "history= model1.fit(X_train, Y_train, epochs=500, batch_size=32, validation_split = 0.2, callbacks=callback_list)\n",
    "#history= NN_model.fit(X_train, Y_train, epochs=7, batch_size=32, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.save('fractal_dimension_2_2_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.851, Validation loss: 7.524\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD4CAYAAADFAawfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAABVlUlEQVR4nO2dd3xV9f3/n5/snZANCZAwwt5bhiBgHai4tY5aW+36Wq3V/rrtnn77tbZqq3XUidZVR7UqgoqAEKaMBEgISQhkkr2Tz++Pzz3Jzc3duTN8no8Hj5vcc+45nwPc93mf1+f1fn+ElBKNRqPRBC4h/h6ARqPRaOyjA7VGo9EEODpQazQaTYCjA7VGo9EEODpQazQaTYAT5o2DpqamypycHG8cWqPRaIYlu3btqpFSplnb5pVAnZOTQ35+vjcOrdFoNMMSIcQJW9u09KHRaDQBjg7UGo1GE+DoQK3RaDQBjlc0amt0dXVRXl5Oe3u7r045rImKiiI7O5vw8HB/D0Wj0XgZnwXq8vJy4uPjycnJQQjhq9MOS6SU1NbWUl5eTm5urr+Ho9FovIzPpI/29nZSUlJ0kPYAQghSUlL004lGc5bgU41aB2nPof8uNZqzBz2ZqNH4gt5e2P0MdHf4eySaIOSsCdT19fU8/PDDLn/uoosuor6+3vMD0pxdnMyHN/4HCt/x90g0QchZH6h7enrsfu4///kPSUlJXhqV5qzhjKnorOmUf8ehCUp85vrwN9///vcpKipi9uzZhIeHExcXx8iRI9m7dy+HDh1i/fr1lJWV0d7ezp133sntt98O9JfDNzc3c+GFF7Js2TK2bt1KVlYW//73v4mOjvbzlWmCgnojUJ/27zg0QYlfAvXP3zzIoYpGjx5z6qgE7rtkms3tv/vd7zhw4AB79+5l8+bNXHzxxRw4cKDP3vbEE0+QnJxMW1sbCxYs4MorryQlJWXAMY4ePcoLL7zAY489xjXXXMMrr7zCjTfe6NHr0AxTGsrUqw7UGjc4a6QPSxYuXDjAg/zggw8ya9YsFi9eTFlZGUePHh30mdzcXGbPng3AvHnzKCkp8dFoNUFPfal6bdaBOmjo7oBnroCC//h7JP7JqO1lvr4iNja27+fNmzfzwQcfsG3bNmJiYli5cqVVj3JkZGTfz6GhobS1tflkrJphQL2RUVf6dxwa56k6BEUb4cSncMvbkD3fb0M5azLq+Ph4mpqarG5raGhgxIgRxMTEUFBQwPbt2308Os2wRsp+6UNn1MFD5UH1GhEHL1zXPyHsB86aQJ2SksLSpUuZPn06995774BtF1xwAd3d3cycOZOf/OQnLF682E+j1AxLmqugux1i06HtjPZSBwunD0B4jMqmezrh+Wugrd4vQzlrXB8Azz//vNX3IyMjeecd6/5WQ4dOTU3lwIEDfe/fc889Hh+fZphiZNOjF0LBW9BcCUlj/DsmjWMqD0D6VEifDNc+C89cDi/dDDe+AqG+bYZ21mTUGo3fMKx52QvUq9apAx8plfSRYZpPy10BlzwIxz+Ct76jtvuQsyqj1mj8gjGRaARqrVMHPk2noa0OMqb3vzfnBqgrhk/uh5TxsOw7PhuOzqg1Gm9TXwpRSZAyQf2uvdSBjzGRmGHhUFv1I5h+JXzwM6gZbOH1FjpQazTepqFMadKxqSBClEatCWwqTfNRGVMHvh8SAqt/qn4u+tBnw9GBWqPxNvWlKlCHhCrnh+73EfhUHoSEbIgeMXjbiBxIGgvFH/lsODpQazTeREqlURsuj/gMPZkYDJhPJFojdwWUbIGebp8M56wJ1CtXruS///3vgPceeOABvvnNb9rcPz8/H7Dd6vRnP/sZ999/v93zvv766xw6dKjv95/+9Kd88MEHLo5eE7S01kFXCySOVr/Hj9STiYFOdwfUFNoP1ONWQkcDnN7nkyGdNYH6+uuvZ8OGDQPe27BhA9dff73Dzw6l1alloP7FL37BmjVr3DqWJghpMPX4MDLqOB9m1B3NcMo3gWRYUXMEershc7rtfXJXqFcfyR9nTaC+6qqreOutt+joUFVhJSUlVFRU8PzzzzN//nymTZvGfffdZ/WzOTk51NTUAPDrX/+aSZMmsWbNGgoLC/v2eeyxx1iwYAGzZs3iyiuvpLW1la1bt/LGG29w7733Mnv2bIqKirjlllt4+eWXAdi4cSNz5sxhxowZ3HrrrX1jy8nJ4b777mPu3LnMmDGDgoICb/7VaLyJ0YwpycioM6Gl2jePzJ89Ao+uhJpj3j/XcKLP8WEnUMelq2KY474J1P7xUb/zfTj9uWePmTkDLvydzc0pKSksXLiQd999l8suu4wNGzZw7bXX8oMf/IDk5GR6enpYvXo1+/fvZ+bMmVaPsWvXLjZs2MCePXvo7u5m7ty5zJs3D4ArrriC2267DYAf//jHPP7449xxxx1ceumlrFu3jquuumrAsdrb27nlllvYuHEjeXl53HzzzTzyyCPcddddgKqE3L17Nw8//DD3338///jHPzzwl6TxOfVWMmqkCtYJI7177tMHQPaqgH3x/3r3XMOJygMQGgnJ4+3vl3su7HoSutohPMqrQzprMmoYKH8YssdLL73E3LlzmTNnDgcPHhwgU1jyySefcPnllxMTE0NCQgKXXnpp37YDBw6wfPlyZsyYwXPPPcfBgwftjqWwsJDc3Fzy8vIA+NKXvsTHH3/ct/2KK64AdDvVoKe+DCITlI8aVEYNvtGpq01PfHueU1q5xjkqD6qy8VAHeWzuCtXDpXyn14fkVEYthPgO8FVAAp8DX5ZSDu4D6ix2Ml9vsn79eu6++252795NW1sbI0aM4P7772fnzp2MGDGCW265xWp7U3Nsrf59yy238PrrrzNr1iyeeuopNm/ebPc40kEJqtFSNTQ0lO5u38wsa7xAfamaSDT+38SZArW3deqeLqg9BnkXwpF3IP9xWHGv489pVKCe4MQ8Us5S5Ys//hHkLvfqkBxm1EKILODbwHwp5XQgFLjOq6PyEnFxcaxcuZJbb72V66+/nsbGRmJjY0lMTKSystJmYyaDFStW8Nprr9HW1kZTUxNvvvlm37ampiZGjhxJV1cXzz33XN/7ttqrTp48mZKSEo4dU/rhM888w7nnnuuhK9UEDA1lAxswxWeoV29n1HXHobcLpq2H8athx2PB07Wvp8tvXeporlYFSfYcHwZRiTBqrk8mFJ2VPsKAaCFEGBADVHhvSN7l+uuvZ9++fVx33XXMmjWLOXPmMG3aNG699VaWLl1q97Nz587l2muvZfbs2Vx55ZUsX95/F/3lL3/JokWLWLt2LZMnT+57/7rrruOPf/wjc+bMoaioqO/9qKgonnzySa6++mpmzJhBSEgIX//61z1/wRr/Ul/aP5EIquAFvF9GXm2agE6bBEu+pYLP5y/b/0zdcegKgMUwtjwAf10AvfYXnvYKVTZKx20x7lw4uQs6rPe69xhSSod/gDuBZqAaeM7GPrcD+UD+mDFjpCWHDh0a9J5maOi/0wCn9YyU9yVI+emDA9//fa6Ub9zp3XNv/oM6d0ezlL29Uj60RP3p7bW+f9FmKX+eLOUHv/DuuJzh2avU2KuP+v7cn/5Fnbu52rn9izar/QvfHfKpgXxpIwY7I32MAC4DcoFRQKwQYtCKrlLKR6WU86WU89PS0jx1H9FoghfD8ZE4euD7cZne7/dRXaAkl4hYpY8v+ZbKFos3Wdn3CLx0k/IOF2/27ricoeqweq30sDPMGSoPKmdObKpz+49eqBwiXpY/nJE+1gDHpZTVUsou4FXgHK+OSqMJBro7oLPV9nZjwQDLRQLiM7wvfdQUQlq/BMeMq1QA2vrXgfu11MDzV0NoBMy8Fir2eP8x3h7tjf1/b6cP2N/XG1QecF72AAiPhjGLvO6ndiZQlwKLhRAxQlkeVgOH3TmZ9HGz7eGM/rsMAN68C/65zvZ2Sw+1gbcz6t4e1YIzbVL/e2GRsPA2tVhrpcmC2tUOG25QN43rXoBZ14HsgdLPvDc2R1SZhZZKHwfqnm71JGKv0MUaueeqsbbUeGdcOBGopZSfAS8Du1HWvBDgUVdPFBUVRW1trQ4wHkBKSW1tLVFR3jXZaxxQvFlNJDWUW99eX6rW3ItJGfh+fIYK1L293hlX/Qnl7zXPqAHmfwXComH7Q6pZ1Bv/A2XbYf0jMHoBjF4EIWFwYot3xuUMVaabSPbC/gpBX1F7TK2N6GqgHrdSvR7/2O5uQ8EpH7WU8j7Aen21k2RnZ1NeXk51dfVQDqMxERUVRXZ2tr+HcfbSWAFNJvPTsQ9g3i2D9zHam1p67+NHKj24rc55LdQVjEKX1EkD349JVquU7H5a3UA+/xec9xOYroqriIiFrHmqK5y/qDqsVv2edCFs/LlaDNhaq1Fv0NeD2gXpA2DkbFXUdPyj/r9LD+OzEvLw8HByc3N9dTqNxruc3K1eQ8Lg6Pu2A7XlRCKYyshRkoNXArVhzcsbvG3xN2Hn47DjUZh9Ayz/7sDtY5fCp39WDZ0i4zw/NkdUHVJPApmmNg6VByFnmW/OXXlQ/XumWvl7s0domPp78+KE4llVQq7ReIyTu9SXevpV6gva0zV4H8tiFwNvl5FXF0L8KFWQYUnKeJj/ZZi8DtY9MDjbz1mmdOoyP+nUVYchfUp/5zpfTihWHlRPIWERrn923Llw5nj/vISH0YFao3GHk/nqEXnKOuhsGhzYOprUY3uSg4zaG1QXDJxItGTd/8F1z1kPSIZO7Q/5o7kaWmtUV7q4DKXt+9Ki56rjw5xcU1Wxl3RqHag1ziGlz1azCHh6e+HkHsiar76ghvxhTr0Nax70Z9TeCNRSKl+05USis0TGwag5/gnUxkRi+hSV6WdM911G3VoHjSfdD9TpUyA2zWvyhw7UGufY/Dv42zIVCM52ao+qLDprHkQlwOjFcGzjwH36il2sBOrwaIhM9I5Fr6FcrShjL6N2RM4yqNgNnS2eG5czGNa8dNOCspkz1NOBLxIE4ybhquPDQAjVTe/4R175juhArXGO8h1QfVhlHWc7J3ep1yzVi5yJa9QjeqPZorW2il0MvFX0Yjg+3M2oQQXq3m7f69RVh5TcEWfqh5IxXdkM64rsf84TVLrY48Mai74BlzyoeoB7GB2oNc5Ra/qy+KD3bsBTng8R8f3uAKMlZpFZVl1/QpUWx9popxCX4Z2M2rwZk7uMXgQi1PfyR9VhlU0bE5x9E4o+0KkrD6ibhCFLucPoBTDpArXavIfRgVrjmO7O/gyxPN+/YwkETu6CrDkQYvr6ZExX1YbmOnV9mZpIDLHxFYvP9FJGXaBuDjHJ7h8jMt73OrWU/Y4Pg9Q8pf/7okLx9OcDbxIBhg7UGsecKel/nDvbA3VXuwochuwB6ss9YY1qeGToqUaxiy2MjNrTemZ14dBkD4OcZcor7iuduqFc6f7mgTosUtnlvD2h2NEMp/ZD9gLvnmcI6ECtcYyhEWYvhFN7rXuGzxZO71f6bdb8ge9PXAPtDcq2B7aLXQziM5X+2t7gubFJaQrUQ5A9DHKWq4UHynZY397V3i+HeQLLiUSDzOneLyUv+0x5x31VWOMGOlBrHGN8IWdeo4KLr5vlBBKWE4kG41aqZZmOvq866rXWOMiojaIXD+rUzZXQ0eCZjHqMHZ26pxteuA4eWaqyUU9guC4sx54xXZXqe3PNx5ItSmIZvch75xgiOlBrHFNXpKrc8r6gfj+b5Y+Tu1TVn+UK4tEj1BPHsQ8cOz6gf0kuT+rUnphINIiMh1Gz4cSng7d9cJ+SebrbPOcMqToECVkQnTTwfV9MKJ74VGny/iiZdxIdqDWOqSuG5PHqUT4uw/+Bur7Ue53nHHFyF2TPs75t4holDRlZt91AbQr0Hg3UHrDmmZOzTP1bm/fc3rcBtv0V5txoqmD8xDPnqjo0UJ82MHzN3nqK62xR/14BLHuADtQaZ6gtVj0ihFATLif9GKjPlMCfZ8Phf/v+3K116qZlKXsYGDa9XU+pV3satVFG7sl+H9UFEJVk2xLoKmOXKZ263KRTn9wNb3xb6dfrHlALu3rCGdLTraoprQXquHS1zqSrE4rdnXDgFcc39LIdas5hrA7UmmCmq109yiePV79nzVN9e72pGdrjxDY18eMLb60lRsc8W4E6c5YKkmWfQUi4fU9uZLxqNdpkQ6Pe/ghs/r1r4zMcH56ymI1ZrHT3ki1qnC/eqG4wVz8FoeGQu1z9nQx1RZgzx6GnY/BEokHmdNd7fhx8FV6+FY68a3+/ki1Kix8TuPo06ECtccSZEkCqjBr6LUzG472vMbL52mN+OPcuQKj+w9YICYHxq9XPiVn2Cx+EMFn0rGTUnS3w4a9Vu9HuTufH56gZk6tEJahrLdoEL92sbs7XPdffmjVnuWlFmO1DO495jw9rZExXNyFX3EbG/88Dr9jfr2SL0uIj450/th/QgVpjH8OaZ2TUo+aoLMtfOrVRGVnjj0CdrzLWqATb+0xcq17t6dMG8ZnWM+pDbyhPcVdLv+zgiJYaaK31nD5tkLNUXXfZdlj/EIyc2b9t9CL15DBUnbrqMCBsjz1zhlp5peao88c0nn4K/2PbC97ZGhT6NOhArXGEYc1LGadeI+PUI6o/Ssk7W5VWGRKmbiC+nFCU0lSRaEP2MBi3ChDWmzFZYiuj3vOM+rwIhaIPnRtf30SiBzNqgNyV6nXZ3TD9yoHbImIgez4cH2qgPgTJ41SzKmsY/TecnVDs7lTS2Ki50NVqW/4o36E0+ADXp0EHao0j6oqU9cx8OaSseSpo+dp5cWqvetSesEb5uX3ZIKr+hMpYs+ba3y82BS57CBZ/w/ExrWXUtUXKLjb/FiUzFW1ybnx91jwPZ9QTVsOt76klu6yRs0z9u7Q3un8Oy9JxS1Lz1Crpzs5LVB9Wmvfibyp3zec25I+ST9XT4ZjFro/Zx+hArbFPbVG/7GGQvQDa633T1cwcQ26Zea169aVObWie2fPt7wdqXULD/2uPuAwlcZg/mu95VgWPWdfD+FVQsce5idvqQtUoKmGU431dQQg10WarZ0nOctVeoHSbe8c3KhxtTSSCmrhMm+R8Rm3IHtnzYNrlcOx9aKsfvF/JFhg5y76UFSDoQK2xT93x/olEA2NC0dc6dflOGJEDY5ao330ZqMt3QViU/YDiKpYLCPR0w74XYMJaFXDHnwdI1ePYEdUFao1EXzcVGr1QZbvurmxSe1Q9JdnLqAEyZjhv0avYrZ4AR+SqpdJ6OqHg7YH7dLUp7T0I9GnQgVpjj642aCwfnFGn5qlVl32tU5fnqx4b8ZkQHuvZXhOOOLlLZV+h4Z47puWSXEUboekUzL1J/T5qrlpgwBmd2lPNmFwlPFrduN31U9vq8WFJ5nRoqYLmKsfHPLnHNOktlFQ1IgcOvDxwn/KdKoAHgT4NPlyFXBOE1B1Xr5YZdUiI+gL4MlA3nFQ9H7IXqC9gyvihZdT1ZepRuqVG9eUwXBPtjepLPmG1sqaFhChb2Kl9MP9Wj10OMHiR2z3PQEwqTDSV6oeGKa9y0WY1mWkrW247o47h6YlEZ8lZDh//QckLliXgjqg6pJwjlv/HLDGfUIw7z/Z+na3qmHnfUb8LoSZBtzyg1mSMMxUDBZE+DTpQa+zRZ80bN3hb1nzY8n/qixER4/2xGP5pQ3ZJmaD0W3d58sL+nhwAYdHKHxwWBYVvw6ZfqaA5YbV6guhuczyR6CpGY6amShVECt+BRV8fuOjs+FVQ8JZ6ekidYP04paZ+G56UZVwhdzl89DulU0+60LXPVh02TRY6eFLJmKFeTx8wSUI2qDygpJRRc/rfm34lfPK/cOh1WHibeq9ki7L9uXpj8RM6UGts02fNs5LtZC9QX4hTe2HsOd4fS/lOtWJKpukLmzJBffG6O62vpm2PzlYVpBd+DZZ8SwXoiNj+7c3VqunQ0fdVk6X9L6r3nZlIdIWYZJVNNp+G/RtUKfOcmwbuYwSl4k22A/X2h1SjKGMlbF+TNV/92xz/xI1AfUg1s3JEbIpycDhyfvRVj5rdVNOnKlnowKsqUHe1q/9PRtAOArRGrbFNXZHKKqMSB28zgpavJhTL81WxhRGUUyYot8GZEteP1VCuXrPnw4ixA4M0qMfjmdfAlY/BPcfgtk1wwytK6/QkQvRb9PY8q25+6RY6c/I4SBprW6eu2KMm8hZ/w/UblqcIj1KTiq4WvnQ0qQZbjiYSDcYsUTq+vcVuK3arJxVz94sQalKxdKuS0E7mK/ve2KWujdeP6ECtsY3RjMkasakqcPlCp+7pgoq9A1fgSDFll7UuVKsZGCuEO1M9aOjxE9e4fh5niMtQwae6YHA2bTD+PJWtWiuh/vRBNbE77xbvjM9ZcleobNeVHjDGRKKzC8pOW6/mEU7Ymbg8udu6RDX9CvV68FWlTyNg7BLnx+pndKDW2KbOiofanOwFvun5UXlQacTm0oNxA3FnQrHBFKjtdbfzFfGZ0FKtGjQZwcSS8auU39ry6eVMiZJ/5t3ify9wzjJAwomtzn+m4G1VfensElgT1iq3z8HXrW9vb1Q3bnN92iBlvHr/wCsq88+cPrCIK8BxGKiFEJOEEHvN/jQKIe7ywdg03qSnG17/pnIzWKOzRVnFrE0kGmTNV9WBDV6uEDSydvMvdHSS6lTnTqCuL3Xc3c5XGBa9aZfbbgyUu0I5FIotqhS3PawCnTNVkN4ma56akHXWptfbC5+/rCZrjSZPjoiIUYtXHH7Duvxxaq96HWVj0nf6lUoqKt2mnCpBhMNALaUslFLOllLOBuYBrcBr3h6YxsvUFMLe5+CTP1nf3mfNsxOo+zrpeVmnLs9XPYktM+CUCe55qevLHHe38xXGAgK2ZA9Qmd+ouQN16tY6ZeebeY3nqxHdISzSNZ26dJvy6M+4xrXz2JM/jIlEaxk1qJshmPpPB48+Da5LH6uBIinlCW8MRuNDDH2w8D/Kh2uJZdc8a2ROV1Vp3tapT+b3+6fNcddL7WiFcF8y6zq48A+O/bzjz1Myk1EKvfNx1XDonDu8PkSnyV2u7HHO6NSfv6TknskXuXaOPvnDSq5YsVtNvMamWP9sYjaMOQelT/vAqeRBXA3U1wEvWNsghLhdCJEvhMivrq4e+sg03sXoAdzTqWxLltiz5hmERSrrkzdXiW6tU8HYmjUuZYJa0NXVhkANZc51t/MFSaNh0dccl36PX6VcLsc/VhWjO/4OE8933jHhC3JWqFdH8kd3p9KZJ1882HHjiD75483B8kfFHtvZtMHqn8LanytrZBDhdKAWQkQAlwL/srZdSvmolHK+lHJ+WpqHlgLSeI+qAkidBGlT1Dp4ltQVKbnBUUP11DzP9IaW0vr7fc2QrEw4Gc4PV5pDdXco7T1QMmpnyV4AEXFKp973gpqAPOfb/h7VQEbNUVmyI/nj2AeqqZersofBtMsHyx8tNepJyVFR0tglsPRO987rR1zJqC8EdkspPbi+vcZvVB2CjKkw+3rVl9cy2Nqz5pmTOlFlqOYLoLrKez+BR5ZaX+i1fKeaSLM6k29Y9FwI1IaHOikAHB+uEBquJsCObYStf1V/H4HWUCgsQkk0+1+yL398/hLEpKinBHeYaEX+MKpUbU0kBjmuBOrrsSF7aIKMzhZl7UqfqrIaEaKyNHMcWfMMUiYA0v2WpzXHYNtDUHUQnrli8Be8PF+NMzJu8GdH5ALCNZ3aFQ91oDF+leqLXVeksmlfd8pzhvN+ogpZPvyV9e3tjapUftoV7je4Co+GSRcMlD9O7gaEWlZrGOJUoBZCxABrAStipiboqC4EpNI3E0aqVUn2v9i/EEBHk9J+7Tk+DFInqldXlkkyZ/NvlNZ9xT+UB/b5a/r7M/f2miYSbZRuh0epzNidQB0IHmpXMcrJk8bClEv9OxZbpE9Wpdm7nrRe7l3wllr0YaabsofB1PVK/jBkloo9pq6Ogb32obs4FaillK1SyhQpZYO3B6TxAZatJWd/UckXhuZnWPOcyaiNfdxxX5z+XBUgLP4GzLwarnxcadIbblBacu0xaG9Qfm1bpExw7dwNZcp7nJDl+nj9TcoE9QR0/i9VZ71AZeX3ISoJ3vn+4LmH/S+pG42zRS62MOSPQ6+rc1TsdjyRGMToysSzkapDqkuc0bti8sWqDHmvSf6oc8LxYRARoxwU7mTUG3+p+ogYk2JTL4VLHlQTZq/eBmWmrnD2vtQpE5R8Ymsy0pL6UuU7DuRAZwshVP+RqZf5eyT2iR4B5/1Y3fgPvd7/flOlWgRhxtVDl23M5Y/6UvUE6OnuhgGEDtRnI1WHVe9io+AjPFoVEhz6N3Q090/O2atKNCd1gus9N0q3w9H/wtK7BraanHsTnP9rNZb//lDdQFLzbB8nZYIqr3amoTyoYpdg1KeDjXm3qNak7/2kf6L5wCvKYjhU2cPAcH9s/Yv6fZhOJIIO1GcnVYeVLc+cWddDV4vKUOqKVQcyZz2uKRNVRu1sVislbPyFKp9e9LXB28/5H1h+D3Q0qtJkW+v1ges9P+pLg1OfDjZCQuHC3yupaeuD6r3P/wWZMz23wMGENcqyuOtJtTK9M+tUBik6UHuLrnZ1p7fW8cyftJ1RK6VYFkqMWaKkkH0vqIzaGdnDIHUidDZbt9dZo2ijWml7xb22bwbn/Ri+8BtYfrf9Y/VZ9JwI1D1d6tp1Ru0bcpaqrHfLA1C8WenInsqmQT0J5l2gSsLTp6jfhyk6UHuL4s3w3o+dW5jUl1QVqFfL1UCEUFn18Y/VJJ+zsge41nK0t1dl00ljYO6XbO8nhGrqn7vC/vESR6sydmcCdeNJ9egdbB7qYGbtL9XrhhsA07JYnmTaevU6jGUP0IHae7TWqtfaYv+OwxKjdNxa6fHMawGpJBCXMmqThuzMhOLhN1THvpU/9Eyj+5BQdVNxpuil3rT0ls6ofUfSaFh2l3riylnm+QZSE9ao/h2BPsE6RIJw6jtIMAJ1XaAF6sMQEa8a1FiSnKv+05dudc6aZ5AwyrQquIOstqdbFUKkTfbsI3DKBOduEsHsoQ5mlt6pCpcW3u75Y4dHw63veP64AYbOqL1Fm6nCzt2KPW9RXaCyaVv2qLk3q1dnV92A/lXBa47Y36/wbSWPrPqRZ1uMpkxQN8TeHvv7NZQBwvpNSuM9wqPhxpch73x/jyRo0YHaWwRiRi2l6nRnr+ParOvgjt2uSR+gJhQdZbXFH6lsfvLFrh3bESkToLerP2O2RX2pWiwgLNKz59dovIwO1N7C6Flx5oT9xTh9SUu1yvQtJxLNMbJjV0mZqAJhV7vtfU5sVc3lPd2w39nmTIHUh1qjcQEdqL2F0Yy/t8v0yB0A9E0kTra/nzukTkQ1Z7LxBNFaB9WHvdOw3VnXifZQa4IUHai9RWstxJjWggsU+cOyx4cnMZoz2QqWpdvUqzcCdWwqRCban8zs7VH2PJ1Ra4IQHai9RWtdf4+KgAnUh1Qf4FgvLOxgZLW2JhRPbFV+Z2/4XQ25xl6gbjqlCiO0h1oThOhA7Q2kNGnBk5VtLWAC9WGVTXujj3FErOpIZ2u1l9JtqgteeJTnzw2OF7rVHmpNEKMDtTfoaFTZW0yK88UY3kZKU6D24hp7KTaaM3U0Q8VetQySN89tb6WZPg+1DtSa4EMHam9gOD6ik1URSSBk1A1lqjrMm4E6daL1lqPlO0H2eHfl55Gz1KuhhVvSYKzsoqUPTfChA7U3MAJ1TIrSTs+U+N+i582JRIPUPOhoUDZAc05sVct9ZS/03rnHnatkpoK3rG+vL1Xa/DBu3KMZvuhA7Q2MqsSYZCV99HZBY7l/x2RY89K8YM0zsDWhWLoNMmdAVIL3zh0eDRPXQMHb/UuKmaP7UGuCGB2ovcEA6cPol+xnnbrqMMSPGtik39NYWz+xu1NJH2OXeu+8BpMvUSt9lO8cvE17qDVBjA7U3sAoHzcyavC/Tl11yLv6NEBCNoRFD7TJVexRi5mO8eJEokHe+RASDgVvDny/txcaynVGrQladKD2Bm11SpONSlK9JcJj/Buoe3ug+oj3A3VIiKk5k1lGXbpVvXpzItEgKlFp1YffHDih2VIFPR06UGuCFh2ovUFrnVrgMyREeZaTx/k3UNcdV4HKmxOJBqkTB1r0TmxTk4yxqd4/N8CUS9TkbeXB/ve0h1oT5OhA7Q1aa5U+beBvL7W9xQI8TcpEFSi7O1QmX7rdN7KHwaSLAKGyaoP6E+pVa9SaIEUHam/QVqf0aYPkcSp4OeqX7C2qDgPCc4uK2iN1olruqu64ukF0NPhmItEgLl3dGMxtekZTLO2h1gQpOlB7g9Y65aE2SBlv6qLnJ4te1SG1cK2zq4oPBfNOdicMfdqHGTXAlHVQeaBfbqovVVJUZLxvx6HReAgdqL1Ba91g6QP8t9pLdYF3/dPm9HmpTYE6cbTvteHJ69TrYVNWrftQa4IcHag9jdGQKWZE/3vuWvR6e+HNO5XFzV16TTJE6gT3j+EKUQkQl6kCdek23+rTBiPGQubMfvmjvkzr05qgRgdqT9PVqnzD5tJH/EiTv9jFQN1UAbueGjgx5iqNJ5Xjw7hZ+ILUiVC0URWf+Fr2MJhyKZR9Bo2nTBn1WP+MQ6PxAE4FaiFEkhDiZSFEgRDisBDCT9++IMC8KtHAXYueYStrOOn+eIxz+jpQN1eqn305kWjOFJP8secZ6G7TE4maoCbMyf3+DLwrpbxKCBEBxHhxTMFNX5+PlIHvp4yD6kLXjmW4FRqDLFCnmErJY1KUh9ofpE1WevmOx9TvWqPWBDEOM2ohRAKwAngcQErZKaWs9/K4ghfz8nFz3LHoGYF6KGsu1hVDaKQq7/YVRs+PMUu8s0iBMwihJhVbqtTvWqPWBDHOSB/jgGrgSSHEHiHEP4QQPvB5BSnWpA9QzZl6Ol2z6BnSR2OF9Y5wzlBXrKx5IT6cjjAcJjnLfXdOa0y5pP9nnVFrghhnvr1hwFzgESnlHKAF+L7lTkKI24UQ+UKI/OrqasvNZw+tNqQPd5wfRlDv6YTWGvfGU1fsW9kDlB5863sw/1bfnteSUXNVx8DIBO92DdRovIwzgbocKJdSfmb6/WVU4B6AlPJRKeV8KeX8tDQvLJ4aLBgadfSIge+nmNqduuKlbiiDsKj+n13FsOb5OlADjFkEYRG+P685ISGw5Fsw/Qr/jkOjGSIOA7WU8jRQJoQw6o9XA4e8OqpgprUOIhMh1GKeNi5TWfTqjjt3HClVRp01T/3ujvOj+bRyPKT4IVAHCuf8D1zyZ3+PQqMZEs4Kl3cAzwkh9gOzgd94bUTBTmvt4IlEUNmdK82Z2s6oNQ7HLFa/u+P8MM7lj4xao9F4DKcCtZRyr0nWmCmlXC+lPOPtgfmVQ2/APy+FrjbXP2vZkMkcVxa6NfTpzJkqE3enT4g/rHkajcbj6MpEa+x8DI5/BFv/6vpnLRsymZM8Ds4cd86iZ97xLTHL/UAdEq6taRpNkBO8gfrYRnjmCvdta7ZorYOST9Uk3pY/ua4NWzZkMifFZNFzRsYwAnPiGEjMdjNQF5mseaGuf1aj0QQMQRyoP1D9JNo8rMIcfQ9kD6x/RGW+H/zMtc/blT5MEoQzOnV9qbpZxKaqYhV3NGp/OT40Go1HCd5AXV+qXg07nKcoeFs1UZq6Hs65Az5/Ccp2OPfZ7g41AWgzUBsWPSd06oZylUkLoaSPptPQ0+XcOEC5RvzhodZoNB4neAO1IQW0ejBQd7UpSWXSRcqlsew7Kmi/8/+ck1hsVSUaxI9UWbJTgbpMBWowvUpVoegszZWqk5/h39ZoNEFLEAdq02SbJ6WP4o+gqwUmX6x+j4yDNT+Hit2wf4Pjz/c1ZLIRqA2LntMZtWkSMCFLvboif/RZ83Kd/4xGowlIgjNQd7b0Nz/ypPRR8JYqNzbvUTHjasheoLTqjib7n+9ryGTD9QHOBequdpURG4HayKxdmdjU1jyNZtgQnIHa3AHhKemjtwcK34GJaweWPoeEwAW/V4Hzk/+1fwxH0gf0B+qebtv7GJlzkkVG7UoZeV0xhIQp14hGowlqgjRQmwUsT2XUZTtU4yND9jAnex7Muh62PWS/BNxWL2pzMqYri17tUdv7GNdnZNSRcRCV5Jr0UVekVjWxLGXXaDRBR3AGaqP9pwjxXEZd8JYqDpmw1vr21fep7Zt/a/sYtnpRm5M5Q72e/tz2Pn0earMe0onZrksfWvbQaIYFwRmoG8pAhKpiDk9MJkqpbHnjzlWLs1ojYSRMukAt2GqL1jMQHgthkbb3SZ2oGvmf3m97n/oyQPRLHuBa0YuU2kOt0QwjgjRQl6sgFpvmGemjukCVdluTPcxJn6r827YmFVtr7cseAKHhkD7FcUYdnzlQK0/IgkYnA3VLtfJza2ueRjMsCM5AXV+mJtqik1UWO1QK3lKvky6yv1/6VPVaVWB9e1sdxIywvs2czBkqUEtpfXtD6eD+HIlZpo56LY6Pr7vmaTTDiuAM1A1lKpDFJHsmoy54G7LmqyzWHulT1GuVjXbc9vp8mJM5U2XfTaesbzeqEs0xArczOrW25mk0w4rgC9Q93apCL2m0WkVlqBp1QzlU7HEse4ByUYTHQtVh69udkT7A/oRib68aU5JFRt1X9OKE/FFXrDR8vU6gRjMsCL5A3XRKNU1KzFaBuqtVFYi4S+E76nXyOsf7hoRA+mTbGbW9hkzmZExTr9YmFFuqlH3PmvQBzmfUSWOUHq7RaIKe4AvU5h5jIygORf4oeBtSJkJannP7p0+xnlH3dEN7g3MZdVQCjMi1nlH3WfMsAnX8KEA45/yoK9Kyh0YzjAi+QG14qJPG9OvB7nqp2+qh5BPnZA+D9Kkq622xWBXckGCc0aihf0LREqMroKVGHRYBcRmOpQ9tzdNohh3BF6gbzAKZsdK3uzp10YfQ2+3Y7WGOrQlFRw2ZLMmcqSQKS6ufkTFbatRgWunFgfTRWgsdjdqap9EMI4IwUJdDTCqERw9d+ijepFYMN1b6doZ0k75sKX84U5VojjGhWGkR8BvKVGOoqMTBn3Gm6EVb8zSaYUfwBWrDQw1Dkz6khKLNkLvctX4YcenqvJYZtTMNmczpc35YTCiatze1xFjpxZb/GrQ1T6MZhgRfoDY81DC0jLq2SMko41e59jkhlE5tmQm7Kn0kjFJB3VKnri8brE8bJGYpl4s9qaeuWPVASRrr3Dg0Gk3AE1yBWkpTRm3yB4dHqxVT3NGoizep13EuBmrod36YZ7bO9KI2RwjrE4oNZdb1aTDrS21H/qgrVjcy8/JzjUYT1ARXoG6tg+62gRmnu2XkRZtUwHdHIkifAp1Ng/tih0ZCeIzzx8mcoSQUozd1RxO019vOqBNM79trd6qteRrNsCO4AnWf48Ms43SnjLynS9nyxp+nMltXybAyodhWp7JpV46XORO626H2mPrdlofaoK/oxUZGLSXU6vamGs1wI7gCdZ+H2iyQRY9wfTLx5C5lYXNH9gBIm6xeqw72v9fqZFWiOZal5PVmxTzWiE1XPbFtBeq2M9DRoK15Gs0wI7gCteXKJ+Bev4+iTYCA3BXujSM6SfXeMM+oW+v6fd3OYtmbusHKjcickBA1CWlL+tCOD41mWBJkgbpcNUUyD4juSB/Fm2DUHNczYHPSpwy06DnbkMkcy97UDWVqncO4DNufseel1h5qjWZYElyBur5UZZvmOnB0ssqo7XmLzWlvgPJ8pU8PhfQpUH2kfyLQ2YZMlpj3pjYWRAgJtb2/vSW5ao8CQq18o9Fohg1OBWohRIkQ4nMhxF4hRL63B2UTcw+1QUyyKgPvaHTuGMc/Ud33XPVPW5I+DXo6lNzQ26tuFs4Wu5iTOVMtqtt0eqD10BYJWdBUoVZNN6erHfY8B2MW218KTKPRBB2uZNSrpJSzpZTzvTYaR1grBnG130fxJiWfZC8c2ljMe36014PsdV36gIETitYWDLAkMUvdmJorB76/60kVwFf90PUxaDSagCZ4pI/OFiUvWE60uVpGXrQJcpYOvSAkbRIg1ISicZNwR/owrH4Vu1WgteX4MLC20ktnC3zyv2py1N0JUo1GE7A4G6gl8J4QYpcQ4nZrOwghbhdC5Ash8qurqz03QoM+j7GFNOBKGfmZE6ogZKj6NKiqyORxKqM2bhLuZNRGb+qj76ms3FFGbW2ll8/+rha0Pe8nrp9fo9EEPM4G6qVSyrnAhcC3hBCD0jYp5aNSyvlSyvlpaWkeHSRg3UMNZhm1E9LHUMrGrWE4P4zycXc0alDyx8ld6mdb1jwDy6KXtnr49M8w8QsweohyjkajCUicCtRSygrTaxXwGuD7iGDeh9qcvozaiUBdtAniR5pkCw+QMU1NJhq+ZmdWILdG5sz+nx1JH1FJEBHXL31sf1hp5Of92L1zazSagMdhoBZCxAoh4o2fgfOBA94e2CAaypXHOH7kwPejktSrI+mjtweOf6SyaXfKxq2RPkXJFaXb1e/uSB/QP6EIjqUPIZT80VgOLbWw7SGYuh5GzrT/OY1GE7Q404g5A3hNqOAWBjwvpXzXq6OyRn2Zqsqz9BiHhqnm/44mE0/tU1m3J/Rpg/Sp6vXEp+omEpng3nGMQG0siOCIxCx14/r0AdX2VDs9NJphjcNALaUsBmb5YCz2aSgbPJFoEDPCcUZd9KF6HbfSc2NKHgehEUr6iE1zP1M3elM70qcNErNV0U5VAcy81nNSjkajCUiCx55nr6G+UZ1oj+LNkDED4jw40RkaDqmm1cvdlT1ABfi5N8GUS53bPyFbFfj0dsG5/8/982o0mqDAhTWo/EhPt/IY28o4HXXQ6+mGss9goVVn4dBInwqVB9x3fBis/YXz+xrOjzk3QXLu0M6r0WgCnuDIqJsqTB5jG4HaUWOmhlLo6eyvJvQkxjGH0uDJVXKWKQnn3O/57pwajcZvBEdGbctDbeBolRdvtv80JhR9GahH5MDN//bd+TQajV8JjozaWh9qc2KSVcN8o5OdJXXH1atXArUpox6q9KHRaDQ2CLJAbWsy0VRo0l5vfXttkVrL0F6fZ3dJHA1zboS8Czx/bI1GoyGYpI/YNNseY/PGTLGpg7fXmdYR9FShizkhIXDZQ54/rkaj0ZgInozaXsWeUbpta0Kxrtjn7ohjVc0s/d2HlJ9p9el5NRrN8CNIAnW5/R4Y9lqd9vbAmRJI9u2Cr9uKazlZ38ae0nqfnlej0Qw/Aj9QS+l45RN7iwc0lKvCEB+vI3i0sgmAE7UtPj2vRqMZfgSWRn3kv6qXR3gMhEUpTbq7A7rb7GfU9npS+2ll7qOVzQAcr9HSh0ajGRqBFahfuhm6261vGzHW9uciE1RTJGvSh78CdZXOqDUajWcIrED9lfegq0396W7v/1mEwPjVtj8nhJI/bGXUYVGD26N6kbqWTmqaOwkNEZQMMVDXNncwIiaCkBAvOFY0Gk1QEFiBeuQQmvRFj7CuUdcdV0tdhfhOjj9i0qcX5iSzrbiWpvYu4qPCXT5Oc0c3K/6wiXu/MIlbluqeHhrN2UrgTyY6S3SybenDTxOJ509TBTYnat3TqY9UNtHS2cOmQi+sQanRaIKG4ROoY6y0Ou3thTPHfe6hPlrVTHxkGIvHqdan7sofR06rgJ9fUkdXT6/HxqfRaIKL4ROorWXUTRVK6/ZxRn2ksokJGXGMTYkBoKTGzUBtco60dPZw4GSDx8an0WiCi2EUqJMGZ9R+tOblpccTExFGRkKk2xa9I5VNZCWpsvntxQ5WsLHDsapmTta3uf15jUbjX4ZPoI5JVn7rLrOAZATqFN9VJdY2d1Db0snEjDgAclJi3bboHalsYtG4ZCamx7G9uNatY0gpueXJHVz9yFbqWjrdOoZGo/EvwydQWysjrytWaxomZLl0qGNVTTz16XG3hmHIFXkZ8YAK1O5o1PWtnVQ1dTApI57F41Lc1qkLK5soP9NGRUM7d27YQ0+vdPkYGo3GvwyfQG2tOrGuWDXZt1y53AF//6iYn715iPpW1zPQY6ZCl75AnRpLTXMnTe1dLh3HPOAvHpfitk79YUEVAHetmcgnR2t4cONRl4+h0Wj8y/AJ1Nb6fdQdd0uf3maSGYqqXc+Ej1Qqx0dGQiQAOaYJRVcteoYXOy8znkXj1E3IHZ16U0EV07MSuHP1RK6al82DHx5lc2GVy8fReIeO7h6Kqpv9PQxNgDOMArWF9CGlWx7qsrpWys8ondudL9CRyiYmZsQhTL2vc1JjAdctekcqm4iLDGNUYhSpcZHkZbiuU59p6WTXiTOcNykdIQS/vGw6kzLiuevFvbr9aoDw9NYTfOH/Ptb/Hhq7DJ9AbSl9NFdCV6vLgXqbWTB0J1AfrWrukz0Aty16lgHfHZ3646PV9EpYNTkdgOiIUP524zx6eiTfem43Hd09Lo1J43k+O15Hd6/k33sr/D0UTQATUIG6vauH1k4b6x46wpA+jIy6z5rnWrHL9uJaUmIjmJAeR7GL0kdtcwd1LZ1MNAvUhkWvxGXpQ1n8DNzRqT8sqCIlNoJZ2Ul97+WkxnL/NbPYV97AL9865NKYNJ5FSsnesnoAXt1djpTBP9G7s6SOXSfct5JqrBMwgbqxvYtV92/m0Y+L3TtAeDSERXO68hQ7S+rc8lBLKdleVMvicSlMSItzOaPunwCMG/B+TkqsSxl1jSng52X2B+qFua7p1N09vXx0pJqVk9IHNXT6wrRMvrZiHM9uL+Wdz085PS6NZyk/00ZNcwfTRiVQVN3CgZON/h7SkJBS8p0X9/LN53brSloPEzCBOiEqnBlZiTy+5TgNba45JAzawxPZsv8otz+dT3d1kWp9mmhnwQELSutaqWhoZ/G4ZManx1Ja2+rSfzijtelEs0wYXLfoGaXj5gHfVZ16T1k99a1dnGeSPSy59wuTmJwZz2/fKaCz23dfql5tD+zDyKZ/eNEUIkJDeHVPuX8HNESKqpspP9NGZWMH7x2s9PdwhhUBE6gB7lqTR1N7N49vcd3DvOVoDcdbI8kMb+VMaxcVxQfUqjChzjcINILgkvEpjE+Lo7tXuuTWOFLZRHxUv+PDwFWLnuH4mJQxMOC7olN/WFBFWIhgeZ6VxX6BsNAQvn/hZErrWnn+sxNOjWuobC6sYsbP/kupm02qhht7y+qJDAthYW4y501O5819FXQHcSa62dQ8LDUugn9uK/HvYIYZTgdqIUSoEGKPEOItbw1m6qgELpyeyRNbjrvkYd5ZUsdtT+fTHp7I4kyYMjKBjqpjSBfXSdxWVEtqXCTj0+IYn6ayWVfkjyOVaiJRWKx27qpF70hVM4nR4aTFDwz4hk79uRM69aaCKhbkJJNgp73quXlpLBmXwoMfHnPZ523Q2d3rdEb+1v5TtHT28Mz2ErfONdzYU3qGGVmJhIeGcPncLGqaO/nkWI2/h+U2mwqryMuI4/YV49hxvI6C08Et5QQSrmTUdwKHvTWQvpOsmUhzRzf/+MS5rHpfWT1ffnInI5OimDIuh7DOBm49ZyyZPac4Fer8YgFSSrYV17J4XDJCCMalKVudK4H6WFXzIH0aXLfoHTndxCQrAb9fp7Yvf5ysb6PgdJNN2cNACMH3L5xMXUsnj7kxN7C3rJ4lv93IT14/4HBfKSUfHVEZ10v55bR1nt2Ok87uXg5UNDJnTBIAqyalkxQTzmu7T/p3YG7S3NHNjuN1rJyUzjXzRxMZFsLT23zzpHY24FSgFkJkAxcD//DucGByZgIXzxjJk58e54yD3hSHTzVy8xM7GBEbznNfXURUfAq01nHJxAjiRRsfVcfb/bw5JbWtVDZ2sGS8ak0aHxVORkIkRVXOBVdjAtBSnwbXLHpSyj5rniX9OrX9CUWjGnGVg0ANMGt0EhfPHMljnxynqsnGMmhWeP9QJdc9uo3alk7e/vyUQ6vf4VNNVDd1cNW8bBraunhz39ltRys43Uhndy+zRyu3UkRYCOtmjuS9Q6dp7nDN+XSyvo31D33KsSr/Fc5sPVZDV49k5aQ0kmIiuGz2KF7bfdLt+SbNQJzNqB8AvgfYfMYVQtwuhMgXQuRXVw+t0f2daybS2tXDY5/YzvKOVjZx0+OfER0eyvNfXczIxOi+ntRRjSUAvFcZ47TbYluRSZ829ZAGGO+C88PQla0FWFcsepWNHTS2dzMp0/pNxhmdelNBFWNTYhhveipwxL3nT6Krp5c/f+Bcefkz20r42jP55GXE84crZ9Lc0e3w5vHxUfV/4p7zJ5GXEcfT20uGhR3NXfaU1gP0ZdQAl8/Jpr2r12UnznPbT7C3rJ7Ht7jpmPIAmwqriYsMY/5Y9dR385Ic2rp6eHlXcE+QBgoOA7UQYh1QJaXcZW8/KeWjUsr5Usr5aWlpQxpUXkY862aO4qmtJdQ2dwzavvVYDVc8shUQPHfbIkYnq4yV6GSQPVCxF4ByRvLU1hKnzrmtuJaMhEhyU/uDmxGonQkoRy2aMVky1kmLXl/At5KZgwrUrXZ06rbOHj49VsMqUzWiM+SkxvLFRWPYsLOMYjs3pt5eye/eKeAn/z7IqknpbLh9MZfOHkVMRCjvHzpt9xwfFVYzOTOezMQoblqSw4GTjewxuR7ORvaW1ZMeH8nIxKi+9+aOSWJsSgyv7XFe/uju6eVfpmD4+p4Kv2SwUko+Kqxi6YQUIsJUSJmelci8sSN4ZluJdvp4AGcy6qXApUKIEmADcJ4Q4lmvjgq4c/UE2rp6eNQiq35pZxk3P7GDzIQoXvvmOX2TfkB/deLJXSBCmDVjJi/vKnc4USalZJvJP20e3ManxdLU3k21lZuFJUcqm0iICiPdYgLQIDcl1qmMuq/Hh5XMHGCRA516W3ENHd29DvVpS+44byJRYSH88b+FVrd3dPdw14t7+dtHRdywaAx/v2keMRFhRIWHsmJiGu8fqrT5hWzu6Cb/RB3nTlI38MvnZBEXGcYzZ7GGubesntmjkwb8fxNCsH52FtuKaznV4Fz/8E2F1VQ3dXDn6om0dfXw6m7fZ7BHKpupaGhn1aSB/+duXjKWktrWvqcpjfs4DNRSyh9IKbOllDnAdcCHUsobvT2wCenxXDZrFE9vPUFNcwe9vZLfv1vA917Zz5LxKbzyzXP6M2kDozrxZD4kjubmZWpi8l/59v/zFlW3UNPcMUD2ABifbnJ+OKFTG6XjtrLYsakx1DR3OLxpHKlsIjUugpQ46wE/JS6SSRnxNqWGDwuqiIkI7Wvk5Cxp8ZHctmIc7xw4ze5S1diqvauHDw5Vcu+/9rH4Nxt5Y18F37tgEr9aP52w0P7/OmunZlDZ2GEzy99WVEtXj+TciSpQx0WGceXcLN7ef4oaJ26Cw40zLZ0cr2lhzpgRg7ZdPicLKXG6pPzFnaWkx0dyx3kTmD06iWe3n/C5pLTJ1OTLuBEbXDh9JKlxkXpS0QMElI/akm+vnkhHdw8PbjzKHS/s4ZHNRXxx0RieuGWBdduZ0ZjJ1IxpZnYS88eO4KmtJXb7MG8z80+b46xFT0rJ0cqmAaXjluSmKEnFkUWvsLLZpuxhsHhcMvkldbRYTDpJKdlUUM2yCalEhrnW2hXgq8vHkRoXwU9eP8A3nt3F3F++z1efzufdg6c5Ny+NZ76ykG+unDDoZnTe5HRCQwTvH7Je5PDxkWpiIkKZl9MfmG5aMpbOnl5e3Fnm8ji9xd6yelb8YRNbvWyR21teD8Ds0UmDtuWkxjJ3TBKv7T7pMOCebmjnw4IqrpqXTVhoCDctHktRdcuAfjW+YHNhFZMz49U8kRkRYSF8cdEYNhVWub14hkbhUqCWUm6WUq7z1mAsGZcWx/rZWTy97QT/OXCKH100hV+vn054qI1hx5hlkabS8S8vzaW0rrXPCWGN7UW1jEyMYoxFhp6ZEEVMRKjDQF3T3MmZ1i4mpluXK8A5i56UkmOVTTYnEg2WTkiltbOH2b94j6se2cof/1vAx0eq2VNWz8n6NpdlD4O4yDDuWpPHwYpGdp04w+Vzsnj61oXs+vFaHrhuDssnWp97GBEbwfyxI6wGaiklm49Ucc74lAE3jwnp8ZwzPoXnPysNiMUMKhvbuf3pfErrWvnDfwu9mpXuLa0nRMDM7ESr2y+fm01hZROHTtn3Ib+yu5xeCdfMHw3AxTNHkhQTzrPbfZfBNrV3kV9yxqbD6IZFYwgVwqdjGo4EdEYNqlpx7pgkHrlhHretGGd/gix6cKD+wrQMRiVG8aSNFVuklGwvrmWJhT4NEBKi/NSO+lIfrRy4WIA1nLHonaxvo6Wzx6pzxJw1UzL4560LuXVZLt29kr99VMzNT+zgioe3As7Z8mxxw6IxbL5nJdt/sJpfXz6DFXlpfRNE9lg7NYPCyqZBmVNJbStldW2cmzc4yN+8ZCwn69vYeNi/5cbtXT3c/swumju6ueWcHPaW1fPpMe9lpXvK6snLiCc20nrV7LoZIwkPFXY91b29khd3lrFkXEpfEhAVHsq180fz34OVVDY6b7UcCp8eq6G7V7LSyr8vQEZCFF+YnsmLO8v6vPO9vZK2zh7qWjqpqG+jrK6V4zUtHKtqpvB0E4cqGh1ac882nK+v9hNjUmJ49ZtLnds5yixDMa2TGBYaws3n5PC7dwo4VNHI1FEJAz5ytKqZ2pZOFlvIHgbj0+LILzljdZv5McD2BCA4Z9GzVTpuSUiI4Ny8tL7g19LRza4TZ9heXEtsZBgZCVF2P28PIUTfF98Vzp+aya/ePsz7hyr56vL+RlgfmfTLFVa+yGumZDAyMYpntp/g/GmZbo95KEgp+eGrn7OvrJ6/3TiPVZPTeOfAKf666SjLJlovvx8Kvb2SfWX1XDTD9vWOiI3g/GmZPLP9BOvnZDE9a3Dmvb24ltK6Vr57ft6A97+4aAyPflLMCztKuWtN3qDPeZpNBdXER4Uxd+xgvd3gS0tyeHv/KRb+5gO6enpp73JcyZoYHc77d68gPd79/8vDiYAP1C4RGqaCdXvDgK551y0YzV8/PMYX/7Gde86fxPULxxBq6ihnzT9tzvi0OP69t4K2zh6iI6zrvkcqm6yWfFviyKJndN+zp3VbIzYyjBV5aVaDoa8YkxLD5Mx43rMI1B8frSEnJYaxKYODf1hoCDcsGsP97x2huLqZcWn2nyS8wWOfFPPqnpPcvTaPC6ar4Hnb8nH86u3D7DpRx7yxrk3KOuJ4bQsNbV1W9Wlzfn7pNHafOMPXntnFW3csY0RsxIDtG3aWkRgdzhcsbnBjU2JZMTGNF3aU8q1VE2zLhB7AkLWWT0y1e54FOSP47to8qpo6iI4IJSo8lOjwUKLDQ4gKDyUsNITQEAgRgrCQEDq6e/h/r+znT+8d4XdXzvTa+IOJgJc+XCY6GRCQNLbvraSYCF795jlMzoznx68f4LKHtrDrhMqStxXVkpUUPdhBYsKYUCyusa1TH61UpeOOfMuOLHpHTjeRmRBFYrTt/hyBzNqpGeSX1PWtdt7e1cO2olqrsofBtQvGEB4q+MeW4z73224qrOK37xRw0YxM7jhvQt/7X1w0huTYCP764TGPn3NvX6GL7QwUVBXq326cR3VzB9+2WJT4TEsn7x44zeVzsogKH5w83LR4LJWNHV6XlA6faqKysYOVkxy3Krhj9UR+uX46P7xoCnevzeMbK8dzy9Jcrls4hqvmZXP5nGwum53FxTNHcsXcbG5eksOL+WUcqvB8vxBn5x86u3v5yesH+NP7R9hx3L3FpT3F8AvUMcmQmA3hAx+Z8jLieeG2xTx4/Ryqmzq48pGt3POvfXx2vHaQ28Oc8elGzw/rmbCUkiNVTUxw4NQAxxa9I1XWS8eDhbVTM+iV/SXs+SVnaOvqGWTbMictPpL1s7N4/rNSVt6/mb9sPEpFvWMPsZSSivo2NhVW8fePirj7pb3cuWFP303CEceqmvn283uYnJnA/VfPGnCTjYkI4yvLctlUWO1woQZXJx33ltUTFxk20P9vg1mjk/jVZdP55GjNAH/763tP0tnTy7ULRlv93KrJ6WQlRfOMlyfwDFueLX16KHz7vIkkRYfzq7cPeWxid9eJOq56ZCur//QR7V2Oe828srucZ7af4C8fHuWav29j9s/f49andvL4luMUnG706ST48JI+AHJXQKf1rFUIwaWzRrF6cjoPfniUJ7Ycp6tHstiG7AGql7QQ2KzYM/o+m5cC2xyamUXPUnfs6ZUcrWzmpsVjrX00KJiRlUhmQhTvHzrNVfOy+ehIFRGhIXb/fgF+dfl0lk5I5aX8Mv73/SP86YMjLJuQyjXzR5M1IprTDe2camjnVH0bpxrbqahv41hlM01m9sSMhEjOtHZx8kwbz351kdVM06CqsZ2v/nMnEWEhPHazKtyx5KYlY/nbR0U8tOkYj9w4b9D2htYuvr1hD22dPTz71UVOTbgC7Ck7w6zRiX3SmyOuWTCaveX1/O2jImZmJ3Lh9Ew27ChjVnYiU0YmWP1MaIjgi4vG8Mf/FlJU3ezUTcEdPiqsZtqoBNKHMCdii8SYcO5ak8d9bxxk4+Eq1kzNsLlv4ekmYiJCyR4RbfWp9nhNC79/p4B3D54mOTaCupZOnt5Wwu0rbHfX7Ozu5a8fHmPW6CSe/vJCthXX8umxGj49VtOXiMRGhDI9K5HZo5OYmZ3EzOxEm2MYKsMvUK/5mcNdYiPD+MGFU7h63mje3Fdhd2InKjyU0SNibGbUL+4oIyYilItmOO7UZ+i0JbUtgwJ1WV0rHd29dp0jgY4QgjVT03ll10nau3r4+EgNC3JHWA2E5kSGhbJ+Thbr52RRVtfKv3aV83J+GXe8sGfAflHhIYxMjGZkYhSXz80iLyOeSZnx5KXHkxgTztv7T/Gt53dz78v7+fO1swetbANQ3dTB9Y9tp6qpg2e+sojsEdYlr4SocL60JIeHNh/jmMUTU2ltK19+agcnalvp7pU88MERvnfBZId/P+1dPRScauJr57q2jud9l0zl8KlG7vnXPlo6uimsbOI3l8+w+5lrF4zmgQ+O8Nz2Un56yVSXzucMDa1d7Co9wzfOda2VsCt8cdEYnt5Wwm/+c9iq+0hKycObi/qeNkYlRrFoXAqLcpNZNC6F+Kgw/rLxKM99VkpkWAh3r83jq8tz+fqzu3l4cxHXLRxjsw3wK7vLOVnfxq8un05iTDgXTM/sm8M4Wd/G9qJa9pfXs6+8gSc/LaHTJItkJUXzyfdWWf2/NxSGX6B2gQnpcXxnreOZ8fFpsRRZ6UzW3NHNm/srWDdzJHE2rFbm5KTatugVGhY/Bx7qQGft1Eye3V7Kv3aVU1jZxJXzHAcwc0Ynx3D32jzuXD2Rz47X0t7VQ2ZCNKOSlHZvL1u5eOZISusm8/t3CxibHMM9X5g0YHtdSyc3/uMzTta38c8vL2SeHacCwK3Lcnl8y3Ee3lTEn66dDcDu0jPc9s98eqTkua8u4pXd5fztoyJWTU5nQY79iccDJxvo7pV9HfOcJTIslEdumMe6v2zh3pf3Ex0eyiWz7CcGqXGRXDRjJBt2ljI6OZobFo11Out3ho+PVtPTq7rleYvw0BB+dPEUbn0qn+c+O8GXl/avf9rbK/n1fw7z+JbjXDprFPNzRvBZcR2fHK3u65USIlTycP3C0dy5Oq9vsv97X5jEur9s4bGPi/nu+ZMGndc8m7Ym62QlRXPlvGyunJfdt3/h6Sb2ltdT39Lp8SANZ3mgdpbxaXFsK66lt1cO+Ed4e38FrZ09NrVCS+xZ9I72NWMKXo0alHsmPjKM/31PZTnn5rnn6Q4NEZwz3nV73NfPHUdpXQt/3XSMMckxXGP6t6lvVUG6pLaFJ25ZwCIHcgxAcmwENywaw5NbS7hrTR6fn2zg7pf2kpkYxZO3LGBcWhzTshLZXlzHd17cyzt3LifezkINRsc8R44Pa2QmRvHwDXP54mPbuWz2KLvnMfjeBZOpburg528e4olPj3PP+ZO4ZOaoIQeSqsZ2fvOfw2QlRbt1La6walI6yyem8sAHR7l8ThZJMRF09fTy/17ez6t7TnLLOTn8dN1UQkIENy/JQUpJUXUz24vrKK1r5Zr5o5lg8Z2anpXIupkj+ccnx7l5Sc4gt5Z5Nu2MjBERFsKM7ERm2Chg8gTDbzLRC4xPj6O9q5cKi0Y5L+4sY3xaLHMdzOCbY27R6+zupfxMK7tO1LGtuJbsEdE2iyCChYiwEM6dlEZ9axeZCVF2veXeQAjBLy6bzvKJqfzwtc/ZcrSGhrYubnp8B8eqmnn05vksneD8DeC2FeMIFYKvPr2Tbz2/mxlZibz2zaV9VsK4yDD+dM0sKurb+MWb9ld131tWT/aIaIc2TlsszE3mg7vP5WeXTnNq/6ykaJ776iKevnUh8ZHh3LlhL5f8dQufmJoktXZ2U1LTwo7jdby5r4Knt5VQVme/xUFbZw+3PZ1PQ1sXj948b0DPF28ghOBHF0+hqb2LP288SltnD197Zhev7jnJd9fmcd8lUwfceIQQTEiP58bFY/nhRVMGBWmD754/ic6eXv764cDWvo6yaX8R3FHBR/T3/Gjp0zSPVjaxu7SeH100xaXJg9yUWF7ZXc78X71PTfNAh8K6mc6vSBPIrJ2awVv7T7EiL9UrEyuOCA8N4aEb5nL1I9v4xrO7yEmNpeB0I3+7cZ5dq6A1MhKiuGZBNs9uL+WSWaP441UzB01Uzs9J5hsrx/PQpiJWT8no0zIt2VtW79Sksz1cLUYSQrAiL41lE1J5Y18F979XyE2P7yAmIpRWK6vsJEYf4eEb5lq9mfX2Sr77r73sP9nAozfNZ9oo72WQ5kzOTODaBWN4ZtsJdp84w/6TDfxq/XRuHMLEe25qLNcuGM3zO0r5yrJxjDFVDruaTfsKHaidoG9Zrqrmvi/6izvLCAsRXD43y6VjXTE3i9qWTtLiI8hIiCIzIYqMRPXqrdl5X3Pe5HRmjU7iqnnOSULeICEqnCe+vID1D33K4VONPHTDXFZPse0csMcPLpzCyrx0zpucblM2uHN1Hh8dqeaHr33O3LFJAyrqqps6eGt/BSfr27h1Wa7Vz3ubkBDB+jlZXDgjk5d2llFU3UJGQhTp8ZGkJ0SSHh9Fr5TctWEvNz+xg59cPIUvnZMzIFj96f0j/Ofz0/zooimstePC8AZ3r83jzX0VHDrVyF+un8O6maOGfMw7V0/klV3l/N8HR/i/a2f3ZdOzAyybBhDeaD4zf/58mZ+f7/Hj+gspJbN/8T7rZo7k15fPoLO7l8W/3cjCnGT+dtNg65YmcCira6WupZNZXtZSAY5VNXHxg1s4Z3wK9189i3cPnubt/afYXlxLr4TJmfE89eWFZCYGbll0c0c3d23YyweHK7l+4Wh+ful0IsJCeHV3OXe/tI/rFozmt1fM8Eu2uevEGcJDBTOzkzx2zN+9U8DfPy7iP99ezt6yen7w6uc89eUFDot4vIEQYpeUcr61bTqjdgIhhHJ+mLzUHxyupK6lk2sX+i9j1DjH6OQYm1WnnmZCejw/uHAyP3vzEPN//QFSwrjUWP5n1QQunjnKYVfEQCAuMoxHb5rHn94/wl83HeNYVTNfWTaO77/yOeeMT+GX6/0nCThy6bjDN84dz/OfneB37xRwrKqZ2aOTXJbHfIEO1E4yPi2OzaZVtF/cWcbIxChW2Gj7qTl7uXlJDqca2gkJEaybOZKpIxMCSut0hpAQwT1fmMSkzHjufXkfX392F+NSY3nkhnle7R3iDxJjwvn6yvH84V3lUvp1gGnTBjpQO8n49Dj+taucgtONfHy0mv9ZNcHp6jLN2UNIiOAHF03x9zA8wiWzRpGTEsvfPi7invMnkRgTnD1oHPHlc3L559YSRiVFB2Q2DTpQO40x0ff7dwqQZs3aNZrhzIzsRB764lx/D8OrREeE8vq3lhIVFhqQ2TToQO00403Oj02F1SydkOIz3VOj0Xgfy2XEAo3hJTh5kdHJMYSHqrvttQvG+Hk0Go3mbEIHaicJDw1hbEosidHhnO9jD6lGozm70dKHC9y1ZiKA3RaaGo1G42l0oHYBT1RDaTQajato6UOj0WgCHB2oNRqNJsDRgVqj0WgCHB2oNRqNJsDRgVqj0WgCHB2oNRqNJsDRgVqj0WgCHB2oNRqNJsDxygovQohq4ISbH08Fajw4nEDkbLhGODuu82y4Rjg7rtPf1zhWSmm1z6pXAvVQEELk21qOZrhwNlwjnB3XeTZcI5wd1xnI16ilD41GowlwdKDWaDSaACcQA/Wj/h6ADzgbrhHOjus8G64Rzo7rDNhrDDiNWqPRaDQDCcSMWqPRaDRm6ECt0Wg0AU7ABGohxAVCiEIhxDEhxPf9PR5PIYR4QghRJYQ4YPZeshDifSHEUdPrCH+OcagIIUYLITYJIQ4LIQ4KIe40vT9srlMIESWE2CGE2Ge6xp+b3h8212iOECJUCLFHCPGW6fdhdZ1CiBIhxOdCiL1CiHzTewF7jQERqIUQocBDwIXAVOB6IcRU/47KYzwFXGDx3veBjVLKicBG0+/BTDfwXSnlFGAx8C3Tv99wus4O4Dwp5SxgNnCBEGIxw+sazbkTOGz2+3C8zlVSytlm3umAvcaACNTAQuCYlLJYStkJbAAu8/OYPIKU8mOgzuLty4B/mn7+J7Del2PyNFLKU1LK3aafm1Bf8CyG0XVKRbPp13DTH8kwukYDIUQ2cDHwD7O3h911WiFgrzFQAnUWUGb2e7npveFKhpTyFKggB6T7eTweQwiRA8wBPmOYXadJDtgLVAHvSymH3TWaeAD4HtBr9t5wu04JvCeE2CWEuN30XsBeY6AsbiusvKd9g0GGECIOeAW4S0rZKIS1f9bgRUrZA8wWQiQBrwkhpvt5SB5HCLEOqJJS7hJCrPTzcLzJUillhRAiHXhfCFHg7wHZI1Ay6nJgtNnv2UCFn8biCyqFECMBTK9Vfh7PkBFChKOC9HNSyldNbw+76wSQUtYDm1FzD8PtGpcClwohSlAS5HlCiGcZZtcppawwvVYBr6Hk14C9xkAJ1DuBiUKIXCFEBHAd8Iafx+RN3gC+ZPr5S8C//TiWISNU6vw4cFhK+SezTcPmOoUQaaZMGiFENLAGKGAYXSOAlPIHUspsKWUO6nv4oZTyRobRdQohYoUQ8cbPwPnAAQL4GgOmMlEIcRFKGwsFnpBS/tq/I/IMQogXgJWoFoqVwH3A68BLwBigFLhaSmk54Rg0CCGWAZ8An9Ova/4QpVMPi+sUQsxETTCFohKcl6SUvxBCpDBMrtESk/Rxj5Ry3XC6TiHEOFQWDUr+fV5K+etAvsaACdQajUajsU6gSB8ajUajsYEO1BqNRhPg6ECt0Wg0AY4O1BqNRhPg6ECt0Wg0AY4O1BqNRhPg6ECt0Wg0Ac7/B0VmIHeJ8CwbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluate the model\n",
    "train_loss, train_acc = model1.evaluate(X_train, Y_train, verbose=0)\n",
    "test_loss, test_acc = model1.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Train loss: %.3f, Validation loss: %.3f' % (train_loss, test_loss))\n",
    "# plot training history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latest=tf.train.latest_checkpoint(checkpoint_dir)\n",
    "weights_file = 'fractal_dimension_2_2/Weights-004--3.70026.hdf5' # choose the best checkpoint \n",
    "model1.load_weights(weights_file) # load it\n",
    "model1.compile(loss='mean_absolute_percentage_error', optimizer='adam', metrics=['mean_absolute_percentage_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percentage error on test set:  [0.01739401 0.0583743  0.05260417]\n"
     ]
    }
   ],
   "source": [
    "error= mean_absolute_percentage_error(Y_test, Y_pred, multioutput='raw_values')   \n",
    "print('Mean absolute percentage error on test set: ', error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leave fractal dimesnion: =2.1, 2.2 as test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2968, 36)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set=df1[(df1['fractal_dimension']<2.1) | (df1['fractal_dimension']>2.2)]\n",
    "test_set=df1[(df1['fractal_dimension']==2.1) | (df1['fractal_dimension']==2.2)]\n",
    "test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = train_set.iloc[:,25:28]\n",
    "X_train = train_set.iloc[:,:8]\n",
    "Y_test = test_set.iloc[:,25:28]\n",
    "X_test = test_set.iloc[:,:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model2 = load_model('random_split_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               1152      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 166,531\n",
      "Trainable params: 166,531\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model1 = Sequential()\n",
    "# # The Input Layer :\n",
    "# model1.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n",
    "\n",
    "# # The Hidden Layers :\n",
    "# model1.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "# model1.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "# model1.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "\n",
    "# # The Output Layer :\n",
    "# model1.add(Dense(3, kernel_initializer='normal',activation='linear'))\n",
    "# #model1.add(Dense(3, kernel_initializer='normal',activation='relu'))\n",
    "# # Compile the network :\n",
    "# model1.compile(loss='mean_absolute_percentage_error', optimizer='adam', metrics=['accuracy'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"fractal_dimension_2_1_2_2/Weights-{epoch:03d}--{val_loss:.5f}.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, verbose=1, monitor='val_loss',save_best_only=True, mode='auto')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # patient early stopping\n",
    "es = EarlyStopping(monitor='val_loss', patience=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_csv=CSVLogger('fractal_dimension_2_1_2_2_loss_logs.csv', separator=',', append=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_list=[checkpoint, es, log_csv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "136/172 [======================>.......] - ETA: 0s - loss: 3.7090 - accuracy: 0.9862\n",
      "Epoch 00001: val_loss improved from inf to 4.47613, saving model to fractal_dimension_2_1_2_2\\Weights-001--4.47613.hdf5\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 3.6646 - accuracy: 0.9858 - val_loss: 4.4761 - val_accuracy: 0.9891\n",
      "Epoch 2/500\n",
      "137/172 [======================>.......] - ETA: 0s - loss: 3.8043 - accuracy: 0.9861\n",
      "Epoch 00002: val_loss improved from 4.47613 to 3.73243, saving model to fractal_dimension_2_1_2_2\\Weights-002--3.73243.hdf5\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 3.7712 - accuracy: 0.9869 - val_loss: 3.7324 - val_accuracy: 0.9942\n",
      "Epoch 3/500\n",
      "132/172 [======================>.......] - ETA: 0s - loss: 3.5072 - accuracy: 0.9853\n",
      "Epoch 00003: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 3.6185 - accuracy: 0.9861 - val_loss: 3.9669 - val_accuracy: 0.9956\n",
      "Epoch 4/500\n",
      "138/172 [=======================>......] - ETA: 0s - loss: 3.5909 - accuracy: 0.9851\n",
      "Epoch 00004: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 3.5820 - accuracy: 0.9843 - val_loss: 4.4165 - val_accuracy: 0.9934\n",
      "Epoch 5/500\n",
      "141/172 [=======================>......] - ETA: 0s - loss: 3.8158 - accuracy: 0.9874\n",
      "Epoch 00005: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 3.8808 - accuracy: 0.9865 - val_loss: 4.4455 - val_accuracy: 0.9883\n",
      "Epoch 6/500\n",
      "137/172 [======================>.......] - ETA: 0s - loss: 3.5702 - accuracy: 0.9877\n",
      "Epoch 00006: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 3.6096 - accuracy: 0.9852 - val_loss: 5.3033 - val_accuracy: 0.9862\n",
      "Epoch 7/500\n",
      "137/172 [======================>.......] - ETA: 0s - loss: 3.6610 - accuracy: 0.9859\n",
      "Epoch 00007: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 3.5220 - accuracy: 0.9867 - val_loss: 5.0732 - val_accuracy: 0.9862\n",
      "Epoch 8/500\n",
      "138/172 [=======================>......] - ETA: 0s - loss: 3.9243 - accuracy: 0.9853\n",
      "Epoch 00008: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 3.8556 - accuracy: 0.9838 - val_loss: 5.4755 - val_accuracy: 0.9876\n",
      "Epoch 9/500\n",
      "141/172 [=======================>......] - ETA: 0s - loss: 4.0777 - accuracy: 0.9843\n",
      "Epoch 00009: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 4.0800 - accuracy: 0.9852 - val_loss: 5.1557 - val_accuracy: 0.9781\n",
      "Epoch 10/500\n",
      "128/172 [=====================>........] - ETA: 0s - loss: 3.2845 - accuracy: 0.9873\n",
      "Epoch 00010: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 3.3928 - accuracy: 0.9878 - val_loss: 5.6727 - val_accuracy: 0.9920\n",
      "Epoch 11/500\n",
      "137/172 [======================>.......] - ETA: 0s - loss: 3.9915 - accuracy: 0.9872\n",
      "Epoch 00011: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 3.9187 - accuracy: 0.9869 - val_loss: 5.2748 - val_accuracy: 0.9898\n",
      "Epoch 12/500\n",
      "141/172 [=======================>......] - ETA: 0s - loss: 3.6334 - accuracy: 0.9869\n",
      "Epoch 00012: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 3.6436 - accuracy: 0.9865 - val_loss: 6.4249 - val_accuracy: 0.9781\n",
      "Epoch 13/500\n",
      "142/172 [=======================>......] - ETA: 0s - loss: 3.5316 - accuracy: 0.9850\n",
      "Epoch 00013: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 3.5258 - accuracy: 0.9869 - val_loss: 5.2831 - val_accuracy: 0.9854\n",
      "Epoch 14/500\n",
      "136/172 [======================>.......] - ETA: 0s - loss: 3.3965 - accuracy: 0.9901\n",
      "Epoch 00014: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 3.4261 - accuracy: 0.9889 - val_loss: 4.8226 - val_accuracy: 0.9891\n",
      "Epoch 15/500\n",
      "135/172 [======================>.......] - ETA: 0s - loss: 3.8213 - accuracy: 0.9854\n",
      "Epoch 00015: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 3.8068 - accuracy: 0.9858 - val_loss: 6.2372 - val_accuracy: 0.9854\n",
      "Epoch 16/500\n",
      "141/172 [=======================>......] - ETA: 0s - loss: 3.8315 - accuracy: 0.9852\n",
      "Epoch 00016: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 3.7858 - accuracy: 0.9849 - val_loss: 5.0241 - val_accuracy: 0.9854\n",
      "Epoch 17/500\n",
      "142/172 [=======================>......] - ETA: 0s - loss: 3.9066 - accuracy: 0.9866\n",
      "Epoch 00017: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 3.8471 - accuracy: 0.9861 - val_loss: 6.4646 - val_accuracy: 0.9862\n",
      "Epoch 18/500\n",
      "135/172 [======================>.......] - ETA: 0s - loss: 3.7047 - accuracy: 0.9861\n",
      "Epoch 00018: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 3.6261 - accuracy: 0.9869 - val_loss: 5.8051 - val_accuracy: 0.9840\n",
      "Epoch 19/500\n",
      "133/172 [======================>.......] - ETA: 0s - loss: 3.4727 - accuracy: 0.9850\n",
      "Epoch 00019: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 3.4647 - accuracy: 0.9863 - val_loss: 6.2042 - val_accuracy: 0.9818\n",
      "Epoch 20/500\n",
      "142/172 [=======================>......] - ETA: 0s - loss: 3.8191 - accuracy: 0.9872\n",
      "Epoch 00020: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 3.9367 - accuracy: 0.9872 - val_loss: 5.9012 - val_accuracy: 0.9883\n",
      "Epoch 21/500\n",
      "142/172 [=======================>......] - ETA: 0s - loss: 3.6360 - accuracy: 0.9872\n",
      "Epoch 00021: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 3.6013 - accuracy: 0.9867 - val_loss: 6.1940 - val_accuracy: 0.9840\n",
      "Epoch 22/500\n",
      "137/172 [======================>.......] - ETA: 0s - loss: 3.8974 - accuracy: 0.9875\n",
      "Epoch 00022: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 3.7662 - accuracy: 0.9861 - val_loss: 6.1021 - val_accuracy: 0.9854\n",
      "Epoch 23/500\n",
      "141/172 [=======================>......] - ETA: 0s - loss: 3.5073 - accuracy: 0.9863\n",
      "Epoch 00023: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 3.4843 - accuracy: 0.9867 - val_loss: 6.8643 - val_accuracy: 0.9803\n",
      "Epoch 24/500\n",
      "136/172 [======================>.......] - ETA: 0s - loss: 3.1012 - accuracy: 0.9869\n",
      "Epoch 00024: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 3.1684 - accuracy: 0.9871 - val_loss: 6.2281 - val_accuracy: 0.9840\n",
      "Epoch 25/500\n",
      "133/172 [======================>.......] - ETA: 0s - loss: 3.9560 - accuracy: 0.9847\n",
      "Epoch 00025: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 3.8015 - accuracy: 0.9860 - val_loss: 6.6189 - val_accuracy: 0.9840\n",
      "Epoch 26/500\n",
      "137/172 [======================>.......] - ETA: 0s - loss: 3.7460 - accuracy: 0.9854\n",
      "Epoch 00026: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 3.7833 - accuracy: 0.9847 - val_loss: 6.9324 - val_accuracy: 0.9774\n",
      "Epoch 27/500\n",
      "140/172 [=======================>......] - ETA: 0s - loss: 3.8181 - accuracy: 0.9857\n",
      "Epoch 00027: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 3.8007 - accuracy: 0.9872 - val_loss: 7.0111 - val_accuracy: 0.9840\n",
      "Epoch 28/500\n",
      "168/172 [============================>.] - ETA: 0s - loss: 3.6990 - accuracy: 0.9892\n",
      "Epoch 00028: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 3.6896 - accuracy: 0.9891 - val_loss: 6.8322 - val_accuracy: 0.9840\n",
      "Epoch 29/500\n",
      "171/172 [============================>.] - ETA: 0s - loss: 3.5247 - accuracy: 0.9859\n",
      "Epoch 00029: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 3.5261 - accuracy: 0.9860 - val_loss: 6.3522 - val_accuracy: 0.9847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/500\n",
      "139/172 [=======================>......] - ETA: 0s - loss: 4.0311 - accuracy: 0.9865\n",
      "Epoch 00030: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 4.0136 - accuracy: 0.9869 - val_loss: 6.4419 - val_accuracy: 0.9847\n",
      "Epoch 31/500\n",
      "136/172 [======================>.......] - ETA: 0s - loss: 3.7512 - accuracy: 0.9869\n",
      "Epoch 00031: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 3.9044 - accuracy: 0.9867 - val_loss: 7.5232 - val_accuracy: 0.9862\n",
      "Epoch 32/500\n",
      "143/172 [=======================>......] - ETA: 0s - loss: 4.1832 - accuracy: 0.9845\n",
      "Epoch 00032: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 4.0185 - accuracy: 0.9843 - val_loss: 6.0538 - val_accuracy: 0.9876\n",
      "Epoch 33/500\n",
      "137/172 [======================>.......] - ETA: 0s - loss: 3.7423 - accuracy: 0.9859\n",
      "Epoch 00033: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 3.7892 - accuracy: 0.9854 - val_loss: 6.2273 - val_accuracy: 0.9854\n",
      "Epoch 34/500\n",
      "136/172 [======================>.......] - ETA: 0s - loss: 3.7218 - accuracy: 0.9871\n",
      "Epoch 00034: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 3.6330 - accuracy: 0.9871 - val_loss: 7.7954 - val_accuracy: 0.9869\n",
      "Epoch 35/500\n",
      "137/172 [======================>.......] - ETA: 0s - loss: 3.3488 - accuracy: 0.9865\n",
      "Epoch 00035: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 3.4244 - accuracy: 0.9854 - val_loss: 6.5943 - val_accuracy: 0.9774\n",
      "Epoch 36/500\n",
      "140/172 [=======================>......] - ETA: 0s - loss: 3.2283 - accuracy: 0.9848\n",
      "Epoch 00036: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 3.3822 - accuracy: 0.9849 - val_loss: 6.2557 - val_accuracy: 0.9840\n",
      "Epoch 37/500\n",
      "172/172 [==============================] - ETA: 0s - loss: 3.5185 - accuracy: 0.9852\n",
      "Epoch 00037: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 3.5185 - accuracy: 0.9852 - val_loss: 7.2669 - val_accuracy: 0.9825\n",
      "Epoch 38/500\n",
      "137/172 [======================>.......] - ETA: 0s - loss: 3.7463 - accuracy: 0.9861\n",
      "Epoch 00038: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 3.5539 - accuracy: 0.9867 - val_loss: 6.7986 - val_accuracy: 0.9825\n",
      "Epoch 39/500\n",
      "137/172 [======================>.......] - ETA: 0s - loss: 3.2766 - accuracy: 0.9916\n",
      "Epoch 00039: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 3.2991 - accuracy: 0.9887 - val_loss: 8.9607 - val_accuracy: 0.9738\n",
      "Epoch 40/500\n",
      "140/172 [=======================>......] - ETA: 0s - loss: 3.5703 - accuracy: 0.9879\n",
      "Epoch 00040: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 3.5223 - accuracy: 0.9885 - val_loss: 7.2756 - val_accuracy: 0.9854\n",
      "Epoch 41/500\n",
      "141/172 [=======================>......] - ETA: 0s - loss: 3.9661 - accuracy: 0.9849\n",
      "Epoch 00041: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 3.9060 - accuracy: 0.9849 - val_loss: 9.3601 - val_accuracy: 0.9847\n",
      "Epoch 42/500\n",
      "135/172 [======================>.......] - ETA: 0s - loss: 3.4756 - accuracy: 0.9843\n",
      "Epoch 00042: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 3.5251 - accuracy: 0.9841 - val_loss: 6.8227 - val_accuracy: 0.9789\n",
      "Epoch 43/500\n",
      "140/172 [=======================>......] - ETA: 0s - loss: 3.6405 - accuracy: 0.9864\n",
      "Epoch 00043: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 3.6926 - accuracy: 0.9861 - val_loss: 9.9521 - val_accuracy: 0.9774\n",
      "Epoch 44/500\n",
      "140/172 [=======================>......] - ETA: 0s - loss: 3.8338 - accuracy: 0.9855\n",
      "Epoch 00044: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 4.0009 - accuracy: 0.9865 - val_loss: 8.3312 - val_accuracy: 0.9810\n",
      "Epoch 45/500\n",
      "139/172 [=======================>......] - ETA: 0s - loss: 3.9422 - accuracy: 0.9849\n",
      "Epoch 00045: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 3.9416 - accuracy: 0.9849 - val_loss: 7.3094 - val_accuracy: 0.9759\n",
      "Epoch 46/500\n",
      "139/172 [=======================>......] - ETA: 0s - loss: 3.4817 - accuracy: 0.9867\n",
      "Epoch 00046: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 3.3898 - accuracy: 0.9865 - val_loss: 7.0848 - val_accuracy: 0.9876\n",
      "Epoch 47/500\n",
      "141/172 [=======================>......] - ETA: 0s - loss: 3.1545 - accuracy: 0.9887\n",
      "Epoch 00047: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 3.3481 - accuracy: 0.9885 - val_loss: 8.1158 - val_accuracy: 0.9789\n",
      "Epoch 48/500\n",
      "140/172 [=======================>......] - ETA: 0s - loss: 3.9212 - accuracy: 0.9848\n",
      "Epoch 00048: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 3.8975 - accuracy: 0.9861 - val_loss: 6.5055 - val_accuracy: 0.9796\n",
      "Epoch 49/500\n",
      "141/172 [=======================>......] - ETA: 0s - loss: 3.4027 - accuracy: 0.9858\n",
      "Epoch 00049: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 3.4325 - accuracy: 0.9865 - val_loss: 8.8827 - val_accuracy: 0.9752\n",
      "Epoch 50/500\n",
      "139/172 [=======================>......] - ETA: 0s - loss: 3.5215 - accuracy: 0.9856\n",
      "Epoch 00050: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 3.5722 - accuracy: 0.9860 - val_loss: 7.2369 - val_accuracy: 0.9767\n",
      "Epoch 51/500\n",
      "140/172 [=======================>......] - ETA: 0s - loss: 3.8768 - accuracy: 0.9866\n",
      "Epoch 00051: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 3.8653 - accuracy: 0.9871 - val_loss: 7.0629 - val_accuracy: 0.9825\n",
      "Epoch 52/500\n",
      "139/172 [=======================>......] - ETA: 0s - loss: 3.7293 - accuracy: 0.9852\n",
      "Epoch 00052: val_loss did not improve from 3.73243\n",
      "172/172 [==============================] - 0s 1ms/step - loss: 3.6876 - accuracy: 0.9856 - val_loss: 6.7792 - val_accuracy: 0.9840\n",
      "Epoch 00052: early stopping\n"
     ]
    }
   ],
   "source": [
    "history= model2.fit(X_train, Y_train, epochs=500, batch_size=32, validation_split = 0.2, callbacks=callback_list)\n",
    "#history= NN_model.fit(X_train, Y_train, epochs=7, batch_size=32, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save('fractal_dimension_2_1_2_2_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 4.316, Validation loss: 6.648\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluate the model\n",
    "train_loss, train_acc = model2.evaluate(X_train, Y_train, verbose=0)\n",
    "test_loss, test_acc = model2.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Train loss: %.3f, Validation loss: %.3f' % (train_loss, test_loss))\n",
    "# plot training history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latest=tf.train.latest_checkpoint(checkpoint_dir)\n",
    "weights_file = 'fractal_dimension_2_1_2_2/Weights-002--3.73243.hdf5' # choose the best checkpoint \n",
    "model2.load_weights(weights_file) # load it\n",
    "model2.compile(loss='mean_absolute_percentage_error', optimizer='adam', metrics=['mean_absolute_percentage_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percentage error on test set:  [0.02312221 0.0303103  0.02899866]\n"
     ]
    }
   ],
   "source": [
    "error= mean_absolute_percentage_error(Y_test, Y_pred, multioutput='raw_values')   \n",
    "print('Mean absolute percentage error on test set: ', error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leave fraction of coating: ==50 as test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1264, 36)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set=df1[(df1['fraction_of_coating']<=40) | (df1['fraction_of_coating']>50)]\n",
    "test_set=df1[df1['fraction_of_coating']==50]\n",
    "test_set.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = train_set.iloc[:,25:28]\n",
    "X_train = train_set.iloc[:,:8]\n",
    "Y_test = test_set.iloc[:,25:28]\n",
    "X_test = test_set.iloc[:,:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model3 = load_model('random_split_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               1152      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 166,531\n",
      "Trainable params: 166,531\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model1 = Sequential()\n",
    "# # The Input Layer :\n",
    "# model1.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n",
    "\n",
    "# # The Hidden Layers :\n",
    "# model1.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "# model1.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "# model1.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "\n",
    "# # The Output Layer :\n",
    "# model1.add(Dense(3, kernel_initializer='normal',activation='linear'))\n",
    "# #model1.add(Dense(3, kernel_initializer='normal',activation='relu'))\n",
    "# # Compile the network :\n",
    "# model1.compile(loss='mean_absolute_percentage_error', optimizer='adam', metrics=['accuracy'])\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"fraction_of_coating_50/Weights-{epoch:03d}--{val_loss:.5f}.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, verbose=1, monitor='val_loss',save_best_only=True, mode='auto')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # patient early stopping\n",
    "es = EarlyStopping(monitor='val_loss', patience=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_csv=CSVLogger('fraction_of_coating_50_loss_logs.csv', separator=',', append=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_list=[checkpoint, es, log_csv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "179/215 [=======================>......] - ETA: 0s - loss: 3.8400 - accuracy: 0.9906\n",
      "Epoch 00001: val_loss improved from inf to 5.83718, saving model to fraction_of_coating_50\\Weights-001--5.83718.hdf5\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 4.0541 - accuracy: 0.9895 - val_loss: 5.8372 - val_accuracy: 0.9854\n",
      "Epoch 2/500\n",
      "192/215 [=========================>....] - ETA: 0s - loss: 3.8009 - accuracy: 0.9906\n",
      "Epoch 00002: val_loss improved from 5.83718 to 4.11986, saving model to fraction_of_coating_50\\Weights-002--4.11986.hdf5\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 3.7349 - accuracy: 0.9905 - val_loss: 4.1199 - val_accuracy: 0.9848\n",
      "Epoch 3/500\n",
      "183/215 [========================>.....] - ETA: 0s - loss: 3.8126 - accuracy: 0.9903\n",
      "Epoch 00003: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 3.7908 - accuracy: 0.9902 - val_loss: 5.5573 - val_accuracy: 0.9907\n",
      "Epoch 4/500\n",
      "188/215 [=========================>....] - ETA: 0s - loss: 4.0442 - accuracy: 0.9919\n",
      "Epoch 00004: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 3.9027 - accuracy: 0.9918 - val_loss: 5.2322 - val_accuracy: 0.9877\n",
      "Epoch 5/500\n",
      "190/215 [=========================>....] - ETA: 0s - loss: 4.1416 - accuracy: 0.9903\n",
      "Epoch 00005: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 4.0878 - accuracy: 0.9901 - val_loss: 6.0824 - val_accuracy: 0.9778\n",
      "Epoch 6/500\n",
      "190/215 [=========================>....] - ETA: 0s - loss: 3.5013 - accuracy: 0.9903\n",
      "Epoch 00006: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 3.4842 - accuracy: 0.9908 - val_loss: 5.2060 - val_accuracy: 0.9860\n",
      "Epoch 7/500\n",
      "192/215 [=========================>....] - ETA: 0s - loss: 3.9566 - accuracy: 0.9907\n",
      "Epoch 00007: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 3.9091 - accuracy: 0.9902 - val_loss: 5.3835 - val_accuracy: 0.9854\n",
      "Epoch 8/500\n",
      "172/215 [=======================>......] - ETA: 0s - loss: 3.6468 - accuracy: 0.9898\n",
      "Epoch 00008: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 3.7451 - accuracy: 0.9908 - val_loss: 4.3867 - val_accuracy: 0.9866\n",
      "Epoch 9/500\n",
      "191/215 [=========================>....] - ETA: 0s - loss: 3.7131 - accuracy: 0.9910\n",
      "Epoch 00009: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 3.6507 - accuracy: 0.9912 - val_loss: 6.9470 - val_accuracy: 0.9796\n",
      "Epoch 10/500\n",
      "188/215 [=========================>....] - ETA: 0s - loss: 3.4220 - accuracy: 0.9924\n",
      "Epoch 00010: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 3.3837 - accuracy: 0.9921 - val_loss: 9.1582 - val_accuracy: 0.9755\n",
      "Epoch 11/500\n",
      "190/215 [=========================>....] - ETA: 0s - loss: 4.0813 - accuracy: 0.9913\n",
      "Epoch 00011: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 4.0225 - accuracy: 0.9914 - val_loss: 7.6353 - val_accuracy: 0.9790\n",
      "Epoch 12/500\n",
      "190/215 [=========================>....] - ETA: 0s - loss: 3.7287 - accuracy: 0.9900\n",
      "Epoch 00012: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 3.7134 - accuracy: 0.9899 - val_loss: 6.0878 - val_accuracy: 0.9766\n",
      "Epoch 13/500\n",
      "187/215 [=========================>....] - ETA: 0s - loss: 3.5839 - accuracy: 0.9908\n",
      "Epoch 00013: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 3.6690 - accuracy: 0.9909 - val_loss: 5.0949 - val_accuracy: 0.9819\n",
      "Epoch 14/500\n",
      "185/215 [========================>.....] - ETA: 0s - loss: 3.5906 - accuracy: 0.9899\n",
      "Epoch 00014: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 3.4870 - accuracy: 0.9901 - val_loss: 5.2387 - val_accuracy: 0.9831\n",
      "Epoch 15/500\n",
      "184/215 [========================>.....] - ETA: 0s - loss: 3.6613 - accuracy: 0.9918\n",
      "Epoch 00015: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 3.6503 - accuracy: 0.9924 - val_loss: 6.0427 - val_accuracy: 0.9714\n",
      "Epoch 16/500\n",
      "186/215 [========================>.....] - ETA: 0s - loss: 3.5701 - accuracy: 0.9909\n",
      "Epoch 00016: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 3.6773 - accuracy: 0.9905 - val_loss: 7.5095 - val_accuracy: 0.9737\n",
      "Epoch 17/500\n",
      "189/215 [=========================>....] - ETA: 0s - loss: 3.8732 - accuracy: 0.9911\n",
      "Epoch 00017: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 3.8338 - accuracy: 0.9912 - val_loss: 6.6592 - val_accuracy: 0.9755\n",
      "Epoch 18/500\n",
      "180/215 [========================>.....] - ETA: 0s - loss: 3.3577 - accuracy: 0.9913\n",
      "Epoch 00018: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 3.4611 - accuracy: 0.9911 - val_loss: 7.9021 - val_accuracy: 0.9807\n",
      "Epoch 19/500\n",
      "184/215 [========================>.....] - ETA: 0s - loss: 3.7895 - accuracy: 0.9905\n",
      "Epoch 00019: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 3.9564 - accuracy: 0.9905 - val_loss: 6.9304 - val_accuracy: 0.9831\n",
      "Epoch 20/500\n",
      "196/215 [==========================>...] - ETA: 0s - loss: 3.7216 - accuracy: 0.9919\n",
      "Epoch 00020: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 3.6977 - accuracy: 0.9912 - val_loss: 6.2169 - val_accuracy: 0.9825\n",
      "Epoch 21/500\n",
      "183/215 [========================>.....] - ETA: 0s - loss: 3.7238 - accuracy: 0.9898\n",
      "Epoch 00021: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 3.7207 - accuracy: 0.9901 - val_loss: 6.5601 - val_accuracy: 0.9749\n",
      "Epoch 22/500\n",
      "191/215 [=========================>....] - ETA: 0s - loss: 3.6106 - accuracy: 0.9877\n",
      "Epoch 00022: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 3.5889 - accuracy: 0.9885 - val_loss: 8.4861 - val_accuracy: 0.9778\n",
      "Epoch 23/500\n",
      "189/215 [=========================>....] - ETA: 0s - loss: 3.4827 - accuracy: 0.9921\n",
      "Epoch 00023: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 3.5719 - accuracy: 0.9924 - val_loss: 7.6578 - val_accuracy: 0.9737\n",
      "Epoch 24/500\n",
      "191/215 [=========================>....] - ETA: 0s - loss: 3.8517 - accuracy: 0.9902\n",
      "Epoch 00024: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 3.7566 - accuracy: 0.9908 - val_loss: 8.4929 - val_accuracy: 0.9796\n",
      "Epoch 25/500\n",
      "174/215 [=======================>......] - ETA: 0s - loss: 3.3781 - accuracy: 0.9921\n",
      "Epoch 00025: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 3.5150 - accuracy: 0.9918 - val_loss: 9.6922 - val_accuracy: 0.9749\n",
      "Epoch 26/500\n",
      "192/215 [=========================>....] - ETA: 0s - loss: 6.1560 - accuracy: 0.9901\n",
      "Epoch 00026: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 5.8357 - accuracy: 0.9899 - val_loss: 10.1811 - val_accuracy: 0.9766\n",
      "Epoch 27/500\n",
      "194/215 [==========================>...] - ETA: 0s - loss: 4.1158 - accuracy: 0.9887\n",
      "Epoch 00027: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 4.1243 - accuracy: 0.9885 - val_loss: 10.7654 - val_accuracy: 0.9691\n",
      "Epoch 28/500\n",
      "212/215 [============================>.] - ETA: 0s - loss: 4.1623 - accuracy: 0.9885\n",
      "Epoch 00028: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 4.1608 - accuracy: 0.9883 - val_loss: 6.8985 - val_accuracy: 0.9755\n",
      "Epoch 29/500\n",
      "185/215 [========================>.....] - ETA: 0s - loss: 4.0378 - accuracy: 0.9905\n",
      "Epoch 00029: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 3.9738 - accuracy: 0.9905 - val_loss: 7.5429 - val_accuracy: 0.9714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/500\n",
      "188/215 [=========================>....] - ETA: 0s - loss: 4.0071 - accuracy: 0.9905\n",
      "Epoch 00030: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 3.9044 - accuracy: 0.9912 - val_loss: 7.5786 - val_accuracy: 0.9720\n",
      "Epoch 31/500\n",
      "188/215 [=========================>....] - ETA: 0s - loss: 3.8333 - accuracy: 0.9907\n",
      "Epoch 00031: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 4.0111 - accuracy: 0.9896 - val_loss: 8.2117 - val_accuracy: 0.9685\n",
      "Epoch 32/500\n",
      "183/215 [========================>.....] - ETA: 0s - loss: 3.9668 - accuracy: 0.9899\n",
      "Epoch 00032: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 3.8808 - accuracy: 0.9904 - val_loss: 6.6568 - val_accuracy: 0.9609\n",
      "Epoch 33/500\n",
      "192/215 [=========================>....] - ETA: 0s - loss: 3.3304 - accuracy: 0.9928\n",
      "Epoch 00033: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 3.2990 - accuracy: 0.9930 - val_loss: 6.2608 - val_accuracy: 0.9726\n",
      "Epoch 34/500\n",
      "190/215 [=========================>....] - ETA: 0s - loss: 3.3931 - accuracy: 0.9910\n",
      "Epoch 00034: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 3.4334 - accuracy: 0.9915 - val_loss: 7.2519 - val_accuracy: 0.9796\n",
      "Epoch 35/500\n",
      "189/215 [=========================>....] - ETA: 0s - loss: 4.1508 - accuracy: 0.9894\n",
      "Epoch 00035: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 4.0752 - accuracy: 0.9895 - val_loss: 8.5717 - val_accuracy: 0.9766\n",
      "Epoch 36/500\n",
      "190/215 [=========================>....] - ETA: 0s - loss: 3.4338 - accuracy: 0.9924\n",
      "Epoch 00036: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 3.4044 - accuracy: 0.9923 - val_loss: 7.7517 - val_accuracy: 0.9766\n",
      "Epoch 37/500\n",
      "184/215 [========================>.....] - ETA: 0s - loss: 3.7332 - accuracy: 0.9888\n",
      "Epoch 00037: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 3.6766 - accuracy: 0.9895 - val_loss: 8.4288 - val_accuracy: 0.9772\n",
      "Epoch 38/500\n",
      "195/215 [==========================>...] - ETA: 0s - loss: 4.0608 - accuracy: 0.9881\n",
      "Epoch 00038: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 3.9954 - accuracy: 0.9882 - val_loss: 8.2972 - val_accuracy: 0.9778\n",
      "Epoch 39/500\n",
      "192/215 [=========================>....] - ETA: 0s - loss: 3.9129 - accuracy: 0.9896\n",
      "Epoch 00039: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 4.0964 - accuracy: 0.9891 - val_loss: 10.0883 - val_accuracy: 0.9778\n",
      "Epoch 40/500\n",
      "176/215 [=======================>......] - ETA: 0s - loss: 3.4833 - accuracy: 0.9909\n",
      "Epoch 00040: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 3.5290 - accuracy: 0.9899 - val_loss: 8.0208 - val_accuracy: 0.9766\n",
      "Epoch 41/500\n",
      "185/215 [========================>.....] - ETA: 0s - loss: 3.7245 - accuracy: 0.9914\n",
      "Epoch 00041: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 3.7496 - accuracy: 0.9914 - val_loss: 11.4936 - val_accuracy: 0.9673\n",
      "Epoch 42/500\n",
      "187/215 [=========================>....] - ETA: 0s - loss: 4.4812 - accuracy: 0.9903\n",
      "Epoch 00042: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 4.3567 - accuracy: 0.9907 - val_loss: 8.8568 - val_accuracy: 0.9743\n",
      "Epoch 43/500\n",
      "191/215 [=========================>....] - ETA: 0s - loss: 3.8750 - accuracy: 0.9900\n",
      "Epoch 00043: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 3.7804 - accuracy: 0.9904 - val_loss: 8.0208 - val_accuracy: 0.9714\n",
      "Epoch 44/500\n",
      "193/215 [=========================>....] - ETA: 0s - loss: 3.7687 - accuracy: 0.9888\n",
      "Epoch 00044: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 3.7750 - accuracy: 0.9898 - val_loss: 8.0907 - val_accuracy: 0.9802\n",
      "Epoch 45/500\n",
      "186/215 [========================>.....] - ETA: 0s - loss: 3.5125 - accuracy: 0.9921\n",
      "Epoch 00045: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 3.4229 - accuracy: 0.9927 - val_loss: 8.7656 - val_accuracy: 0.9778\n",
      "Epoch 46/500\n",
      "201/215 [===========================>..] - ETA: 0s - loss: 3.3315 - accuracy: 0.9918\n",
      "Epoch 00046: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 3.3859 - accuracy: 0.9915 - val_loss: 7.9401 - val_accuracy: 0.9708\n",
      "Epoch 47/500\n",
      "178/215 [=======================>......] - ETA: 0s - loss: 3.4938 - accuracy: 0.9903\n",
      "Epoch 00047: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 3.4899 - accuracy: 0.9901 - val_loss: 6.5631 - val_accuracy: 0.9778\n",
      "Epoch 48/500\n",
      "185/215 [========================>.....] - ETA: 0s - loss: 3.9377 - accuracy: 0.9902\n",
      "Epoch 00048: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 3.7850 - accuracy: 0.9909 - val_loss: 8.7856 - val_accuracy: 0.9644\n",
      "Epoch 49/500\n",
      "188/215 [=========================>....] - ETA: 0s - loss: 3.8292 - accuracy: 0.9905\n",
      "Epoch 00049: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 3.7234 - accuracy: 0.9908 - val_loss: 11.6910 - val_accuracy: 0.9667\n",
      "Epoch 50/500\n",
      "189/215 [=========================>....] - ETA: 0s - loss: 6.3061 - accuracy: 0.9871\n",
      "Epoch 00050: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 6.0321 - accuracy: 0.9872 - val_loss: 8.7264 - val_accuracy: 0.9749\n",
      "Epoch 51/500\n",
      "191/215 [=========================>....] - ETA: 0s - loss: 4.1336 - accuracy: 0.9874\n",
      "Epoch 00051: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 4.1200 - accuracy: 0.9876 - val_loss: 10.1889 - val_accuracy: 0.9673\n",
      "Epoch 52/500\n",
      "185/215 [========================>.....] - ETA: 0s - loss: 3.5015 - accuracy: 0.9900\n",
      "Epoch 00052: val_loss did not improve from 4.11986\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 3.5277 - accuracy: 0.9899 - val_loss: 7.8978 - val_accuracy: 0.9813\n",
      "Epoch 00052: early stopping\n"
     ]
    }
   ],
   "source": [
    "history= model3.fit(X_train, Y_train, epochs=500, batch_size=32, validation_split = 0.2, callbacks=callback_list)\n",
    "#history= NN_model.fit(X_train, Y_train, epochs=7, batch_size=32, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.save('fraction_of_coating_50_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 5.682, Validation loss: 6.080\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluate the model\n",
    "train_loss, train_acc = model3.evaluate(X_train, Y_train, verbose=0)\n",
    "test_loss, test_acc = model3.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Train loss: %.3f, Validation loss: %.3f' % (train_loss, test_loss))\n",
    "# plot training history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latest=tf.train.latest_checkpoint(checkpoint_dir)\n",
    "weights_file = 'fraction_of_coating_50/Weights-002--4.11986.hdf5' # choose the best checkpoint \n",
    "model3.load_weights(weights_file) # load it\n",
    "model3.compile(loss='mean_absolute_percentage_error', optimizer='adam', metrics=['mean_absolute_percentage_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percentage error on test set:  [0.01518299 0.05194054 0.04350393]\n"
     ]
    }
   ],
   "source": [
    "error= mean_absolute_percentage_error(Y_test, Y_pred, multioutput='raw_values')   \n",
    "print('Mean absolute percentage error on test set: ', error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leave fraction of coating: ==40, 50 as test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1580, 36)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set=df1[(df1['fraction_of_coating']<40) | (df1['fraction_of_coating']>50)]\n",
    "test_set=df1[(df1['fraction_of_coating']==40) | (df1['fraction_of_coating']==50)]\n",
    "test_set.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = train_set.iloc[:,25:28]\n",
    "X_train = train_set.iloc[:,:8]\n",
    "Y_test = test_set.iloc[:,25:28]\n",
    "X_test = test_set.iloc[:,:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model4 = load_model('random_split_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               1152      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 166,531\n",
      "Trainable params: 166,531\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model1 = Sequential()\n",
    "# # The Input Layer :\n",
    "# model1.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n",
    "\n",
    "# # The Hidden Layers :\n",
    "# model1.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "# model1.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "# model1.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "\n",
    "# # The Output Layer :\n",
    "# model1.add(Dense(3, kernel_initializer='normal',activation='linear'))\n",
    "# #model1.add(Dense(3, kernel_initializer='normal',activation='relu'))\n",
    "# # Compile the network :\n",
    "# model1.compile(loss='mean_absolute_percentage_error', optimizer='adam', metrics=['accuracy'])\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"fraction_of_coating_40_50/Weights-{epoch:03d}--{val_loss:.5f}.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, verbose=1, monitor='val_loss',save_best_only=True, mode='auto')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # patient early stopping\n",
    "es = EarlyStopping(monitor='val_loss', patience=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_csv=CSVLogger('fraction_of_coating_40_50_loss_logs.csv', separator=',', append=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_list=[checkpoint, es, log_csv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "177/207 [========================>.....] - ETA: 0s - loss: 4.2357 - accuracy: 0.9883\n",
      "Epoch 00001: val_loss improved from inf to 3.54632, saving model to fraction_of_coating_40_50\\Weights-001--3.54632.hdf5\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 4.0831 - accuracy: 0.9886 - val_loss: 3.5463 - val_accuracy: 0.9964\n",
      "Epoch 2/500\n",
      "186/207 [=========================>....] - ETA: 0s - loss: 3.9751 - accuracy: 0.9926\n",
      "Epoch 00002: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 3.8837 - accuracy: 0.9927 - val_loss: 5.0553 - val_accuracy: 0.9939\n",
      "Epoch 3/500\n",
      "181/207 [=========================>....] - ETA: 0s - loss: 4.0201 - accuracy: 0.9917\n",
      "Epoch 00003: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 4.0718 - accuracy: 0.9921 - val_loss: 4.9765 - val_accuracy: 0.9952\n",
      "Epoch 4/500\n",
      "187/207 [==========================>...] - ETA: 0s - loss: 3.5804 - accuracy: 0.9913\n",
      "Epoch 00004: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 3.5960 - accuracy: 0.9906 - val_loss: 3.8763 - val_accuracy: 0.9903\n",
      "Epoch 5/500\n",
      "186/207 [=========================>....] - ETA: 0s - loss: 4.1948 - accuracy: 0.9903\n",
      "Epoch 00005: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 4.1508 - accuracy: 0.9898 - val_loss: 4.1997 - val_accuracy: 0.9879\n",
      "Epoch 6/500\n",
      "190/207 [==========================>...] - ETA: 0s - loss: 4.0088 - accuracy: 0.9891\n",
      "Epoch 00006: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 3.8906 - accuracy: 0.9897 - val_loss: 6.7264 - val_accuracy: 0.9818\n",
      "Epoch 7/500\n",
      "189/207 [==========================>...] - ETA: 0s - loss: 4.0449 - accuracy: 0.9904\n",
      "Epoch 00007: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 4.0052 - accuracy: 0.9900 - val_loss: 4.7057 - val_accuracy: 0.9921\n",
      "Epoch 8/500\n",
      "188/207 [==========================>...] - ETA: 0s - loss: 3.8744 - accuracy: 0.9910\n",
      "Epoch 00008: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 3.8071 - accuracy: 0.9909 - val_loss: 5.3655 - val_accuracy: 0.9909\n",
      "Epoch 9/500\n",
      "192/207 [==========================>...] - ETA: 0s - loss: 3.8330 - accuracy: 0.9901\n",
      "Epoch 00009: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 3.8778 - accuracy: 0.9905 - val_loss: 6.8726 - val_accuracy: 0.9842\n",
      "Epoch 10/500\n",
      "188/207 [==========================>...] - ETA: 0s - loss: 4.1136 - accuracy: 0.9894\n",
      "Epoch 00010: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 4.1183 - accuracy: 0.9900 - val_loss: 5.2013 - val_accuracy: 0.9867\n",
      "Epoch 11/500\n",
      "182/207 [=========================>....] - ETA: 0s - loss: 3.5702 - accuracy: 0.9911\n",
      "Epoch 00011: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 3.5424 - accuracy: 0.9909 - val_loss: 5.2779 - val_accuracy: 0.9824\n",
      "Epoch 12/500\n",
      "187/207 [==========================>...] - ETA: 0s - loss: 3.6993 - accuracy: 0.9911\n",
      "Epoch 00012: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 3.6592 - accuracy: 0.9912 - val_loss: 7.6369 - val_accuracy: 0.9812\n",
      "Epoch 13/500\n",
      "188/207 [==========================>...] - ETA: 0s - loss: 3.8384 - accuracy: 0.9897\n",
      "Epoch 00013: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 3.9288 - accuracy: 0.9895 - val_loss: 6.2810 - val_accuracy: 0.9836\n",
      "Epoch 14/500\n",
      "190/207 [==========================>...] - ETA: 0s - loss: 4.2147 - accuracy: 0.9908\n",
      "Epoch 00014: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 4.2437 - accuracy: 0.9908 - val_loss: 5.4715 - val_accuracy: 0.9776\n",
      "Epoch 15/500\n",
      "186/207 [=========================>....] - ETA: 0s - loss: 3.2530 - accuracy: 0.9913\n",
      "Epoch 00015: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 3.2744 - accuracy: 0.9914 - val_loss: 5.5149 - val_accuracy: 0.9782\n",
      "Epoch 16/500\n",
      "182/207 [=========================>....] - ETA: 0s - loss: 3.5537 - accuracy: 0.9904\n",
      "Epoch 00016: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 3.6111 - accuracy: 0.9903 - val_loss: 5.4375 - val_accuracy: 0.9842\n",
      "Epoch 17/500\n",
      "188/207 [==========================>...] - ETA: 0s - loss: 3.7245 - accuracy: 0.9932\n",
      "Epoch 00017: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 3.6510 - accuracy: 0.9924 - val_loss: 5.9394 - val_accuracy: 0.9794\n",
      "Epoch 18/500\n",
      "187/207 [==========================>...] - ETA: 0s - loss: 3.5865 - accuracy: 0.9898\n",
      "Epoch 00018: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 3.6039 - accuracy: 0.9905 - val_loss: 6.7605 - val_accuracy: 0.9867\n",
      "Epoch 19/500\n",
      "187/207 [==========================>...] - ETA: 0s - loss: 4.4025 - accuracy: 0.9893\n",
      "Epoch 00019: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 4.3396 - accuracy: 0.9888 - val_loss: 7.3283 - val_accuracy: 0.9794\n",
      "Epoch 20/500\n",
      "185/207 [=========================>....] - ETA: 0s - loss: 3.7791 - accuracy: 0.9907\n",
      "Epoch 00020: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 3.7865 - accuracy: 0.9901 - val_loss: 6.2637 - val_accuracy: 0.9764\n",
      "Epoch 21/500\n",
      "192/207 [==========================>...] - ETA: 0s - loss: 3.6657 - accuracy: 0.9899\n",
      "Epoch 00021: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 3.6465 - accuracy: 0.9903 - val_loss: 5.9185 - val_accuracy: 0.9806\n",
      "Epoch 22/500\n",
      "189/207 [==========================>...] - ETA: 0s - loss: 3.7188 - accuracy: 0.9911\n",
      "Epoch 00022: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 3.7689 - accuracy: 0.9901 - val_loss: 5.8301 - val_accuracy: 0.9836\n",
      "Epoch 23/500\n",
      "187/207 [==========================>...] - ETA: 0s - loss: 3.2749 - accuracy: 0.9896\n",
      "Epoch 00023: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 3.2756 - accuracy: 0.9900 - val_loss: 6.7766 - val_accuracy: 0.9812\n",
      "Epoch 24/500\n",
      "184/207 [=========================>....] - ETA: 0s - loss: 3.8338 - accuracy: 0.9900\n",
      "Epoch 00024: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 3.7595 - accuracy: 0.9905 - val_loss: 7.6574 - val_accuracy: 0.9806\n",
      "Epoch 25/500\n",
      "189/207 [==========================>...] - ETA: 0s - loss: 3.4291 - accuracy: 0.9921\n",
      "Epoch 00025: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 3.3834 - accuracy: 0.9923 - val_loss: 6.6563 - val_accuracy: 0.9794\n",
      "Epoch 26/500\n",
      "170/207 [=======================>......] - ETA: 0s - loss: 3.8867 - accuracy: 0.9923\n",
      "Epoch 00026: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 3.7249 - accuracy: 0.9926 - val_loss: 7.4938 - val_accuracy: 0.9818\n",
      "Epoch 27/500\n",
      "184/207 [=========================>....] - ETA: 0s - loss: 3.9327 - accuracy: 0.9918\n",
      "Epoch 00027: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 3.9392 - accuracy: 0.9918 - val_loss: 9.2342 - val_accuracy: 0.9739\n",
      "Epoch 28/500\n",
      "182/207 [=========================>....] - ETA: 0s - loss: 3.9884 - accuracy: 0.9909\n",
      "Epoch 00028: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 4.0457 - accuracy: 0.9906 - val_loss: 6.7480 - val_accuracy: 0.9788\n",
      "Epoch 29/500\n",
      "182/207 [=========================>....] - ETA: 0s - loss: 3.8974 - accuracy: 0.9911\n",
      "Epoch 00029: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 3.8275 - accuracy: 0.9912 - val_loss: 7.2628 - val_accuracy: 0.9739\n",
      "Epoch 30/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179/207 [========================>.....] - ETA: 0s - loss: 3.6144 - accuracy: 0.9918\n",
      "Epoch 00030: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 3.5376 - accuracy: 0.9912 - val_loss: 7.9835 - val_accuracy: 0.9764\n",
      "Epoch 31/500\n",
      "178/207 [========================>.....] - ETA: 0s - loss: 3.5822 - accuracy: 0.9898\n",
      "Epoch 00031: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 3.6535 - accuracy: 0.9903 - val_loss: 6.8873 - val_accuracy: 0.9800\n",
      "Epoch 32/500\n",
      "189/207 [==========================>...] - ETA: 0s - loss: 3.2726 - accuracy: 0.9899\n",
      "Epoch 00032: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 3.3051 - accuracy: 0.9901 - val_loss: 6.6636 - val_accuracy: 0.9800\n",
      "Epoch 33/500\n",
      "183/207 [=========================>....] - ETA: 0s - loss: 4.0620 - accuracy: 0.9908\n",
      "Epoch 00033: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 3.9631 - accuracy: 0.9906 - val_loss: 6.7437 - val_accuracy: 0.9745\n",
      "Epoch 34/500\n",
      "177/207 [========================>.....] - ETA: 0s - loss: 3.6500 - accuracy: 0.9908\n",
      "Epoch 00034: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 3.6826 - accuracy: 0.9908 - val_loss: 8.4253 - val_accuracy: 0.9770\n",
      "Epoch 35/500\n",
      "182/207 [=========================>....] - ETA: 0s - loss: 3.7616 - accuracy: 0.9895\n",
      "Epoch 00035: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 3.6997 - accuracy: 0.9898 - val_loss: 6.5623 - val_accuracy: 0.9836\n",
      "Epoch 36/500\n",
      "194/207 [===========================>..] - ETA: 0s - loss: 3.4708 - accuracy: 0.9929\n",
      "Epoch 00036: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 3.4569 - accuracy: 0.9926 - val_loss: 7.3816 - val_accuracy: 0.9752\n",
      "Epoch 37/500\n",
      "188/207 [==========================>...] - ETA: 0s - loss: 3.6270 - accuracy: 0.9919\n",
      "Epoch 00037: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 3.6774 - accuracy: 0.9918 - val_loss: 8.7610 - val_accuracy: 0.9855\n",
      "Epoch 38/500\n",
      "187/207 [==========================>...] - ETA: 0s - loss: 3.4814 - accuracy: 0.9938\n",
      "Epoch 00038: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 3.4673 - accuracy: 0.9924 - val_loss: 7.2722 - val_accuracy: 0.9764\n",
      "Epoch 39/500\n",
      "170/207 [=======================>......] - ETA: 0s - loss: 3.3595 - accuracy: 0.9932\n",
      "Epoch 00039: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 3.3684 - accuracy: 0.9926 - val_loss: 7.1226 - val_accuracy: 0.9788\n",
      "Epoch 40/500\n",
      "179/207 [========================>.....] - ETA: 0s - loss: 3.9792 - accuracy: 0.9885\n",
      "Epoch 00040: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 3.8449 - accuracy: 0.9894 - val_loss: 6.9498 - val_accuracy: 0.9806\n",
      "Epoch 41/500\n",
      "174/207 [========================>.....] - ETA: 0s - loss: 3.1969 - accuracy: 0.9919\n",
      "Epoch 00041: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 3.2218 - accuracy: 0.9920 - val_loss: 7.6494 - val_accuracy: 0.9818\n",
      "Epoch 42/500\n",
      "189/207 [==========================>...] - ETA: 0s - loss: 3.5133 - accuracy: 0.9921\n",
      "Epoch 00042: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 3.5016 - accuracy: 0.9921 - val_loss: 10.1929 - val_accuracy: 0.9776\n",
      "Epoch 43/500\n",
      "180/207 [=========================>....] - ETA: 0s - loss: 3.6680 - accuracy: 0.9927\n",
      "Epoch 00043: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 3.6759 - accuracy: 0.9920 - val_loss: 7.4275 - val_accuracy: 0.9758\n",
      "Epoch 44/500\n",
      "187/207 [==========================>...] - ETA: 0s - loss: 4.0939 - accuracy: 0.9901\n",
      "Epoch 00044: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 4.0418 - accuracy: 0.9906 - val_loss: 8.6295 - val_accuracy: 0.9836\n",
      "Epoch 45/500\n",
      "187/207 [==========================>...] - ETA: 0s - loss: 3.7548 - accuracy: 0.9898\n",
      "Epoch 00045: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 3.8094 - accuracy: 0.9901 - val_loss: 8.3435 - val_accuracy: 0.9848\n",
      "Epoch 46/500\n",
      "186/207 [=========================>....] - ETA: 0s - loss: 3.8235 - accuracy: 0.9926\n",
      "Epoch 00046: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 3.7621 - accuracy: 0.9924 - val_loss: 6.8312 - val_accuracy: 0.9873\n",
      "Epoch 47/500\n",
      "182/207 [=========================>....] - ETA: 0s - loss: 3.7746 - accuracy: 0.9897\n",
      "Epoch 00047: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 3.8185 - accuracy: 0.9898 - val_loss: 7.9359 - val_accuracy: 0.9636\n",
      "Epoch 48/500\n",
      "179/207 [========================>.....] - ETA: 0s - loss: 4.0537 - accuracy: 0.9894\n",
      "Epoch 00048: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 3.9710 - accuracy: 0.9895 - val_loss: 7.8009 - val_accuracy: 0.9782\n",
      "Epoch 49/500\n",
      "184/207 [=========================>....] - ETA: 0s - loss: 3.4819 - accuracy: 0.9905\n",
      "Epoch 00049: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 3.4624 - accuracy: 0.9906 - val_loss: 7.5082 - val_accuracy: 0.9794\n",
      "Epoch 50/500\n",
      "185/207 [=========================>....] - ETA: 0s - loss: 3.2264 - accuracy: 0.9926\n",
      "Epoch 00050: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 3.2103 - accuracy: 0.9927 - val_loss: 7.5729 - val_accuracy: 0.9739\n",
      "Epoch 51/500\n",
      "181/207 [=========================>....] - ETA: 0s - loss: 3.6423 - accuracy: 0.9917\n",
      "Epoch 00051: val_loss did not improve from 3.54632\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 3.6432 - accuracy: 0.9912 - val_loss: 8.2500 - val_accuracy: 0.9776\n",
      "Epoch 00051: early stopping\n"
     ]
    }
   ],
   "source": [
    "history= model4.fit(X_train, Y_train, epochs=500, batch_size=32, validation_split = 0.2, callbacks=callback_list)\n",
    "#history= NN_model.fit(X_train, Y_train, epochs=7, batch_size=32, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.save('fraction_of_coating_40_50_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 4.004, Validation loss: 4.019\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluate the model\n",
    "train_loss, train_acc = model4.evaluate(X_train, Y_train, verbose=0)\n",
    "test_loss, test_acc = model4.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Train loss: %.3f, Validation loss: %.3f' % (train_loss, test_loss))\n",
    "# plot training history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latest=tf.train.latest_checkpoint(checkpoint_dir)\n",
    "weights_file = 'fraction_of_coating_40_50/Weights-001--3.54632.hdf5' # choose the best checkpoint \n",
    "model4.load_weights(weights_file) # load it\n",
    "model4.compile(loss='mean_absolute_percentage_error', optimizer='adam', metrics=['mean_absolute_percentage_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model4.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percentage error on test set:  [0.02274045 0.04976534 0.03182999]\n"
     ]
    }
   ],
   "source": [
    "error= mean_absolute_percentage_error(Y_test, Y_pred, multioutput='raw_values')   \n",
    "print('Mean absolute percentage error on test set: ', error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leave fraction of coating: ==40, 50, 60 as test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1896, 36)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set=df1[(df1['fraction_of_coating']<40) | (df1['fraction_of_coating']>60)]\n",
    "test_set=df1[(df1['fraction_of_coating']>=40) & (df1['fraction_of_coating']<=60)]\n",
    "test_set.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = train_set.iloc[:,25:28]\n",
    "X_train = train_set.iloc[:,:8]\n",
    "Y_test = test_set.iloc[:,25:28]\n",
    "X_test = test_set.iloc[:,:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model5 = load_model('random_split_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               1152      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 166,531\n",
      "Trainable params: 166,531\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model1 = Sequential()\n",
    "# # The Input Layer :\n",
    "# model1.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n",
    "\n",
    "# # The Hidden Layers :\n",
    "# model1.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "# model1.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "# model1.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "\n",
    "# # The Output Layer :\n",
    "# model1.add(Dense(3, kernel_initializer='normal',activation='linear'))\n",
    "# #model1.add(Dense(3, kernel_initializer='normal',activation='relu'))\n",
    "# # Compile the network :\n",
    "# model1.compile(loss='mean_absolute_percentage_error', optimizer='adam', metrics=['accuracy'])\n",
    "model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"fraction_of_coating_40_50_60/Weights-{epoch:03d}--{val_loss:.5f}.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, verbose=1, monitor='val_loss',save_best_only=True, mode='auto')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # patient early stopping\n",
    "es = EarlyStopping(monitor='val_loss', patience=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_csv=CSVLogger('fraction_of_coating_40_50_60_loss_logs.csv', separator=',', append=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_list=[checkpoint, es, log_csv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "185/199 [==========================>...] - ETA: 0s - loss: 3.6910 - accuracy: 0.9910\n",
      "Epoch 00001: val_loss improved from inf to 3.76456, saving model to fraction_of_coating_40_50_60\\Weights-001--3.76456.hdf5\n",
      "199/199 [==============================] - 0s 2ms/step - loss: 3.7043 - accuracy: 0.9912 - val_loss: 3.7646 - val_accuracy: 0.9981\n",
      "Epoch 2/500\n",
      "179/199 [=========================>....] - ETA: 0s - loss: 4.2463 - accuracy: 0.9909\n",
      "Epoch 00002: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 4.3565 - accuracy: 0.9912 - val_loss: 5.7807 - val_accuracy: 0.9912\n",
      "Epoch 3/500\n",
      "183/199 [==========================>...] - ETA: 0s - loss: 4.7308 - accuracy: 0.9899\n",
      "Epoch 00003: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 4.6599 - accuracy: 0.9901 - val_loss: 5.0564 - val_accuracy: 0.9924\n",
      "Epoch 4/500\n",
      "189/199 [===========================>..] - ETA: 0s - loss: 4.0710 - accuracy: 0.9921\n",
      "Epoch 00004: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 4.0367 - accuracy: 0.9923 - val_loss: 4.7803 - val_accuracy: 0.9918\n",
      "Epoch 5/500\n",
      "174/199 [=========================>....] - ETA: 0s - loss: 3.8106 - accuracy: 0.9925\n",
      "Epoch 00005: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 3.7311 - accuracy: 0.9924 - val_loss: 5.7817 - val_accuracy: 0.9893\n",
      "Epoch 6/500\n",
      "187/199 [===========================>..] - ETA: 0s - loss: 3.6807 - accuracy: 0.9923\n",
      "Epoch 00006: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 3.6508 - accuracy: 0.9921 - val_loss: 7.0030 - val_accuracy: 0.9817\n",
      "Epoch 7/500\n",
      "187/199 [===========================>..] - ETA: 0s - loss: 3.5211 - accuracy: 0.9915\n",
      "Epoch 00007: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 3.5479 - accuracy: 0.9915 - val_loss: 5.3320 - val_accuracy: 0.9912\n",
      "Epoch 8/500\n",
      "187/199 [===========================>..] - ETA: 0s - loss: 3.6679 - accuracy: 0.9925\n",
      "Epoch 00008: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 3.6889 - accuracy: 0.9927 - val_loss: 4.9359 - val_accuracy: 0.9880\n",
      "Epoch 9/500\n",
      "185/199 [==========================>...] - ETA: 0s - loss: 3.2888 - accuracy: 0.9921\n",
      "Epoch 00009: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 3.2541 - accuracy: 0.9916 - val_loss: 5.2930 - val_accuracy: 0.9893\n",
      "Epoch 10/500\n",
      "181/199 [==========================>...] - ETA: 0s - loss: 3.6469 - accuracy: 0.9926\n",
      "Epoch 00010: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 3.6150 - accuracy: 0.9929 - val_loss: 5.4987 - val_accuracy: 0.9880\n",
      "Epoch 11/500\n",
      "186/199 [===========================>..] - ETA: 0s - loss: 3.2929 - accuracy: 0.9931\n",
      "Epoch 00011: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 3.4549 - accuracy: 0.9924 - val_loss: 7.0099 - val_accuracy: 0.9779\n",
      "Epoch 12/500\n",
      "176/199 [=========================>....] - ETA: 0s - loss: 4.0643 - accuracy: 0.9918\n",
      "Epoch 00012: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 4.0032 - accuracy: 0.9913 - val_loss: 6.1871 - val_accuracy: 0.9792\n",
      "Epoch 13/500\n",
      "187/199 [===========================>..] - ETA: 0s - loss: 3.9702 - accuracy: 0.9911\n",
      "Epoch 00013: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 3.9786 - accuracy: 0.9912 - val_loss: 6.2008 - val_accuracy: 0.9824\n",
      "Epoch 14/500\n",
      "188/199 [===========================>..] - ETA: 0s - loss: 4.0320 - accuracy: 0.9897\n",
      "Epoch 00014: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 3.9876 - accuracy: 0.9898 - val_loss: 5.0281 - val_accuracy: 0.9887\n",
      "Epoch 15/500\n",
      "187/199 [===========================>..] - ETA: 0s - loss: 3.4894 - accuracy: 0.9931\n",
      "Epoch 00015: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 3.5219 - accuracy: 0.9931 - val_loss: 7.1115 - val_accuracy: 0.9735\n",
      "Epoch 16/500\n",
      "188/199 [===========================>..] - ETA: 0s - loss: 3.6220 - accuracy: 0.9929\n",
      "Epoch 00016: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 3.6285 - accuracy: 0.9929 - val_loss: 5.1105 - val_accuracy: 0.9773\n",
      "Epoch 17/500\n",
      "189/199 [===========================>..] - ETA: 0s - loss: 3.7910 - accuracy: 0.9921\n",
      "Epoch 00017: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 3.8392 - accuracy: 0.9924 - val_loss: 6.4046 - val_accuracy: 0.9874\n",
      "Epoch 18/500\n",
      "182/199 [==========================>...] - ETA: 0s - loss: 3.7114 - accuracy: 0.9931\n",
      "Epoch 00018: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 3.7256 - accuracy: 0.9929 - val_loss: 6.0742 - val_accuracy: 0.9798\n",
      "Epoch 19/500\n",
      "188/199 [===========================>..] - ETA: 0s - loss: 3.7275 - accuracy: 0.9915\n",
      "Epoch 00019: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 3.7163 - accuracy: 0.9918 - val_loss: 4.8871 - val_accuracy: 0.9868\n",
      "Epoch 20/500\n",
      "185/199 [==========================>...] - ETA: 0s - loss: 3.5327 - accuracy: 0.9921\n",
      "Epoch 00020: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 3.5952 - accuracy: 0.9921 - val_loss: 10.1143 - val_accuracy: 0.9666\n",
      "Epoch 21/500\n",
      "180/199 [==========================>...] - ETA: 0s - loss: 4.1537 - accuracy: 0.9920\n",
      "Epoch 00021: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 4.1231 - accuracy: 0.9920 - val_loss: 6.0186 - val_accuracy: 0.9824\n",
      "Epoch 22/500\n",
      "193/199 [============================>.] - ETA: 0s - loss: 3.8650 - accuracy: 0.9922\n",
      "Epoch 00022: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 3.8499 - accuracy: 0.9924 - val_loss: 6.4303 - val_accuracy: 0.9805\n",
      "Epoch 23/500\n",
      "187/199 [===========================>..] - ETA: 0s - loss: 3.5973 - accuracy: 0.9938\n",
      "Epoch 00023: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 3.5910 - accuracy: 0.9937 - val_loss: 5.6061 - val_accuracy: 0.9842\n",
      "Epoch 24/500\n",
      "189/199 [===========================>..] - ETA: 0s - loss: 3.7123 - accuracy: 0.9924\n",
      "Epoch 00024: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 3.6711 - accuracy: 0.9924 - val_loss: 5.5099 - val_accuracy: 0.9805\n",
      "Epoch 25/500\n",
      "190/199 [===========================>..] - ETA: 0s - loss: 3.5641 - accuracy: 0.9929\n",
      "Epoch 00025: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 3.5931 - accuracy: 0.9924 - val_loss: 8.7455 - val_accuracy: 0.9824\n",
      "Epoch 26/500\n",
      "184/199 [==========================>...] - ETA: 0s - loss: 3.6066 - accuracy: 0.9925\n",
      "Epoch 00026: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 3.6287 - accuracy: 0.9926 - val_loss: 5.9512 - val_accuracy: 0.9824\n",
      "Epoch 27/500\n",
      "176/199 [=========================>....] - ETA: 0s - loss: 3.4408 - accuracy: 0.9941\n",
      "Epoch 00027: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 3.3822 - accuracy: 0.9940 - val_loss: 5.7310 - val_accuracy: 0.9817\n",
      "Epoch 28/500\n",
      "190/199 [===========================>..] - ETA: 0s - loss: 3.4471 - accuracy: 0.9942\n",
      "Epoch 00028: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 3.5133 - accuracy: 0.9942 - val_loss: 5.7317 - val_accuracy: 0.9779\n",
      "Epoch 29/500\n",
      "186/199 [===========================>..] - ETA: 0s - loss: 3.9848 - accuracy: 0.9923\n",
      "Epoch 00029: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 4.0170 - accuracy: 0.9923 - val_loss: 5.8024 - val_accuracy: 0.9805\n",
      "Epoch 30/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181/199 [==========================>...] - ETA: 0s - loss: 3.6640 - accuracy: 0.9924\n",
      "Epoch 00030: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 3.7491 - accuracy: 0.9929 - val_loss: 8.2885 - val_accuracy: 0.9761\n",
      "Epoch 31/500\n",
      "186/199 [===========================>..] - ETA: 0s - loss: 4.3582 - accuracy: 0.9909\n",
      "Epoch 00031: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 4.2944 - accuracy: 0.9913 - val_loss: 7.4745 - val_accuracy: 0.9811\n",
      "Epoch 32/500\n",
      "189/199 [===========================>..] - ETA: 0s - loss: 3.3740 - accuracy: 0.9921\n",
      "Epoch 00032: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 3.4208 - accuracy: 0.9921 - val_loss: 6.0161 - val_accuracy: 0.9786\n",
      "Epoch 33/500\n",
      "183/199 [==========================>...] - ETA: 0s - loss: 3.2998 - accuracy: 0.9932\n",
      "Epoch 00033: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 3.3347 - accuracy: 0.9935 - val_loss: 6.5323 - val_accuracy: 0.9786\n",
      "Epoch 34/500\n",
      "180/199 [==========================>...] - ETA: 0s - loss: 3.5357 - accuracy: 0.9929\n",
      "Epoch 00034: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 3.5233 - accuracy: 0.9929 - val_loss: 6.0150 - val_accuracy: 0.9817\n",
      "Epoch 35/500\n",
      "185/199 [==========================>...] - ETA: 0s - loss: 3.7489 - accuracy: 0.9927\n",
      "Epoch 00035: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 3.7216 - accuracy: 0.9927 - val_loss: 6.6433 - val_accuracy: 0.9824\n",
      "Epoch 36/500\n",
      "191/199 [===========================>..] - ETA: 0s - loss: 4.0902 - accuracy: 0.9899\n",
      "Epoch 00036: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 4.0815 - accuracy: 0.9898 - val_loss: 6.2076 - val_accuracy: 0.9824\n",
      "Epoch 37/500\n",
      "175/199 [=========================>....] - ETA: 0s - loss: 3.9339 - accuracy: 0.9907\n",
      "Epoch 00037: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 3.8862 - accuracy: 0.9910 - val_loss: 6.2842 - val_accuracy: 0.9786\n",
      "Epoch 38/500\n",
      "191/199 [===========================>..] - ETA: 0s - loss: 3.7057 - accuracy: 0.9933\n",
      "Epoch 00038: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 3.6845 - accuracy: 0.9934 - val_loss: 8.1139 - val_accuracy: 0.9779\n",
      "Epoch 39/500\n",
      "190/199 [===========================>..] - ETA: 0s - loss: 3.9160 - accuracy: 0.9919\n",
      "Epoch 00039: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 3.9328 - accuracy: 0.9921 - val_loss: 8.1258 - val_accuracy: 0.9729\n",
      "Epoch 40/500\n",
      "179/199 [=========================>....] - ETA: 0s - loss: 4.2673 - accuracy: 0.9925\n",
      "Epoch 00040: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 4.2488 - accuracy: 0.9921 - val_loss: 7.1904 - val_accuracy: 0.9792\n",
      "Epoch 41/500\n",
      "188/199 [===========================>..] - ETA: 0s - loss: 3.8501 - accuracy: 0.9924\n",
      "Epoch 00041: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 3.7888 - accuracy: 0.9921 - val_loss: 6.8416 - val_accuracy: 0.9824\n",
      "Epoch 42/500\n",
      "190/199 [===========================>..] - ETA: 0s - loss: 3.4692 - accuracy: 0.9910\n",
      "Epoch 00042: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 3.5325 - accuracy: 0.9909 - val_loss: 6.6209 - val_accuracy: 0.9773\n",
      "Epoch 43/500\n",
      "185/199 [==========================>...] - ETA: 0s - loss: 3.6610 - accuracy: 0.9924\n",
      "Epoch 00043: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 3.6495 - accuracy: 0.9924 - val_loss: 8.7420 - val_accuracy: 0.9786\n",
      "Epoch 44/500\n",
      "187/199 [===========================>..] - ETA: 0s - loss: 3.3977 - accuracy: 0.9916\n",
      "Epoch 00044: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 3.4106 - accuracy: 0.9916 - val_loss: 8.1995 - val_accuracy: 0.9754\n",
      "Epoch 45/500\n",
      "184/199 [==========================>...] - ETA: 0s - loss: 3.9980 - accuracy: 0.9918\n",
      "Epoch 00045: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 4.0212 - accuracy: 0.9913 - val_loss: 8.0143 - val_accuracy: 0.9792\n",
      "Epoch 46/500\n",
      "186/199 [===========================>..] - ETA: 0s - loss: 3.3507 - accuracy: 0.9936\n",
      "Epoch 00046: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 3.3757 - accuracy: 0.9935 - val_loss: 7.9869 - val_accuracy: 0.9754\n",
      "Epoch 47/500\n",
      "184/199 [==========================>...] - ETA: 0s - loss: 3.6245 - accuracy: 0.9920\n",
      "Epoch 00047: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 3.6130 - accuracy: 0.9924 - val_loss: 8.4995 - val_accuracy: 0.9792\n",
      "Epoch 48/500\n",
      "187/199 [===========================>..] - ETA: 0s - loss: 3.6753 - accuracy: 0.9911\n",
      "Epoch 00048: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 3.6351 - accuracy: 0.9916 - val_loss: 9.0051 - val_accuracy: 0.9773\n",
      "Epoch 49/500\n",
      "190/199 [===========================>..] - ETA: 0s - loss: 3.7702 - accuracy: 0.9910\n",
      "Epoch 00049: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 3.7519 - accuracy: 0.9912 - val_loss: 8.8112 - val_accuracy: 0.9792\n",
      "Epoch 50/500\n",
      "198/199 [============================>.] - ETA: 0s - loss: 3.8634 - accuracy: 0.9931\n",
      "Epoch 00050: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 3.8626 - accuracy: 0.9931 - val_loss: 7.3046 - val_accuracy: 0.9805\n",
      "Epoch 51/500\n",
      "181/199 [==========================>...] - ETA: 0s - loss: 4.5613 - accuracy: 0.9898\n",
      "Epoch 00051: val_loss did not improve from 3.76456\n",
      "199/199 [==============================] - 0s 1ms/step - loss: 4.4937 - accuracy: 0.9899 - val_loss: 7.7231 - val_accuracy: 0.9754\n",
      "Epoch 00051: early stopping\n"
     ]
    }
   ],
   "source": [
    "history= model5.fit(X_train, Y_train, epochs=500, batch_size=32, validation_split = 0.2, callbacks=callback_list)\n",
    "#history= NN_model.fit(X_train, Y_train, epochs=7, batch_size=32, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5.save('fraction_of_coating_40_50_60_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.763, Validation loss: 4.250\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluate the model\n",
    "train_loss, train_acc = model5.evaluate(X_train, Y_train, verbose=0)\n",
    "test_loss, test_acc = model5.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Train loss: %.3f, Validation loss: %.3f' % (train_loss, test_loss))\n",
    "# plot training history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latest=tf.train.latest_checkpoint(checkpoint_dir)\n",
    "weights_file = 'fraction_of_coating_40_50_60/Weights-001--3.76456.hdf5' # choose the best checkpoint \n",
    "model5.load_weights(weights_file) # load it\n",
    "model5.compile(loss='mean_absolute_percentage_error', optimizer='adam', metrics=['mean_absolute_percentage_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model5.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percentage error on test set:  [0.01807816 0.0434247  0.02946333]\n"
     ]
    }
   ],
   "source": [
    "error= mean_absolute_percentage_error(Y_test, Y_pred, multioutput='raw_values')   \n",
    "print('Mean absolute percentage error on test set: ', error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leave wavelength = 530 out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6446, 36)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set=df1[(df1['wavelength']==467) | (df1['wavelength']==660)]\n",
    "test_set=df1[df1['wavelength']==530]\n",
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = train_set.iloc[:,25:28]\n",
    "X_train = train_set.iloc[:,:8]\n",
    "Y_test = test_set.iloc[:,25:28]\n",
    "X_test = test_set.iloc[:,:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6 = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model6 = load_model('random_split_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               1152      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 166,531\n",
      "Trainable params: 166,531\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model1 = Sequential()\n",
    "# # The Input Layer :\n",
    "# model1.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n",
    "\n",
    "# # The Hidden Layers :\n",
    "# model1.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "# model1.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "# model1.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "\n",
    "# # The Output Layer :\n",
    "# model1.add(Dense(3, kernel_initializer='normal',activation='linear'))\n",
    "# #model1.add(Dense(3, kernel_initializer='normal',activation='relu'))\n",
    "# # Compile the network :\n",
    "# model1.compile(loss='mean_absolute_percentage_error', optimizer='adam', metrics=['accuracy'])\n",
    "model6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"wavelength_530/Weights-{epoch:03d}--{val_loss:.5f}.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, verbose=1, monitor='val_loss',save_best_only=True, mode='auto')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # patient early stopping\n",
    "es = EarlyStopping(monitor='val_loss', patience=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_csv=CSVLogger('wavelength_530_loss_logs.csv', separator=',', append=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_list=[checkpoint, es, log_csv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "129/162 [======================>.......] - ETA: 0s - loss: 4.7911 - accuracy: 0.9845\n",
      "Epoch 00001: val_loss improved from inf to 4.49648, saving model to wavelength_530\\Weights-001--4.49648.hdf5\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 4.5918 - accuracy: 0.9849 - val_loss: 4.4965 - val_accuracy: 0.9907\n",
      "Epoch 2/500\n",
      "136/162 [========================>.....] - ETA: 0s - loss: 4.3550 - accuracy: 0.9835\n",
      "Epoch 00002: val_loss improved from 4.49648 to 3.40009, saving model to wavelength_530\\Weights-002--3.40009.hdf5\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 4.4827 - accuracy: 0.9839 - val_loss: 3.4001 - val_accuracy: 0.9953\n",
      "Epoch 3/500\n",
      "131/162 [=======================>......] - ETA: 0s - loss: 4.0954 - accuracy: 0.9864\n",
      "Epoch 00003: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 3.9582 - accuracy: 0.9864 - val_loss: 4.0566 - val_accuracy: 0.9969\n",
      "Epoch 4/500\n",
      "140/162 [========================>.....] - ETA: 0s - loss: 4.0540 - accuracy: 0.9850\n",
      "Epoch 00004: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 4.1170 - accuracy: 0.9858 - val_loss: 3.9412 - val_accuracy: 0.9946\n",
      "Epoch 5/500\n",
      "138/162 [========================>.....] - ETA: 0s - loss: 4.8013 - accuracy: 0.9839\n",
      "Epoch 00005: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 4.7102 - accuracy: 0.9839 - val_loss: 4.2646 - val_accuracy: 0.9922\n",
      "Epoch 6/500\n",
      "139/162 [========================>.....] - ETA: 0s - loss: 4.4475 - accuracy: 0.9836\n",
      "Epoch 00006: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 4.3585 - accuracy: 0.9847 - val_loss: 3.6805 - val_accuracy: 0.9930\n",
      "Epoch 7/500\n",
      "141/162 [=========================>....] - ETA: 0s - loss: 4.1122 - accuracy: 0.9845\n",
      "Epoch 00007: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 4.1260 - accuracy: 0.9843 - val_loss: 4.2240 - val_accuracy: 0.9876\n",
      "Epoch 8/500\n",
      "135/162 [========================>.....] - ETA: 0s - loss: 4.0685 - accuracy: 0.9859\n",
      "Epoch 00008: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 4.0135 - accuracy: 0.9862 - val_loss: 5.1627 - val_accuracy: 0.9915\n",
      "Epoch 9/500\n",
      "141/162 [=========================>....] - ETA: 0s - loss: 4.0514 - accuracy: 0.9860\n",
      "Epoch 00009: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 4.0907 - accuracy: 0.9862 - val_loss: 5.5981 - val_accuracy: 0.9984\n",
      "Epoch 10/500\n",
      "139/162 [========================>.....] - ETA: 0s - loss: 4.2041 - accuracy: 0.9865\n",
      "Epoch 00010: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 4.1769 - accuracy: 0.9870 - val_loss: 3.7829 - val_accuracy: 0.9798\n",
      "Epoch 11/500\n",
      "139/162 [========================>.....] - ETA: 0s - loss: 4.3948 - accuracy: 0.9843\n",
      "Epoch 00011: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 4.2956 - accuracy: 0.9851 - val_loss: 4.4529 - val_accuracy: 0.9837\n",
      "Epoch 12/500\n",
      "140/162 [========================>.....] - ETA: 0s - loss: 3.8481 - accuracy: 0.9866\n",
      "Epoch 00012: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 3.7938 - accuracy: 0.9866 - val_loss: 6.8825 - val_accuracy: 0.9899\n",
      "Epoch 13/500\n",
      "141/162 [=========================>....] - ETA: 0s - loss: 4.0742 - accuracy: 0.9865\n",
      "Epoch 00013: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 4.0149 - accuracy: 0.9866 - val_loss: 5.1552 - val_accuracy: 0.9845\n",
      "Epoch 14/500\n",
      "128/162 [======================>.......] - ETA: 0s - loss: 4.7371 - accuracy: 0.9849\n",
      "Epoch 00014: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 4.6906 - accuracy: 0.9847 - val_loss: 5.8727 - val_accuracy: 0.9853\n",
      "Epoch 15/500\n",
      "125/162 [======================>.......] - ETA: 0s - loss: 3.9000 - accuracy: 0.9870\n",
      "Epoch 00015: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 4.0115 - accuracy: 0.9858 - val_loss: 5.0373 - val_accuracy: 0.9853\n",
      "Epoch 16/500\n",
      "139/162 [========================>.....] - ETA: 0s - loss: 4.2871 - accuracy: 0.9863\n",
      "Epoch 00016: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 4.1530 - accuracy: 0.9868 - val_loss: 4.3161 - val_accuracy: 0.9775\n",
      "Epoch 17/500\n",
      "137/162 [========================>.....] - ETA: 0s - loss: 4.3023 - accuracy: 0.9847\n",
      "Epoch 00017: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 4.2777 - accuracy: 0.9862 - val_loss: 5.8877 - val_accuracy: 0.9884\n",
      "Epoch 18/500\n",
      "141/162 [=========================>....] - ETA: 0s - loss: 4.5153 - accuracy: 0.9849\n",
      "Epoch 00018: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 4.4182 - accuracy: 0.9855 - val_loss: 8.6153 - val_accuracy: 0.9729\n",
      "Epoch 19/500\n",
      "139/162 [========================>.....] - ETA: 0s - loss: 4.9655 - accuracy: 0.9847\n",
      "Epoch 00019: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 4.7483 - accuracy: 0.9845 - val_loss: 6.2148 - val_accuracy: 0.9806\n",
      "Epoch 20/500\n",
      "129/162 [======================>.......] - ETA: 0s - loss: 3.8907 - accuracy: 0.9867\n",
      "Epoch 00020: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 3.8286 - accuracy: 0.9866 - val_loss: 5.9386 - val_accuracy: 0.9891\n",
      "Epoch 21/500\n",
      "140/162 [========================>.....] - ETA: 0s - loss: 4.1014 - accuracy: 0.9871\n",
      "Epoch 00021: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 4.0501 - accuracy: 0.9870 - val_loss: 5.6915 - val_accuracy: 0.9907\n",
      "Epoch 22/500\n",
      "138/162 [========================>.....] - ETA: 0s - loss: 4.0616 - accuracy: 0.9864\n",
      "Epoch 00022: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 4.0035 - accuracy: 0.9868 - val_loss: 7.4826 - val_accuracy: 0.9783\n",
      "Epoch 23/500\n",
      "143/162 [=========================>....] - ETA: 0s - loss: 3.9195 - accuracy: 0.9856\n",
      "Epoch 00023: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 3.9196 - accuracy: 0.9855 - val_loss: 7.2383 - val_accuracy: 0.9767\n",
      "Epoch 24/500\n",
      "131/162 [=======================>......] - ETA: 0s - loss: 4.7495 - accuracy: 0.9823\n",
      "Epoch 00024: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 4.5439 - accuracy: 0.9837 - val_loss: 5.5288 - val_accuracy: 0.9767\n",
      "Epoch 25/500\n",
      "161/162 [============================>.] - ETA: 0s - loss: 4.1960 - accuracy: 0.9858\n",
      "Epoch 00025: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 4.1945 - accuracy: 0.9858 - val_loss: 6.5059 - val_accuracy: 0.9760\n",
      "Epoch 26/500\n",
      "136/162 [========================>.....] - ETA: 0s - loss: 3.7183 - accuracy: 0.9876\n",
      "Epoch 00026: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 3.7756 - accuracy: 0.9864 - val_loss: 6.7285 - val_accuracy: 0.9791\n",
      "Epoch 27/500\n",
      "136/162 [========================>.....] - ETA: 0s - loss: 4.2178 - accuracy: 0.9869\n",
      "Epoch 00027: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 4.2406 - accuracy: 0.9858 - val_loss: 5.5807 - val_accuracy: 0.9814\n",
      "Epoch 28/500\n",
      "139/162 [========================>.....] - ETA: 0s - loss: 4.5284 - accuracy: 0.9863\n",
      "Epoch 00028: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 4.4835 - accuracy: 0.9868 - val_loss: 7.4673 - val_accuracy: 0.9884\n",
      "Epoch 29/500\n",
      "143/162 [=========================>....] - ETA: 0s - loss: 4.2390 - accuracy: 0.9836\n",
      "Epoch 00029: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 4.2379 - accuracy: 0.9843 - val_loss: 5.3826 - val_accuracy: 0.9752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/500\n",
      "142/162 [=========================>....] - ETA: 0s - loss: 4.2796 - accuracy: 0.9822\n",
      "Epoch 00030: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 4.2415 - accuracy: 0.9822 - val_loss: 6.2894 - val_accuracy: 0.9752\n",
      "Epoch 31/500\n",
      "135/162 [========================>.....] - ETA: 0s - loss: 4.0852 - accuracy: 0.9880\n",
      "Epoch 00031: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 4.0060 - accuracy: 0.9882 - val_loss: 6.2294 - val_accuracy: 0.9736\n",
      "Epoch 32/500\n",
      "136/162 [========================>.....] - ETA: 0s - loss: 3.8926 - accuracy: 0.9855\n",
      "Epoch 00032: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 3.8710 - accuracy: 0.9855 - val_loss: 6.2701 - val_accuracy: 0.9752\n",
      "Epoch 33/500\n",
      "138/162 [========================>.....] - ETA: 0s - loss: 4.2786 - accuracy: 0.9853\n",
      "Epoch 00033: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 4.3308 - accuracy: 0.9851 - val_loss: 8.5384 - val_accuracy: 0.9698\n",
      "Epoch 34/500\n",
      "135/162 [========================>.....] - ETA: 0s - loss: 4.4549 - accuracy: 0.9847\n",
      "Epoch 00034: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 4.4271 - accuracy: 0.9853 - val_loss: 6.5697 - val_accuracy: 0.9760\n",
      "Epoch 35/500\n",
      "139/162 [========================>.....] - ETA: 0s - loss: 4.1210 - accuracy: 0.9854\n",
      "Epoch 00035: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 4.2602 - accuracy: 0.9858 - val_loss: 6.8778 - val_accuracy: 0.9682\n",
      "Epoch 36/500\n",
      "138/162 [========================>.....] - ETA: 0s - loss: 4.2113 - accuracy: 0.9880\n",
      "Epoch 00036: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 4.2780 - accuracy: 0.9874 - val_loss: 8.1719 - val_accuracy: 0.9636\n",
      "Epoch 37/500\n",
      "137/162 [========================>.....] - ETA: 0s - loss: 3.7595 - accuracy: 0.9856\n",
      "Epoch 00037: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 3.7881 - accuracy: 0.9866 - val_loss: 6.3861 - val_accuracy: 0.9736\n",
      "Epoch 38/500\n",
      "134/162 [=======================>......] - ETA: 0s - loss: 4.2163 - accuracy: 0.9862\n",
      "Epoch 00038: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 4.0938 - accuracy: 0.9862 - val_loss: 7.3185 - val_accuracy: 0.9643\n",
      "Epoch 39/500\n",
      "141/162 [=========================>....] - ETA: 0s - loss: 4.6205 - accuracy: 0.9825\n",
      "Epoch 00039: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 4.5021 - accuracy: 0.9835 - val_loss: 7.7854 - val_accuracy: 0.9705\n",
      "Epoch 40/500\n",
      "140/162 [========================>.....] - ETA: 0s - loss: 3.7185 - accuracy: 0.9866\n",
      "Epoch 00040: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 3.7178 - accuracy: 0.9872 - val_loss: 7.6627 - val_accuracy: 0.9659\n",
      "Epoch 41/500\n",
      "134/162 [=======================>......] - ETA: 0s - loss: 4.3735 - accuracy: 0.9839\n",
      "Epoch 00041: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 4.2943 - accuracy: 0.9847 - val_loss: 6.4009 - val_accuracy: 0.9729\n",
      "Epoch 42/500\n",
      "127/162 [======================>.......] - ETA: 0s - loss: 3.6823 - accuracy: 0.9879\n",
      "Epoch 00042: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 3.7726 - accuracy: 0.9862 - val_loss: 6.4713 - val_accuracy: 0.9729\n",
      "Epoch 43/500\n",
      "139/162 [========================>.....] - ETA: 0s - loss: 4.2925 - accuracy: 0.9876\n",
      "Epoch 00043: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 4.2420 - accuracy: 0.9872 - val_loss: 7.0278 - val_accuracy: 0.9767\n",
      "Epoch 44/500\n",
      "134/162 [=======================>......] - ETA: 0s - loss: 4.0122 - accuracy: 0.9820\n",
      "Epoch 00044: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 4.0540 - accuracy: 0.9827 - val_loss: 5.9886 - val_accuracy: 0.9667\n",
      "Epoch 45/500\n",
      "138/162 [========================>.....] - ETA: 0s - loss: 3.7436 - accuracy: 0.9882\n",
      "Epoch 00045: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 3.7243 - accuracy: 0.9878 - val_loss: 8.9055 - val_accuracy: 0.9744\n",
      "Epoch 46/500\n",
      "139/162 [========================>.....] - ETA: 0s - loss: 4.8747 - accuracy: 0.9847\n",
      "Epoch 00046: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 4.7135 - accuracy: 0.9847 - val_loss: 7.2073 - val_accuracy: 0.9744\n",
      "Epoch 47/500\n",
      "137/162 [========================>.....] - ETA: 0s - loss: 4.3489 - accuracy: 0.9870\n",
      "Epoch 00047: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 4.2588 - accuracy: 0.9866 - val_loss: 6.3117 - val_accuracy: 0.9674\n",
      "Epoch 48/500\n",
      "136/162 [========================>.....] - ETA: 0s - loss: 4.1908 - accuracy: 0.9844\n",
      "Epoch 00048: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 4.1888 - accuracy: 0.9855 - val_loss: 5.8645 - val_accuracy: 0.9736\n",
      "Epoch 49/500\n",
      "141/162 [=========================>....] - ETA: 0s - loss: 4.8736 - accuracy: 0.9860\n",
      "Epoch 00049: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 4.7529 - accuracy: 0.9853 - val_loss: 6.4780 - val_accuracy: 0.9729\n",
      "Epoch 50/500\n",
      "136/162 [========================>.....] - ETA: 0s - loss: 4.1958 - accuracy: 0.9855\n",
      "Epoch 00050: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 4.2442 - accuracy: 0.9856 - val_loss: 6.4110 - val_accuracy: 0.9705\n",
      "Epoch 51/500\n",
      "139/162 [========================>.....] - ETA: 0s - loss: 3.9902 - accuracy: 0.9840\n",
      "Epoch 00051: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 3.8936 - accuracy: 0.9856 - val_loss: 7.2814 - val_accuracy: 0.9736\n",
      "Epoch 52/500\n",
      "131/162 [=======================>......] - ETA: 0s - loss: 4.0198 - accuracy: 0.9871\n",
      "Epoch 00052: val_loss did not improve from 3.40009\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 3.9664 - accuracy: 0.9874 - val_loss: 7.0504 - val_accuracy: 0.9698\n",
      "Epoch 00052: early stopping\n"
     ]
    }
   ],
   "source": [
    "history= model6.fit(X_train, Y_train, epochs=500, batch_size=32, validation_split = 0.2, callbacks=callback_list)\n",
    "#history= NN_model.fit(X_train, Y_train, epochs=7, batch_size=32, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6.save('wavelength_530_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 4.282, Validation loss: 8.594\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluate the model\n",
    "train_loss, train_acc = model6.evaluate(X_train, Y_train, verbose=0)\n",
    "test_loss, test_acc = model6.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Train loss: %.3f, Validation loss: %.3f' % (train_loss, test_loss))\n",
    "# plot training history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latest=tf.train.latest_checkpoint(checkpoint_dir)\n",
    "weights_file = 'Weights-484--1.53014.hdf5' # choose the best checkpoint \n",
    "model6.load_weights(weights_file) # load it\n",
    "model6.compile(loss='mean_absolute_percentage_error', optimizer='adam', metrics=['mean_absolute_percentage_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model6.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percentage error on test set:  [0.01409073 0.07733258 0.04204335]\n"
     ]
    }
   ],
   "source": [
    "error= mean_absolute_percentage_error(Y_test, Y_pred, multioutput='raw_values')   \n",
    "print('Mean absolute percentage error on test set: ', error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extrapolating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leave fractal dimesnion: =2.8 as test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(141, 36)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set=df1[(df1['fractal_dimension']<2.8)]\n",
    "test_set=df1[df1['fractal_dimension']==2.8]\n",
    "test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = train_set.iloc[:,25:28]\n",
    "X_train = train_set.iloc[:,:8]\n",
    "Y_test = test_set.iloc[:,25:28]\n",
    "X_test = test_set.iloc[:,:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model7 = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model7 = load_model('random_split_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               1152      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 166,531\n",
      "Trainable params: 166,531\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model1 = Sequential()\n",
    "# # The Input Layer :\n",
    "# model1.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n",
    "\n",
    "# # The Hidden Layers :\n",
    "# model1.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "# model1.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "# model1.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "\n",
    "# # The Output Layer :\n",
    "# model1.add(Dense(3, kernel_initializer='normal',activation='linear'))\n",
    "# #model1.add(Dense(3, kernel_initializer='normal',activation='relu'))\n",
    "# # Compile the network :\n",
    "# model1.compile(loss='mean_absolute_percentage_error', optimizer='adam', metrics=['accuracy'])\n",
    "model7.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"fractal_dimension_2_8/Weights-{epoch:03d}--{val_loss:.5f}.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, verbose=1, monitor='val_loss',save_best_only=True, mode='auto')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # patient early stopping\n",
    "es = EarlyStopping(monitor='val_loss', patience=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_csv=CSVLogger('fractal_dimension_2_8_loss_logs.csv', separator=',', append=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_list=[checkpoint, es, log_csv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "230/243 [===========================>..] - ETA: 0s - loss: 3.7920 - accuracy: 0.9871\n",
      "Epoch 00001: val_loss improved from inf to 4.57135, saving model to fractal_dimension_2_8\\Weights-001--4.57135.hdf5\n",
      "243/243 [==============================] - 0s 2ms/step - loss: 3.7930 - accuracy: 0.9875 - val_loss: 4.5713 - val_accuracy: 0.9892\n",
      "Epoch 2/500\n",
      "230/243 [===========================>..] - ETA: 0s - loss: 3.9281 - accuracy: 0.9857\n",
      "Epoch 00002: val_loss improved from 4.57135 to 4.16474, saving model to fractal_dimension_2_8\\Weights-002--4.16474.hdf5\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 3.9085 - accuracy: 0.9858 - val_loss: 4.1647 - val_accuracy: 0.9845\n",
      "Epoch 3/500\n",
      "226/243 [==========================>...] - ETA: 0s - loss: 3.4562 - accuracy: 0.9847\n",
      "Epoch 00003: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 3.4636 - accuracy: 0.9849 - val_loss: 7.1095 - val_accuracy: 0.9768\n",
      "Epoch 4/500\n",
      "217/243 [=========================>....] - ETA: 0s - loss: 4.1750 - accuracy: 0.9865\n",
      "Epoch 00004: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 4.0388 - accuracy: 0.9866 - val_loss: 4.9840 - val_accuracy: 0.9881\n",
      "Epoch 5/500\n",
      "204/243 [========================>.....] - ETA: 0s - loss: 4.3705 - accuracy: 0.9862 ETA: 0s - loss: 4.3720 - accuracy: 0. - ETA: 0s - loss: 4.3763 - accuracy: 0.9854\n",
      "Epoch 00005: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 2ms/step - loss: 4.2980 - accuracy: 0.9852 - val_loss: 7.1100 - val_accuracy: 0.9778\n",
      "Epoch 6/500\n",
      "235/243 [============================>.] - ETA: 0s - loss: 3.8974 - accuracy: 0.9838\n",
      "Epoch 00006: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 3.8822 - accuracy: 0.9841 - val_loss: 5.0418 - val_accuracy: 0.9856\n",
      "Epoch 7/500\n",
      "234/243 [===========================>..] - ETA: 0s - loss: 3.3217 - accuracy: 0.9861\n",
      "Epoch 00007: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 3.3710 - accuracy: 0.9862 - val_loss: 4.6698 - val_accuracy: 0.9830\n",
      "Epoch 8/500\n",
      "223/243 [==========================>...] - ETA: 0s - loss: 3.4634 - accuracy: 0.9879\n",
      "Epoch 00008: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 3.5612 - accuracy: 0.9881 - val_loss: 5.9782 - val_accuracy: 0.9799\n",
      "Epoch 9/500\n",
      "235/243 [============================>.] - ETA: 0s - loss: 3.7049 - accuracy: 0.9867\n",
      "Epoch 00009: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 3.7258 - accuracy: 0.9864 - val_loss: 4.6422 - val_accuracy: 0.9876\n",
      "Epoch 10/500\n",
      "234/243 [===========================>..] - ETA: 0s - loss: 3.7941 - accuracy: 0.9865\n",
      "Epoch 00010: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 3.7949 - accuracy: 0.9867 - val_loss: 6.9312 - val_accuracy: 0.9861\n",
      "Epoch 11/500\n",
      "231/243 [===========================>..] - ETA: 0s - loss: 4.3716 - accuracy: 0.9840\n",
      "Epoch 00011: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 4.3082 - accuracy: 0.9846 - val_loss: 7.3672 - val_accuracy: 0.9835\n",
      "Epoch 12/500\n",
      "231/243 [===========================>..] - ETA: 0s - loss: 3.8982 - accuracy: 0.9867\n",
      "Epoch 00012: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 3.8622 - accuracy: 0.9870 - val_loss: 7.0882 - val_accuracy: 0.9819\n",
      "Epoch 13/500\n",
      "232/243 [===========================>..] - ETA: 0s - loss: 3.8683 - accuracy: 0.9850\n",
      "Epoch 00013: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 3.8475 - accuracy: 0.9855 - val_loss: 10.0512 - val_accuracy: 0.9752\n",
      "Epoch 14/500\n",
      "237/243 [============================>.] - ETA: 0s - loss: 3.7019 - accuracy: 0.9862\n",
      "Epoch 00014: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 3.6966 - accuracy: 0.9862 - val_loss: 6.5601 - val_accuracy: 0.9757\n",
      "Epoch 15/500\n",
      "233/243 [===========================>..] - ETA: 0s - loss: 3.8756 - accuracy: 0.9886\n",
      "Epoch 00015: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 3.8611 - accuracy: 0.9886 - val_loss: 5.8496 - val_accuracy: 0.9809\n",
      "Epoch 16/500\n",
      "236/243 [============================>.] - ETA: 0s - loss: 3.5958 - accuracy: 0.9865\n",
      "Epoch 00016: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 3.5923 - accuracy: 0.9861 - val_loss: 6.7922 - val_accuracy: 0.9794\n",
      "Epoch 17/500\n",
      "233/243 [===========================>..] - ETA: 0s - loss: 4.2521 - accuracy: 0.9852\n",
      "Epoch 00017: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 4.2150 - accuracy: 0.9850 - val_loss: 5.3895 - val_accuracy: 0.9788\n",
      "Epoch 18/500\n",
      "231/243 [===========================>..] - ETA: 0s - loss: 3.6271 - accuracy: 0.9862\n",
      "Epoch 00018: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 3.6005 - accuracy: 0.9862 - val_loss: 7.9179 - val_accuracy: 0.9634\n",
      "Epoch 19/500\n",
      "239/243 [============================>.] - ETA: 0s - loss: 3.4777 - accuracy: 0.9872\n",
      "Epoch 00019: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 3.4792 - accuracy: 0.9872 - val_loss: 7.0125 - val_accuracy: 0.9747\n",
      "Epoch 20/500\n",
      "235/243 [============================>.] - ETA: 0s - loss: 3.5450 - accuracy: 0.9886\n",
      "Epoch 00020: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 3.5350 - accuracy: 0.9885 - val_loss: 5.5508 - val_accuracy: 0.9794\n",
      "Epoch 21/500\n",
      "237/243 [============================>.] - ETA: 0s - loss: 3.7046 - accuracy: 0.9843\n",
      "Epoch 00021: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 3.7068 - accuracy: 0.9843 - val_loss: 6.7948 - val_accuracy: 0.9778\n",
      "Epoch 22/500\n",
      "218/243 [=========================>....] - ETA: 0s - loss: 3.4177 - accuracy: 0.9870\n",
      "Epoch 00022: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 3.5250 - accuracy: 0.9868 - val_loss: 7.8359 - val_accuracy: 0.9690\n",
      "Epoch 23/500\n",
      "237/243 [============================>.] - ETA: 0s - loss: 4.3688 - accuracy: 0.9852\n",
      "Epoch 00023: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 4.3510 - accuracy: 0.9849 - val_loss: 6.2321 - val_accuracy: 0.9804\n",
      "Epoch 24/500\n",
      "207/243 [========================>.....] - ETA: 0s - loss: 3.4779 - accuracy: 0.9891\n",
      "Epoch 00024: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 2ms/step - loss: 3.4404 - accuracy: 0.9892 - val_loss: 6.4392 - val_accuracy: 0.9783\n",
      "Epoch 25/500\n",
      "212/243 [=========================>....] - ETA: 0s - loss: 3.4821 - accuracy: 0.9872\n",
      "Epoch 00025: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 2ms/step - loss: 3.5144 - accuracy: 0.9859 - val_loss: 12.1083 - val_accuracy: 0.9732\n",
      "Epoch 26/500\n",
      "231/243 [===========================>..] - ETA: 0s - loss: 3.5881 - accuracy: 0.9874\n",
      "Epoch 00026: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 3.5526 - accuracy: 0.9876 - val_loss: 7.7624 - val_accuracy: 0.9752\n",
      "Epoch 27/500\n",
      "230/243 [===========================>..] - ETA: 0s - loss: 3.3183 - accuracy: 0.9859\n",
      "Epoch 00027: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 3.3446 - accuracy: 0.9855 - val_loss: 6.5717 - val_accuracy: 0.9732\n",
      "Epoch 28/500\n",
      "238/243 [============================>.] - ETA: 0s - loss: 3.7168 - accuracy: 0.9849\n",
      "Epoch 00028: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 3.7130 - accuracy: 0.9849 - val_loss: 7.7031 - val_accuracy: 0.9752\n",
      "Epoch 29/500\n",
      "237/243 [============================>.] - ETA: 0s - loss: 3.8605 - accuracy: 0.9864\n",
      "Epoch 00029: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 3.8844 - accuracy: 0.9863 - val_loss: 8.0687 - val_accuracy: 0.9747\n",
      "Epoch 30/500\n",
      "236/243 [============================>.] - ETA: 0s - loss: 3.7069 - accuracy: 0.9865\n",
      "Epoch 00030: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 3.7070 - accuracy: 0.9866 - val_loss: 6.7518 - val_accuracy: 0.9788\n",
      "Epoch 31/500\n",
      "232/243 [===========================>..] - ETA: 0s - loss: 3.5747 - accuracy: 0.9872\n",
      "Epoch 00031: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 3.5962 - accuracy: 0.9876 - val_loss: 8.8644 - val_accuracy: 0.9737\n",
      "Epoch 32/500\n",
      "226/243 [==========================>...] - ETA: 0s - loss: 3.7512 - accuracy: 0.9862\n",
      "Epoch 00032: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 3.7785 - accuracy: 0.9863 - val_loss: 8.4695 - val_accuracy: 0.9768\n",
      "Epoch 33/500\n",
      "207/243 [========================>.....] - ETA: 0s - loss: 3.7300 - accuracy: 0.9869\n",
      "Epoch 00033: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 3.7575 - accuracy: 0.9864 - val_loss: 8.8041 - val_accuracy: 0.9783\n",
      "Epoch 34/500\n",
      "226/243 [==========================>...] - ETA: 0s - loss: 3.7280 - accuracy: 0.9882\n",
      "Epoch 00034: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 3.7222 - accuracy: 0.9877 - val_loss: 8.2299 - val_accuracy: 0.9727\n",
      "Epoch 35/500\n",
      "234/243 [===========================>..] - ETA: 0s - loss: 3.6491 - accuracy: 0.9860\n",
      "Epoch 00035: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 3.6443 - accuracy: 0.9862 - val_loss: 7.5568 - val_accuracy: 0.9747\n",
      "Epoch 36/500\n",
      "223/243 [==========================>...] - ETA: 0s - loss: 3.7191 - accuracy: 0.9863\n",
      "Epoch 00036: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 3.7983 - accuracy: 0.9867 - val_loss: 10.4567 - val_accuracy: 0.9763\n",
      "Epoch 37/500\n",
      "227/243 [===========================>..] - ETA: 0s - loss: 3.6054 - accuracy: 0.9861\n",
      "Epoch 00037: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 3.6178 - accuracy: 0.9863 - val_loss: 9.0505 - val_accuracy: 0.9773\n",
      "Epoch 38/500\n",
      "228/243 [===========================>..] - ETA: 0s - loss: 3.9272 - accuracy: 0.9868\n",
      "Epoch 00038: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 3.9259 - accuracy: 0.9862 - val_loss: 8.1459 - val_accuracy: 0.9721\n",
      "Epoch 39/500\n",
      "221/243 [==========================>...] - ETA: 0s - loss: 3.8502 - accuracy: 0.9860\n",
      "Epoch 00039: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 3.7485 - accuracy: 0.9858 - val_loss: 7.1483 - val_accuracy: 0.9747\n",
      "Epoch 40/500\n",
      "228/243 [===========================>..] - ETA: 0s - loss: 3.3993 - accuracy: 0.9881\n",
      "Epoch 00040: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 3.4361 - accuracy: 0.9876 - val_loss: 8.2084 - val_accuracy: 0.9788\n",
      "Epoch 41/500\n",
      "229/243 [===========================>..] - ETA: 0s - loss: 3.5455 - accuracy: 0.9870\n",
      "Epoch 00041: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 3.5797 - accuracy: 0.9872 - val_loss: 12.1650 - val_accuracy: 0.9773\n",
      "Epoch 42/500\n",
      "228/243 [===========================>..] - ETA: 0s - loss: 3.8363 - accuracy: 0.9856\n",
      "Epoch 00042: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 3.8141 - accuracy: 0.9861 - val_loss: 10.6034 - val_accuracy: 0.9737\n",
      "Epoch 43/500\n",
      "226/243 [==========================>...] - ETA: 0s - loss: 3.5626 - accuracy: 0.9863\n",
      "Epoch 00043: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 3.5363 - accuracy: 0.9868 - val_loss: 8.7530 - val_accuracy: 0.9768\n",
      "Epoch 44/500\n",
      "231/243 [===========================>..] - ETA: 0s - loss: 3.6634 - accuracy: 0.9858\n",
      "Epoch 00044: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 3.6312 - accuracy: 0.9862 - val_loss: 7.4846 - val_accuracy: 0.9732\n",
      "Epoch 45/500\n",
      "232/243 [===========================>..] - ETA: 0s - loss: 3.4322 - accuracy: 0.9860\n",
      "Epoch 00045: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 2ms/step - loss: 3.4243 - accuracy: 0.9862 - val_loss: 7.1525 - val_accuracy: 0.9794\n",
      "Epoch 46/500\n",
      "225/243 [==========================>...] - ETA: 0s - loss: 3.7561 - accuracy: 0.9874\n",
      "Epoch 00046: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 2ms/step - loss: 3.7736 - accuracy: 0.9866 - val_loss: 7.8781 - val_accuracy: 0.9737\n",
      "Epoch 47/500\n",
      "204/243 [========================>.....] - ETA: 0s - loss: 3.2441 - accuracy: 0.9873\n",
      "Epoch 00047: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 3.2472 - accuracy: 0.9870 - val_loss: 7.8287 - val_accuracy: 0.9696\n",
      "Epoch 48/500\n",
      "229/243 [===========================>..] - ETA: 0s - loss: 3.4928 - accuracy: 0.9880\n",
      "Epoch 00048: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 3.4809 - accuracy: 0.9876 - val_loss: 8.7179 - val_accuracy: 0.9788\n",
      "Epoch 49/500\n",
      "220/243 [==========================>...] - ETA: 0s - loss: 3.7239 - accuracy: 0.9861\n",
      "Epoch 00049: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 3.6873 - accuracy: 0.9867 - val_loss: 8.1587 - val_accuracy: 0.9711\n",
      "Epoch 50/500\n",
      "229/243 [===========================>..] - ETA: 0s - loss: 3.9466 - accuracy: 0.9839\n",
      "Epoch 00050: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 3.8891 - accuracy: 0.9845 - val_loss: 11.3271 - val_accuracy: 0.9727\n",
      "Epoch 51/500\n",
      "225/243 [==========================>...] - ETA: 0s - loss: 3.8800 - accuracy: 0.9858\n",
      "Epoch 00051: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 3.9138 - accuracy: 0.9864 - val_loss: 9.0502 - val_accuracy: 0.9737\n",
      "Epoch 52/500\n",
      "234/243 [===========================>..] - ETA: 0s - loss: 3.3113 - accuracy: 0.9870\n",
      "Epoch 00052: val_loss did not improve from 4.16474\n",
      "243/243 [==============================] - 0s 1ms/step - loss: 3.3577 - accuracy: 0.9867 - val_loss: 8.7299 - val_accuracy: 0.9732\n",
      "Epoch 00052: early stopping\n"
     ]
    }
   ],
   "source": [
    "history= model7.fit(X_train, Y_train, epochs=500, batch_size=32, validation_split = 0.2, callbacks=callback_list)\n",
    "#history= NN_model.fit(X_train, Y_train, epochs=7, batch_size=32, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model7.save('fractal_dimension_2_8_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 4.499, Validation loss: 6.066\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluate the model\n",
    "train_loss, train_acc = model7.evaluate(X_train, Y_train, verbose=0)\n",
    "test_loss, test_acc = model7.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Train loss: %.3f, Validation loss: %.3f' % (train_loss, test_loss))\n",
    "# plot training history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latest=tf.train.latest_checkpoint(checkpoint_dir)\n",
    "weights_file = 'fractal_dimension_2_8/Weights-002--4.16474.hdf5' # choose the best checkpoint \n",
    "model7.load_weights(weights_file) # load it\n",
    "model7.compile(loss='mean_absolute_percentage_error', optimizer='adam', metrics=['mean_absolute_percentage_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model7.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percentage error on test set:  [0.01472464 0.02788006 0.06985561]\n"
     ]
    }
   ],
   "source": [
    "error= mean_absolute_percentage_error(Y_test, Y_pred, multioutput='raw_values')   \n",
    "print('Mean absolute percentage error on test set: ', error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leave wavelength = 467 out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3014, 36)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set=df1[df1['wavelength']>467]\n",
    "test_set=df1[df1['wavelength']==467]\n",
    "test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = train_set.iloc[:,25:28]\n",
    "X_train = train_set.iloc[:,:8]\n",
    "Y_test = test_set.iloc[:,25:28]\n",
    "X_test = test_set.iloc[:,:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model8 = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model8 = load_model('random_split_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               1152      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 166,531\n",
      "Trainable params: 166,531\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model1 = Sequential()\n",
    "# # The Input Layer :\n",
    "# model1.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n",
    "\n",
    "# # The Hidden Layers :\n",
    "# model1.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "# model1.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "# model1.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "\n",
    "# # The Output Layer :\n",
    "# model1.add(Dense(3, kernel_initializer='normal',activation='linear'))\n",
    "# #model1.add(Dense(3, kernel_initializer='normal',activation='relu'))\n",
    "# # Compile the network :\n",
    "# model1.compile(loss='mean_absolute_percentage_error', optimizer='adam', metrics=['accuracy'])\n",
    "model8.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"wavelength_467/Weights-{epoch:03d}--{val_loss:.5f}.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, verbose=1, monitor='val_loss',save_best_only=True, mode='auto')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # patient early stopping\n",
    "es = EarlyStopping(monitor='val_loss', patience=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_csv=CSVLogger('wavelength_467_loss_logs.csv', separator=',', append=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_list=[checkpoint, es, log_csv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "137/171 [=======================>......] - ETA: 0s - loss: 4.1833 - accuracy: 0.9836\n",
      "Epoch 00001: val_loss improved from inf to 5.47018, saving model to wavelength_467\\Weights-001--5.47018.hdf5\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 4.0998 - accuracy: 0.9842 - val_loss: 5.4702 - val_accuracy: 0.9868\n",
      "Epoch 2/500\n",
      "141/171 [=======================>......] - ETA: 0s - loss: 4.3951 - accuracy: 0.9823\n",
      "Epoch 00002: val_loss improved from 5.47018 to 3.74458, saving model to wavelength_467\\Weights-002--3.74458.hdf5\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 4.3088 - accuracy: 0.9839 - val_loss: 3.7446 - val_accuracy: 0.9890\n",
      "Epoch 3/500\n",
      "139/171 [=======================>......] - ETA: 0s - loss: 4.3750 - accuracy: 0.9802\n",
      "Epoch 00003: val_loss improved from 3.74458 to 2.79134, saving model to wavelength_467\\Weights-003--2.79134.hdf5\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 4.2020 - accuracy: 0.9800 - val_loss: 2.7913 - val_accuracy: 0.9905\n",
      "Epoch 4/500\n",
      "130/171 [=====================>........] - ETA: 0s - loss: 3.5680 - accuracy: 0.9849\n",
      "Epoch 00004: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 3.9891 - accuracy: 0.9818 - val_loss: 6.9448 - val_accuracy: 0.9692\n",
      "Epoch 5/500\n",
      "140/171 [=======================>......] - ETA: 0s - loss: 4.2074 - accuracy: 0.9837\n",
      "Epoch 00005: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 3.9533 - accuracy: 0.9844 - val_loss: 4.0673 - val_accuracy: 0.9839\n",
      "Epoch 6/500\n",
      "142/171 [=======================>......] - ETA: 0s - loss: 3.9303 - accuracy: 0.9826 ETA: 0s - loss: 4.0980 - accuracy: 0.\n",
      "Epoch 00006: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 3.9242 - accuracy: 0.9829 - val_loss: 5.1412 - val_accuracy: 0.9897\n",
      "Epoch 7/500\n",
      "139/171 [=======================>......] - ETA: 0s - loss: 4.0668 - accuracy: 0.9827\n",
      "Epoch 00007: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 4.0083 - accuracy: 0.9826 - val_loss: 5.6494 - val_accuracy: 0.9751\n",
      "Epoch 8/500\n",
      "141/171 [=======================>......] - ETA: 0s - loss: 3.4023 - accuracy: 0.9856\n",
      "Epoch 00008: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 3.3646 - accuracy: 0.9846 - val_loss: 4.3906 - val_accuracy: 0.9846\n",
      "Epoch 9/500\n",
      "142/171 [=======================>......] - ETA: 0s - loss: 3.4671 - accuracy: 0.9855\n",
      "Epoch 00009: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 3.4439 - accuracy: 0.9857 - val_loss: 3.8129 - val_accuracy: 0.9868\n",
      "Epoch 10/500\n",
      "140/171 [=======================>......] - ETA: 0s - loss: 4.0563 - accuracy: 0.9826\n",
      "Epoch 00010: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 4.0501 - accuracy: 0.9817 - val_loss: 5.3740 - val_accuracy: 0.9875\n",
      "Epoch 11/500\n",
      "141/171 [=======================>......] - ETA: 0s - loss: 4.0651 - accuracy: 0.9860\n",
      "Epoch 00011: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 3.9934 - accuracy: 0.9840 - val_loss: 5.1130 - val_accuracy: 0.9868\n",
      "Epoch 12/500\n",
      "143/171 [========================>.....] - ETA: 0s - loss: 3.8114 - accuracy: 0.9854\n",
      "Epoch 00012: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 3.6992 - accuracy: 0.9864 - val_loss: 5.0546 - val_accuracy: 0.9853\n",
      "Epoch 13/500\n",
      "139/171 [=======================>......] - ETA: 0s - loss: 4.3822 - accuracy: 0.9822\n",
      "Epoch 00013: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 4.4219 - accuracy: 0.9828 - val_loss: 5.0801 - val_accuracy: 0.9802\n",
      "Epoch 14/500\n",
      "142/171 [=======================>......] - ETA: 0s - loss: 3.8339 - accuracy: 0.9833\n",
      "Epoch 00014: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 3.7392 - accuracy: 0.9831 - val_loss: 4.6489 - val_accuracy: 0.9817\n",
      "Epoch 15/500\n",
      "141/171 [=======================>......] - ETA: 0s - loss: 3.3370 - accuracy: 0.9854\n",
      "Epoch 00015: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 3.3870 - accuracy: 0.9853 - val_loss: 8.9767 - val_accuracy: 0.9787\n",
      "Epoch 16/500\n",
      "137/171 [=======================>......] - ETA: 0s - loss: 4.6903 - accuracy: 0.9797\n",
      "Epoch 00016: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 4.4421 - accuracy: 0.9815 - val_loss: 5.7632 - val_accuracy: 0.9809\n",
      "Epoch 17/500\n",
      "140/171 [=======================>......] - ETA: 0s - loss: 4.3264 - accuracy: 0.9810\n",
      "Epoch 00017: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 4.1596 - accuracy: 0.9820 - val_loss: 4.6210 - val_accuracy: 0.9802\n",
      "Epoch 18/500\n",
      "139/171 [=======================>......] - ETA: 0s - loss: 3.8639 - accuracy: 0.9820\n",
      "Epoch 00018: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 3.7688 - accuracy: 0.9826 - val_loss: 6.8489 - val_accuracy: 0.9802\n",
      "Epoch 19/500\n",
      "139/171 [=======================>......] - ETA: 0s - loss: 4.2899 - accuracy: 0.9854\n",
      "Epoch 00019: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 4.1784 - accuracy: 0.9846 - val_loss: 6.2058 - val_accuracy: 0.9809\n",
      "Epoch 20/500\n",
      "143/171 [========================>.....] - ETA: 0s - loss: 3.7285 - accuracy: 0.9812\n",
      "Epoch 00020: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 3.8389 - accuracy: 0.9824 - val_loss: 7.2783 - val_accuracy: 0.9751\n",
      "Epoch 21/500\n",
      "137/171 [=======================>......] - ETA: 0s - loss: 3.7793 - accuracy: 0.9822\n",
      "Epoch 00021: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 3.9979 - accuracy: 0.9826 - val_loss: 7.4609 - val_accuracy: 0.9787\n",
      "Epoch 22/500\n",
      "139/171 [=======================>......] - ETA: 0s - loss: 3.9003 - accuracy: 0.9836\n",
      "Epoch 00022: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 3.8053 - accuracy: 0.9840 - val_loss: 4.8622 - val_accuracy: 0.9824\n",
      "Epoch 23/500\n",
      "143/171 [========================>.....] - ETA: 0s - loss: 4.2012 - accuracy: 0.9823\n",
      "Epoch 00023: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 4.1001 - accuracy: 0.9829 - val_loss: 6.4498 - val_accuracy: 0.9809\n",
      "Epoch 24/500\n",
      "142/171 [=======================>......] - ETA: 0s - loss: 3.7946 - accuracy: 0.9842\n",
      "Epoch 00024: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 3.7113 - accuracy: 0.9837 - val_loss: 6.3295 - val_accuracy: 0.9795\n",
      "Epoch 25/500\n",
      "146/171 [========================>.....] - ETA: 0s - loss: 3.4733 - accuracy: 0.9842\n",
      "Epoch 00025: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 3.4588 - accuracy: 0.9842 - val_loss: 5.0404 - val_accuracy: 0.9802\n",
      "Epoch 26/500\n",
      "145/171 [========================>.....] - ETA: 0s - loss: 3.7534 - accuracy: 0.9845\n",
      "Epoch 00026: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 3.7577 - accuracy: 0.9848 - val_loss: 5.8237 - val_accuracy: 0.9831\n",
      "Epoch 27/500\n",
      "140/171 [=======================>......] - ETA: 0s - loss: 3.4709 - accuracy: 0.9824\n",
      "Epoch 00027: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 3.3844 - accuracy: 0.9835 - val_loss: 6.8207 - val_accuracy: 0.9795\n",
      "Epoch 28/500\n",
      "146/171 [========================>.....] - ETA: 0s - loss: 3.6146 - accuracy: 0.9869\n",
      "Epoch 00028: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 3.6114 - accuracy: 0.9870 - val_loss: 6.0599 - val_accuracy: 0.9861\n",
      "Epoch 29/500\n",
      "144/171 [========================>.....] - ETA: 0s - loss: 3.6795 - accuracy: 0.9822\n",
      "Epoch 00029: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 3.6616 - accuracy: 0.9826 - val_loss: 5.7686 - val_accuracy: 0.9831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/500\n",
      "143/171 [========================>.....] - ETA: 0s - loss: 3.7466 - accuracy: 0.9845\n",
      "Epoch 00030: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 3.7423 - accuracy: 0.9840 - val_loss: 6.1148 - val_accuracy: 0.9751\n",
      "Epoch 31/500\n",
      "142/171 [=======================>......] - ETA: 0s - loss: 4.3654 - accuracy: 0.9817\n",
      "Epoch 00031: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 4.2967 - accuracy: 0.9820 - val_loss: 9.9928 - val_accuracy: 0.9743\n",
      "Epoch 32/500\n",
      "146/171 [========================>.....] - ETA: 0s - loss: 3.9174 - accuracy: 0.9833\n",
      "Epoch 00032: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 3.9524 - accuracy: 0.9829 - val_loss: 7.5489 - val_accuracy: 0.9802\n",
      "Epoch 33/500\n",
      "142/171 [=======================>......] - ETA: 0s - loss: 4.1384 - accuracy: 0.9817\n",
      "Epoch 00033: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 3.9818 - accuracy: 0.9822 - val_loss: 6.6255 - val_accuracy: 0.9729\n",
      "Epoch 34/500\n",
      "144/171 [========================>.....] - ETA: 0s - loss: 3.5755 - accuracy: 0.9855\n",
      "Epoch 00034: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 3.4936 - accuracy: 0.9857 - val_loss: 7.9291 - val_accuracy: 0.9751\n",
      "Epoch 35/500\n",
      "142/171 [=======================>......] - ETA: 0s - loss: 3.5658 - accuracy: 0.9861\n",
      "Epoch 00035: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 3.5184 - accuracy: 0.9857 - val_loss: 6.0024 - val_accuracy: 0.9853\n",
      "Epoch 36/500\n",
      "145/171 [========================>.....] - ETA: 0s - loss: 3.7032 - accuracy: 0.9853\n",
      "Epoch 00036: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 3.6445 - accuracy: 0.9842 - val_loss: 6.0925 - val_accuracy: 0.9780\n",
      "Epoch 37/500\n",
      "141/171 [=======================>......] - ETA: 0s - loss: 3.8390 - accuracy: 0.9827\n",
      "Epoch 00037: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 3.6793 - accuracy: 0.9822 - val_loss: 6.6777 - val_accuracy: 0.9780\n",
      "Epoch 38/500\n",
      "140/171 [=======================>......] - ETA: 0s - loss: 3.6221 - accuracy: 0.9862\n",
      "Epoch 00038: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 3.5750 - accuracy: 0.9861 - val_loss: 8.8030 - val_accuracy: 0.9714\n",
      "Epoch 39/500\n",
      "142/171 [=======================>......] - ETA: 0s - loss: 3.8283 - accuracy: 0.9859\n",
      "Epoch 00039: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 3.7758 - accuracy: 0.9868 - val_loss: 5.9064 - val_accuracy: 0.9736\n",
      "Epoch 40/500\n",
      "139/171 [=======================>......] - ETA: 0s - loss: 3.0616 - accuracy: 0.9854\n",
      "Epoch 00040: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 3.2222 - accuracy: 0.9859 - val_loss: 6.6412 - val_accuracy: 0.9736\n",
      "Epoch 41/500\n",
      "132/171 [======================>.......] - ETA: 0s - loss: 3.6391 - accuracy: 0.9853\n",
      "Epoch 00041: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 3.5725 - accuracy: 0.9853 - val_loss: 8.8263 - val_accuracy: 0.9736\n",
      "Epoch 42/500\n",
      "140/171 [=======================>......] - ETA: 0s - loss: 3.6519 - accuracy: 0.9848\n",
      "Epoch 00042: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 3.5387 - accuracy: 0.9850 - val_loss: 7.2916 - val_accuracy: 0.9817\n",
      "Epoch 43/500\n",
      "144/171 [========================>.....] - ETA: 0s - loss: 3.4418 - accuracy: 0.9844\n",
      "Epoch 00043: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 3.4865 - accuracy: 0.9850 - val_loss: 7.2172 - val_accuracy: 0.9765\n",
      "Epoch 44/500\n",
      "141/171 [=======================>......] - ETA: 0s - loss: 3.6987 - accuracy: 0.9865\n",
      "Epoch 00044: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 3.6930 - accuracy: 0.9861 - val_loss: 8.1993 - val_accuracy: 0.9773\n",
      "Epoch 45/500\n",
      "139/171 [=======================>......] - ETA: 0s - loss: 3.3290 - accuracy: 0.9847\n",
      "Epoch 00045: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 3.2715 - accuracy: 0.9846 - val_loss: 8.1764 - val_accuracy: 0.9729\n",
      "Epoch 46/500\n",
      "136/171 [======================>.......] - ETA: 0s - loss: 4.2358 - accuracy: 0.9805\n",
      "Epoch 00046: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 4.2021 - accuracy: 0.9798 - val_loss: 7.7459 - val_accuracy: 0.9699\n",
      "Epoch 47/500\n",
      "143/171 [========================>.....] - ETA: 0s - loss: 3.2960 - accuracy: 0.9858\n",
      "Epoch 00047: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 3.2685 - accuracy: 0.9855 - val_loss: 7.3809 - val_accuracy: 0.9773\n",
      "Epoch 48/500\n",
      "141/171 [=======================>......] - ETA: 0s - loss: 3.5732 - accuracy: 0.9885\n",
      "Epoch 00048: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 3.5910 - accuracy: 0.9873 - val_loss: 7.5219 - val_accuracy: 0.9729\n",
      "Epoch 49/500\n",
      "138/171 [=======================>......] - ETA: 0s - loss: 3.4434 - accuracy: 0.9860\n",
      "Epoch 00049: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 3.4561 - accuracy: 0.9859 - val_loss: 8.1997 - val_accuracy: 0.9699\n",
      "Epoch 50/500\n",
      "139/171 [=======================>......] - ETA: 0s - loss: 3.2363 - accuracy: 0.9858\n",
      "Epoch 00050: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 3.3260 - accuracy: 0.9861 - val_loss: 9.0767 - val_accuracy: 0.9699\n",
      "Epoch 51/500\n",
      "133/171 [======================>.......] - ETA: 0s - loss: 3.7866 - accuracy: 0.9840\n",
      "Epoch 00051: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 3.6194 - accuracy: 0.9839 - val_loss: 9.2245 - val_accuracy: 0.9648\n",
      "Epoch 52/500\n",
      "142/171 [=======================>......] - ETA: 0s - loss: 3.3678 - accuracy: 0.9831 ETA: 0s - loss: 3.8630 - accuracy: 0.\n",
      "Epoch 00052: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 3.6025 - accuracy: 0.9824 - val_loss: 9.1318 - val_accuracy: 0.9677\n",
      "Epoch 53/500\n",
      "141/171 [=======================>......] - ETA: 0s - loss: 3.6723 - accuracy: 0.9840\n",
      "Epoch 00053: val_loss did not improve from 2.79134\n",
      "171/171 [==============================] - 0s 1ms/step - loss: 3.5868 - accuracy: 0.9842 - val_loss: 7.7767 - val_accuracy: 0.9795\n",
      "Epoch 00053: early stopping\n"
     ]
    }
   ],
   "source": [
    "history= model8.fit(X_train, Y_train, epochs=500, batch_size=32, validation_split = 0.2, callbacks=callback_list)\n",
    "#history= NN_model.fit(X_train, Y_train, epochs=7, batch_size=32, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model8.save('wavelength_467_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 4.351, Validation loss: 14.760\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAABYm0lEQVR4nO2dd3yV5d3/31f23gkjg4S9N4igOEDFrThp3VU7tHU8bX92Wm371D61S22tW2uruDcqCG5RCBB22IEEQhJIQnZOxvX74zp3cnJy9szJud6vF6+TnHGf6w7J5/6ez/UdQkqJRqPRaEKPiGAvQKPRaDSeoQVco9FoQhQt4BqNRhOiaAHXaDSaEEULuEaj0YQoUYF8s6ysLFlYWBjIt9RoNJqQZ8OGDceklNnW9wdUwAsLCykuLg7kW2o0Gk3II4Q4aOt+baFoNBpNiKIFXKPRaEIULeAajUYTogTUA7dFR0cHFRUVtLW1BXspg4a4uDjy8vKIjo4O9lI0Go0fCbqAV1RUkJycTGFhIUKIYC8n5JFScvz4cSoqKigqKgr2cjQajR9xaqEIIZ4WQlQLIbZZ3JchhFglhNhjvk33dAFtbW1kZmZq8fYRQggyMzP1JxqNJgxwxQN/Flhidd89wGop5Rhgtfl7j9Hi7Vv0z1OjCQ+cCriU8jOg1urui4HnzF8/B1zi22VpNIOI0veg4UiwV6EZhHiahTJESlkJYL7NsfdEIcStQohiIURxTU2Nh2/nP+rr6/nnP//p9uvOO+886uvrfb8gzeCiuwteuhbWPR7slWgGIX5PI5RSPi6lnC2lnJ2d3a8SNOjYE/Curi6Hr1uxYgVpaWl+WpVm0GBqAtkFTdXBXolmEOJpFkqVEGKYlLJSCDEMCNnfznvuuYd9+/Yxffp0oqOjSUpKYtiwYZSUlLBjxw4uueQSysvLaWtr44477uDWW28FetsCNDU1ce6553LKKafw1VdfkZuby1tvvUV8fHyQz0wzIGhvVLdawDV+wFMBfxu4HnjAfPuWLxZz3zvb2XGkwReH6mHi8BTuvXCS3ccfeOABtm3bRklJCZ988gnnn38+27Zt60nBe/rpp8nIyKC1tZU5c+Zw2WWXkZmZ2ecYe/bs4cUXX+SJJ57gyiuv5LXXXuOaa67x6XloQhRDwJsHnn2oCX1cSSN8EVgLjBNCVAghvoMS7rOEEHuAs8zfDwrmzp3bJ3/6oYceYtq0acybN4/y8nL27NnT7zVFRUVMnz4dgFmzZlFWVhag1WoGPD0Cfiy469AMSpxG4FLKZXYeWuTjtTiMlANFYmJiz9effPIJH330EWvXriUhIYHTTz/dZn51bGxsz9eRkZG0trYGZK2aEKDd/ImyuRqkBJ3iqfEhYd8LJTk5mcbGRpuPnThxgvT0dBISEigtLeXrr78O8Oo0IY8RgXeZesVco/ERQS+lDzaZmZksWLCAyZMnEx8fz5AhQ3oeW7JkCf/617+YOnUq48aNY968eUFcqSYkabcIDpqPQVxq8NaiGXSEvYADvPDCCzbvj42N5f3337f5mOFzZ2VlsW1bT5cBfvzjH/t8fZoQxlLAm6ohc1Tw1qIZdIS9haLR+JU+EbjORNH4Fi3gGo0/abPwvbWAa3yMFnCNxp+0N0CiuQJZC7jGx2gPXKPxJ+2NEJ+ueqJoAdf4GC3gGo0/aW+E2GRAaAHX+Bwt4BqNPzEEPDoBmrSAa3xL2Hvgp59+Oh9++GGf+/72t7/xgx/8wO7zi4uLAfstZX/zm9/w4IMPOnzfN998kx07dvR8/+tf/5qPPvrIzdVrBjyGgCdm6Qhc43PCXsCXLVvG8uXL+9y3fPlyli2z10GgF29ayloL+P3338/ixYs9OpZmANPeCLEpaiNTC7jGx4S9gF9++eW8++67tLe3A6pA58iRI7zwwgvMnj2bSZMmce+999p8bWFhIceOqSZFv//97xk3bhyLFy9m165dPc954oknmDNnDtOmTeOyyy6jpaWFr776irfffpuf/OQnTJ8+nX379nHDDTfw6quvArB69WpmzJjBlClTuOmmm3rWVlhYyL333svMmTOZMmUKpaWl/vzRaHxBj4DnQFs9dJqCvSLNIGJgeeDv3wNHt/r2mEOnwLn2myVmZmYyd+5cPvjgAy6++GKWL1/OVVddxc9+9jMyMjLo6upi0aJFbNmyhalTp9o8xoYNG1i+fDmbNm2is7OTmTNnMmvWLACWLl3KLbfcAsAvf/lLnnrqKX74wx9y0UUXccEFF3D55Zf3OVZbWxs33HADq1evZuzYsVx33XU8+uij3HnnnYCq/Ny4cSP//Oc/efDBB3nyySd98EPS+AUpVRqhYaEAtByDlOHBXZdm0BD2ETj0tVEM++Tll19m5syZzJgxg+3bt/exO6z5/PPPufTSS0lISCAlJYWLLrqo57Ft27Zx6qmnMmXKFP773/+yfft2h2vZtWsXRUVFjB07FoDrr7+ezz77rOfxpUuXArptbUhgagakWcB1LrjG9wysCNxBpOxPLrnkEu6++242btxIa2sr6enpPPjgg6xfv5709HRuuOEGm21kLbE3Cf6GG27gzTffZNq0aTz77LN88sknDo8jpXT4uNG6NjIyks7OTofP1QQZo4w+NhmSzGNjdSZK6CMlHPxS/b8OmxbUpegIHEhKSuL000/npptuYtmyZTQ0NJCYmEhqaipVVVV2G1oZLFy4kDfeeIPW1lYaGxt55513eh5rbGxk2LBhdHR08N///rfnfnttbMePH09ZWRl79+4F4Pnnn+e0007z0ZlqAoqlgBsWio7AQ5uWWnjlBnj2fHhsITy9BHa8BV3BCaYGVgQeRJYtW8bSpUtZvnw548ePZ8aMGUyaNImRI0eyYMECh6+dOXMmV111FdOnT2fEiBGceuqpPY/99re/5aSTTmLEiBFMmTKlR7SvvvpqbrnlFh566KGezUuAuLg4nnnmGa644go6OzuZM2cO3/ve9/xz0hr/0iPgKdpCGQzs+Qjeug1ajsOZv4KoOFj3GLx8HaTmw9xbYea1qvI2QAhnH9l9yezZs6WRQ22wc+dOJkyYELA1hAv65zoA2LcGnr8UbvwACubB74fC3Fvg7N8Fe2UadzA1w8pfQfFTkD0Blj4Ow8wJDd1dsGsFfP0vOPgFRCfCdW9B/hyfLkEIsUFKOdv6fq8sFCHEHUKIbUKI7UKIO705lkYz6LC0UIRQqYR6NmZocaQE/nUqFD8NJ98Ot37SK94AEZEw4UK48T347ufQ3QGl7wZseR5bKEKIycAtwFzABHwghHhPStl/6q9GE45YCjgoH7ypOnjr0bjPmz+Ajha4/h0oOtXxc4dNhYxRcGx3YNaGdxH4BOBrKWWLlLIT+BS41JMDBdLGCQf0z3OA0E/AdTVmSNF8HKq3K9vLmXgbZI0JGQHfBiwUQmQKIRKA84B86ycJIW4VQhQLIYpravr/8sbFxXH8+HEtOj5CSsnx48eJi4vr/2D5etj2euAXFa5YC3hStrZQQolDX6nbEY6TGPqQNRZqDwSs4tZjC0VKuVMI8UdgFdAEbAb65dJIKR8HHge1iWn9eF5eHhUVFdgSd41nxMXFkZeX1/+Br/8J5d/A5KWBX1Q40t4AUfEQGa2+NyJwKZUnrhnYHPxKZZoMn+n6a7LGguyC2v2QM95/azPjVRqhlPIp4CkAIcT/AhXuHiM6OpqioiJvlqFxFVMTtNYHexXhQ08vcDOJ2WqTq60+oKlmGg85+CXkzYGoGNdfk60qqDm2OyAC7m0WSo75tgBYCrzoi0Vp/ER7E3Q0Q1dHsFcSHvQTcHM1prZRBj5tJ1RfJnfsE4DMMeo2QD64t5WYrwkhdgDvALdJKet8sCaNvzCZPVkdhQeG9kaIS+n93qjGHKyZKM3HYGfgUuj8yqFvQHbDiPnuvS42CVJyQ0PApZSnSiknSimnSSlX+2pRGj9hala3bfVBXUbYYMtCgYGXiSIlvHoTHPjcu+NseBZe+rbK3gh1Dn4JEdHKQnGXrLGhIeCaEKO9Sd3qCDwwGL3ADYyGVgNNwFuOw7bXYK+XE6Eajqjb2v3erynYHPwKcmdCTIL7r80aC8f2qAujn9ECHk6YzAKuI/DAYPQCN4jPYEAON26sVLettd4dp6lK3Ya6gJua4chG9+0Tg6wx6m/N+Ln6ES3g4UJ3l6ooAx2BBwprCyUyChIyBqCAm4W3xUsBbzyqbkNdwCvWQ3en+xuYBlnmTJSaXY6f5wO0gIcLhv8NOgIPBFJCm1UEDuZ+KANNwI0I3MscBCMCrzvg3XGCzcGvQERA/kmevT57nLo95v+uIlrAwwVLAdcRuP/paFUFHf0EPGvgDXUwImdvInApB4+FcvArGDq1bwaROyQNUXsfAdjI1AIeLhj+N+gIPBBYl9EbDMR+KE1mAfcmAm+tgy6TilxrQzgC72xXFoqn9gmoKtusMXBMWygaX9FuMf1HR+D+x3KYgyVJA7ClrBGBt9Z6njlhRN9Dp6jBzW0nfLO2QHN4I3S2eb6BaZA1TlsoGh+iPfDA0t6gbm1ZKO0noMPxjNWAYnjgXaa+vyduHcN8ESg4Wd2GahR+8Et1a5yHp2SNUT/Xtgbv1+QALeDhgmGhRMXrCDwQOLJQQEWpA4XGKhCR6mtPUwmNCNwQvlDdyDz4lZq6k5jp3XGMTBQ/R+FawMMFI7JKzdMReCCwK+ADrJinu1t54Jmj1feebmT2i8BDcCOzq1N16yz0wv826MlE8e9GphbwcMEQlNQ8HYEHAmcR+EDJRGk5rnKec8zzU72JwKMTIXmIysIIRQvl6Gb1SdVb/xsgvRAiorSAa3yEYaGk5uoIPBD0CHhq3/uNhlYDJQI3MlByJqpbbyLw5CHq6/Si0BTwg+YBDgU+EPDIaMgYqQVc4yMMCyUlT4m5binrX3o2MZP63j/Q+qEY1scQs4B7mkrYVAVJQ9XXGSND0wM/+JVae8ow3xwvAE2ttICHC+2NEJ2gSrkhdNO8QoX2RoiMhajYvvfHJKr/hwEj4OYMlBwvBdwyAs8ogobDqpgpVOjuVgLuTf63NVlj1V6AH4MlLeDhgqkJYpIgLk19r31w/2LdB8WSxKwBJODmCDw1D2KSPbdQrCNwgLoyr5cXMKp3KGvR1wLe3enXn4MW8HDB1Kyiv/g09b32wf2LdSdCSxJzBs5Qh8ajqktiVCwkpHu2idnepAIEywgcPPfBaw9ARTGcOKwyQwKB4X/7YgPTIABNrbyaiakJIdqblB+rI/DA4DACz4YTbo+P9Q+NRyHZHDnHZ3gWgRs54EYEnm4IuAephFLC00t6N1dFhMpqSRkOqflw5i9VkYyv2f+JOn5age+OmeX/8WpawMMFw0LREXhgsB7mYEliFhzZFNj12KOxslfAEzI8i8ANG8aIwBMyVKDgyUZm3QEl3nO/q3KpGyvVoIiGI7B3tVrf9e+4f1xHmFpg3xqYcY3qY+Ir4lIgeZhfi3m0gIcLpiZIyLKIwPX4Ur/S3qAyfmyRZG4p290NEUF2MZuqenPA49M982uNaNmIwEHZKJ5E4BXF6nbmtaqviiVr/wEf/hwOroURXpa6W7JvDXS2wvjzfXdMAz83tfJ2Kv1dQojtQohtQogXhRBxvlqYxscYFoqOwAODMwtFdgX//6C7W0XPSebI2VMLxRgIkWwp4CM9FPD1qiAoe0L/x2bdqPYPPn3A/eM6ovRdiEuFwlN8e1zw+3g1jwVcCJEL/AiYLaWcDEQCV/tqYRofY2xiRsXqfiiBwJmAQ/AzUVqOqQtJsjnvOSFDpZd2d7l3nKajEBmjIniD9CKoL3c/ha6iWM2ijLRhDsQkwIIfKb/60DfuHdceXZ2w630Yu0QV3/iarHHq05ixT+BjvP38FgXECyGigATgiPdL0vgFU5NKEwMVhQc7+hvsOEsjhOBnovR41xabmEj3L+6NVSqKt/SPM0aqi0P9IdeP09EGR7dC7iz7z5l9k7ICfRWFH/xS/S2Mv8A3x7PG2Mj0UyaKxwIupTwMPAgcAiqBE1LKldbPE0LcKoQoFkIU19QMkNzXcENKs4Anqu/j0nQE7k8621VrVkdphBD8CNxawI0iL3c3MpssbBiDnlxwNzYyj26B7g7Im2P/OTGJMP+HyrcuX+/eOm1R+h5ExcHoRd4fyxY9XQn9k4nijYWSDlwMFAHDgUQhxDXWz5NSPi6lnC2lnJ2dne35SjWe09EKsru3rDs+TVdi+hN7wxwMeiwUN1rKlq+HF67ybVWfUYXZJwLHfR+8saqv/w2e5YJXmAU5b7bj5825GRIy4dM/un5sW0ipBHzUmb3Bja9JGa6yv/yUieKNhbIYOCClrJFSdgCvAz7Mgtf4DKORVYxZwHUE7l+MPij2ZiomZKj85mY3LJSdb8PuD9yzJJzRk79tbGKaPWx3M5RsReBJQ1TLALcEvFjlYltfDKyJTYKTb4e9q6Big3trtaSyBBoq/GefgMV4tQEWgaOsk3lCiAQhhAAWATt9syyNT7EWcO2B+xd7rWQNIiJVBOmOhWJ4qA2HvVubJY2VvVWYoCoxwT0LpbNdCb616Aph7kroRiZKRbHz6Ntg7i3qguNNFL7zXXUhHbvE82O4gh+bWnnjgX8DvApsBLaaj/W4j9al8SXtZgGP1RF4QHAm4GAebuyGhVJTqm4bfJgn0Hi0NwMFPLNQrKN4S9zJBW+sghOHINdFAY9NhpNvgz0fel4UVfqe6n3i7fQdZ2SNURdey7m0PsKrLBQp5b1SyvFSyslSymullO2+WpjGhxitZC0jcFNj4PpMhBsuCXiW61kopuZe68SnEfjRvpFzXKoareZOBG4rB9wgY6QqDOrudn6cw+YCHkcbmNbM/a4KRj79P9dfY3B8H9Ts9E/xjjVZ5uk8x/f6/NC6mVU4YMsDB72R6S+MQbb2NjFBZaK4aqEc2wOYC0FO+FrALSJwIZQt4VYEblRh2onAu9qh0YVPDRXr1QSbYVNdf++4FBWF71rhfhRe+q66DYSAZ49XIu7pwGgHaAEPB3oiQosIHLQP7i/sTaS3xB0LxfC/oxN9Z6F0dyn7I9lKeN3th2KdimiJkUroykZmRbEqnY+Od/29AU76rsoLf/8e96odd74LQ6f6tnmVPbLHwu3r/FLpqQU8HOixUCzywEH74P7CVQvF1Oja0INju1R0mj/XdxZKy/G+VZgGbkfgVWojMNFGirCrXQm7u1QE7ar/bUlcKiy+F8q/hq2vuvaaxqMq4p9wofvvN8DQAh4O2MpCAWjTDa38QnujEtwoB62B3BmtVrMLMkapQbm+EnDrHHCD+Az3LuyNR5V4R0T2fyw1DyKinQt4Tan6HXXH/7Zk+jUwbDqs+nXvhr0jdq0ApH/TBwOEFvBwoN2OB64jcP9glNE7ak3qTj+UmlLVWjU1V0XOHW3er9GwPpKsBNxdC6Wpyrb/DUrU00c4r8Z0tYDHHhERcO7/Ka/9i784f37pe+rTQY6NhlkhhhbwcMDUpJoNRcWo77UH7l8c9UExMAS8yYmAd7arCDZ7HKTkqvtc2RR0ht0I3E0LxTqTxRpXuhJWrFeRv+GZe0LBSTD1KvjqYcfv13YC9n8KEy7wbe/vIKEFPBwwhjkY6Ajcvzga5mBgiLGz/tvH96o2CNnjVVk2+CYTpdFO/nZChuqN7epAYkcROJgFvMzxBmPFBhV9eyuoi+9Tls2Hv7T/nI3Pq34rg8A+AS3g4YGpua+AR8cpfzbYEbifeiQHHUfzMA1ShikRL3fSFtUo4Mke1zsgwheZKI2VqhrU+FRm4E4xT3eXsoAcReDpRWqz1l7GTVuDOkdPNjCtSRkGp/0Edr2npvdY0lQDL18HK38BBSd77rcPMLSAhwPtjb0phAbBrMbsNMHbP4R/njw4RdwVCwUg/yQXBHyXyvLIHK0ECnyzkWmdA27gTkfC5hr16cBZBA72bY0jGwHpuf9tzbwfqPf84B7V+EtKlZ3yj7mq7/eie+H6d21vuoYgWsDDAWsLBYLXD6WlFv6zFDb+W1XC+aG4Iei4KuAFJysxri+3/5yaXSr7JDpepYHGpflGwJvseNfuNLRylANuYHQltLeRaWxgOuoB7g5RsXDOH1TvkU/+AC9dA699R4n6dz+HU++2PSwiRNECHg4Y03gsCUYEXrsfnjpLRZ3jzlP3tbjRDyRUcMUDB7XxBo6j8Jpdyv82SM3zkYVytH8GCrhnoVhPo7dFWoH6BGEvAq/YoJo9GRvrvmDsOTB6MXz+Z9izCs76LXxnJeSMd/7aEEMLeDhgzMO0JNAR+KGv4YlFKg3uurdg5vXq/ubjgVtDoHA1As+ZpD4ZHVpr+/GuDrWJmT2u976U4d5H4D1VmDaE1x0LxXoavS2iYpV3b6saU0oVgfvajxYCzv+Lmt7z/S/VGLZBYplYowU8HDA1945TM4hLg9YA9ULZ+io8d6H6eH7zahgxv3es2GCLwLs6VBaHKxF4ZJTyfu3Nd6w9oDImLCPwlOHeZ6E0H1PetU0LxZ0I3NyMy5EHDspGqSntP4yirkz9//vKPrEkfQRc8NfekWaDFC3g4YCpsb+FEqgIfP8nyoPMmwM3fwSZo9T9CeYWnu60VA0FXCmjtyR/HlRv722AZYmRgWKM5QIVzbYc866Yx14OOKgMpegE1zzwpqPqomz0E7fH0ClqXNqfx8G7d8PBr1SHwsPmYQyDJCMkGAweN19jH1sWSlyaSnfr7vLvx8u9q1UR0TWv9W1UNFgjcFcaWVlScJKKhivW95/LaDSx6iPg5lzwxsreDUJ36bE+bGShgIrCXYnA7fno1iz+jfrUtfVVKHkBip9SF6L4NHWxyJno6so1VugIfLDTaVIfw21F4OD/lrKVJeoP1LrLXEwSRMbqCDxvjtrks7WRWVMKqQV9L76GgHvjgzc5yR6JT3fNA7fVzdAWkdGqbesVz8BP9sLSJ2DIJHV+IxYMqqyQQKN/coOdnkZWNjxwUB+VjY0rXyMlVG6GSZf2f0wIFYW3DLJNTHcFPDZZidmhr/s/VrOr7wYmqCwU8C4TxYjAE3NsP56Q7mIaYZWKrN0hNgmmXqn+tdYrcdd4jI7ABzvWvcANAtEPpe6AivCHTbf9eELmII7AXdjENMifp/phW05I6u6C43v6C3iyD4p5GitVD23rKkwDVywUKc255C5E4PaIT/PfNPgwwWMBF0KME0KUWPxrEELc6cO1aXyBdS9wg0D0QzlSom6HT7f9eGLWIPTA3YzAAQrmQUczVG3rva/+IHS29c1AAXUhjkv1LhOlscq+/w2udSRsrYMuk2seuMZveDPUeJeUcrqUcjowC2gB3vDVwjQ+wp6FEogIvLJENReyt0mVkDUII3A3NzFBCTj09cGNDUxrAQe1AeiVhVLpuHoyPkMJtKNZlkYRjzcRuMZrfGWhLAL2SSkP+uh4Gl/RI+BBisCHTLSfZqY9cEVqnhJly4KeniZWY/s/39tinkYn1kdChsqMaXewwW2vn7gmoPhKwK8GXrT1gBDiViFEsRCiuKbGxSGuGt9hDHMItAdubGDa879BeeCmJt8MKBgotDcCwn1vt+AkVdBjNPeq2aVsjrjU/s/1RsC7u6C52rGFYvRDceSD90TgWsCDidcCLoSIAS4CXrH1uJTycSnlbCnl7OxsG3PzAoWU8NrNUPZl8NYQDKzHqRlEx6s0Pn9F4PUH1cXBnv8NgzMX3OiD4m5v6/x5alDDCXNjK2MKjy1S81QnwM5299dndBB0ZqGA49+NnghcWyjBxBcR+LnARilllQ+O5T/aG2HrK7D3o2CvJLD0bGIm9X/Mn9WYxgamwwjcLOCDyQdvb4Q4NzJQDIzGVoe+Ud5zzW7b/jf0LeZxF+M1jqwPV/qhNFWp3ynrT3aagOILAV+GHftkQGFEou7M+xsM2EsjBM87EjYedd7Hu7JEDfZ1VGU3KCNwF4Y52MJobFX+NTRUqKwUexG4N5N5nFVhgmv9UBqP6uh7AOCVgAshEoCzgNd9sxw/YnjB7sz7GwyYmlWln60J6Z5E4G0n4O/TYP2Tjp93pEQNjY12MJm9JwIP8kZmexPs+sA3x2rzUMAtG1vV7Fb32Y3AvSjmcaWHt6sRuPa/g45XAi6lbJFSZkopA9TWzguMSNSVCrPBhKlJpRDa8mQ9icBPVKj85C0v2X+OlCoCd2SfACSaG1oFOwIvfhpevAqqtnt/LFdbydrCaGxlDDmwK+BeFPM0HgUEJNmpwgTzxqnQEXgIED6VmCazgIdbBG6rkZWBJxG4kX1QsV6JuS3qD6kLpaMNTFAXkIio4HvghmD6Yn/EGwE3GluVvKCm1ttrcRCbDLGpHgp4pbKuHJWwR0QqEdcR+IAnfAS8PUw9cFOT/ZQ2T3qCN1rsVe94y/ZzKkvU7bAZjo8lhEolDHYEbrQ1DbaAG42tThyCLDv+t0FqrmcWiqvCm+CgnL69Sf1e6Qg86ISPgJssPPDBOEjXHrbmYRrEp6lije4u149nROAZo2D7m7afU7lZRdZDJjk/XkJWcD3whkoVycanw8G1vRd6T3F1nJotYpNhyGT1tb0NTANPc8EbK10rvjGqMW2hc8AHDOEj4IYH3tUOHS3BXUsgMTXbt1CMakx3Wsoa6WPTl0HFOts2ypESyHaygWmQGOQI/HCxuj35dtV2t+wLz4/V3aWyRzyNwKG3rN6e/23gyWSetgZVIJQ52vlzHfVD0TngA4bwEXCTRWQVTj54u5MIHNzzwZuq1B/uRHOL2B1v933c2MAcPs214wW7H8rhDapfy9xbITrROxvFkzJ6awpOVrc5Exw/LyVPVVR2mlw/9s631Qb05MucPzc+A1rsRODVO9RteqHr763xC+Ej4MYfF4SXD25qtC/gnkTgjWYBzxqtPu7veLPv4ycqVH8TZxkoBsHuSFhRDEMnq+KbooXBF/CJF8PVL0DhKY6f50kxz+blyvrKm+38uY4i8D0rIWOk5xOBND4jjAQ8TCNwU7P9TUwjAncnldCyB/TES1QHPcuP8j0bmNNdO15ClrqAWA+8DQTdXXBkU+9Q3dGLVA/z4/s8O54vBDwiUk2vcVaK3zOZx8WNzPpyKPscpl7lWpl/fIb61God4Xe0woHPYMzZrr2vxq+Ej4BbWijhFIE7SiPsicDrXT9eU3Wv9znpEnW708JGOVICIlJFta7QkwsehI3Mml3q9yLXHJEaMyn3rfHseL4QcFfpmczjog++9WV1O/VK157fc3G3+lsp+0LZMFrABwThI+Dtja6VCA8mujqhs7V/L3ADdyNwU4sqFTcEPGuMKgG3zEapLFEbcNYzMO3haj+U5mO+/3/rmYpuFvCMkeqfpzaKJ9N4PMWd2ZhSwuaXlL/uqu3RU41p5YPvWakGEY9Y4PpaNX4jfATc1ARp+errcKnG7LAzjcfA3QjcSB+zzD6YdIm5f8cRJRRHSpwX8Fjiaj+UF5fBXyfDp39SFxJfcLhYFaxkjOq9b/RiZRF40unPk2EOnhKbrC4UrlgoRzbBsV3KPnEVW8GOlLD7Qyg6zbUMI43fCR8Bb29Sub4xyeETgdvrBW4QHQ+RMa5H4LamsEy8RN3ufEeJScsx1/1vcC0Cl1KVuUfHw8e/g0dmw5aXHU+McYWKDTB8JkRY/BmMWqTSTC2HK7hKICNwgJRc+9Wwlmx5Sf0/G5aXK9jqh3Jsj2oTPFbbJwOF8BFwo6AlPj18PHBHrWRBbWbFpXkXgWePVTnf29/s3cD0KAJ34IE316hPE6f9FG5YocrMX78Fnlykim88wdSs0uGsMzIKT1Fit3e1+8cMZAQO5mIeJxF4VwdsfRXGnds7qMEVbEXge1aq29FnubdOjd8IHwE3KuQS0sMnAjf6v9gTcFA+uKsRuFFGb13JN+kSFbHu/kCVgg9xcQMTzKIiHEfgdWXqNr0QChfALR/DJf9SKXTPLIFN/3X9/QwqN4Ps6t3ANIhNUl6xRwLuws/bl7gi4HtXq09FU69279i2IvA9H6r2wIYVqQk6YSbgSeYS4TARcGcWCrgfgYtI1b/EkomXAFIJafZ4iElwfY0Rkea+Gw4EvPaAuk03b8BFRKhK0B9ugNQCJSzuUmGuwDRSCC0ZvUh1BXS310h7o7LoIgL0Z5Wap/5PHBXzbFmufudHL3bv2NEJ6pOIEey0NahPO2N09D2QCA8Bl7LXQnHUpGewYXKyiQnuReBNR1UbUmuByhmvhFt2wTAXKzAtcVaNWVcGCEgr6Ht/TKJKVzQmuLvD4WJ1vCQbY/4MsXM3Cvd0mIOnpAwHpPp/sUVrPZSuUJWXUTHuHVuIvsHOgU9Vq4Ex53izYo2PCQ8B72yH7s7wi8B75mE6EBW3IvBq+32kjc1MdzYwDZxNp687oMTKVuZD9ng4vtf9QqDDG/vbJwY5E9XEmn3uCrgXnQg9wVkxz463VO+facs8O35CRu/FffeHqoVt/lzPjqXxC+Eh4JZClpBhrvzrDO6aAoHJBQvFLQ/8qP1OdtOuVmLq7kd1UJaMswjcXt+N7PHq4uxO9WRjlRoebK+kXAhlo+z72L3fk4ALuLmYx14mypaXVOOq3JmeHT8+o7d7555VMOoMx33ENQEnPATcci6ksbvur2G+AwnDA3dkocSlqQuaKyl5jiLwjCK47RvVI8VdnPVDcSjg5rarNaWuv5/RgdBeBA4qnbCtHo5sdP24AykCrzsIB79Um5eulM7bIsGcsXV0q7JpdPXlgCM8BLwnAk/q3V0PBx/cOO9oJx44sjcFzh7dXar7nT96QCdkqf8PW33JO1pVtkm6nQrCrLGAcM8HryhW/cqHTbX/nJGnq4waV6syO00qEo5LdX0d3hKXoj5VWgt4dzesf0J97WrpvC2MCNxIH9QbmAMOb4capwkhXhVClAohdgohTvbVwnyKZY+KeBvpUYMVU7MSb0dZEa5WY7YcV+O+/NEDOjELkLYrZOsOqlt7EXhMAqSPgJqdrr/f4Q1q2ISjcv+EDBWh71nl2jHXPqIuNNO/7fo6fEFqrppiD+rn99Uj8PAM+OphGLtE/Ww8xaiZ2LMShs9wPEdTExS8jcD/DnwgpRwPTAPc+CsKID3pdMnqYyGERwRupE46wtV+KP5s4m+kJdrywY0ccEc9PLLHux6Bd3ebOxC60FJ10iXKQtnyiuPn1ZXBp/8HEy4MfJViynCo3glv/wj+PAFW/kJtwF7+NFz5vHfHTshQ+wvl32j7ZIAS5ekLhRApwELgBgAppQlwo7t8ALEsaImKVV+HRQTuYJiDgasReFO1uvVbBI5tH7zOyAEvtP/67PEq5a+rEyKd/Eof263sIld6Ys/9rmoR8O5daiMwc1T/50gJK36i8tmX/NH5MX1NSq7qnnjiMEy9Aubc4tgacgfj0yro9MEBijcR+EigBnhGCLFJCPGkEKKf2SqEuFUIUSyEKK6pqfHi7bzAsqDFXpe1wYijXuAGrkbgRq5xsj8icAf9UOrKzHsXmf0fM8ger3KUa/c7f6/DDgp4rImMgqVPKHF+7Tu2C2Z2vqMshtN/puyMQLPgTrjgb3D3DrjoYd+JN/T+rSRkKQtFM+DwRsCjgJnAo1LKGUAzcI/1k6SUj0spZ0spZ2dn2yiaCASWm5ixKWoDKywslCbnWREuR+A2+qD4CkcReO0BtYHpKJPCnUyUwxtUPnPmGNfWlpYPF/9D2S6r7+v7WHsjfHAPDJkCJ33PteP5mqzRMPvGXrH1JUYEPnpx4KpLNW7hzf9KBVAhpfzG/P2rKEEfeLRbCLgQ4dPQytTkuwi8sUoJn6t9vt2hxwO3UcxTV+Z8I65HwF3wwSuKIXeGe4I04QKYc7PaqLTc1PzkAZUBcsFfnVs3oUj6CBXsuNPFUBNQPBZwKeVRoFwIYf7rYRGwwyer8jXtDRAV3/tHZqRHDXZc8cCjE9RQX1cicH9lIURGq/Q76wi8u1u1L3U2hCAmUZXFO8tEMbWotrSubGBac/bvVZOuN74LDZUqN/rrR2HW9ZA/x/3jhQIpw+En+1QnQ82AxNuw4YfAf4UQMcB+4Ebvl+QHTFZWQkJGeHjgjsapGQjhWjWmMY3eX9jqh9J0VI3vcmX6uSuZKJUl5g6ELvjf1kTHqcyOx0+HN25V+enx6bDoXvePFUoYn9A0AxKvBFxKWQJ4EM4EGGshi8/oTU8bzJiaXWtt6ko/lKYq/25k2arGtGwj64zs8bD/U8eZKPs/UcU5IzwsV8geB+f+Ed7+ofr+0sf84z1rNC4yCI07G1hbCQnp7pVIhyKWHRidkZjVm+dtj8YARODWF1XrNrKOyB6vGjfVldkv59+3RkXf7gw2sGbGtco+aTnu3ogyjcYPhMfWsnU2hmWTnsFKRwsgnVsooLrvVW23//Nob1ITcfwp4ImZtiNwEQGpLgwQyB6vbu1lorTWqwyUkWd4s0plOZ33J2WneNpjRKPxEeEh4KZGqwg8Q0VrHR4Mx+00Qc1u363NX7jSyMpg6BS10Vt/0Pbj/kwhNEgwt5S1vIjUlamOe670ss4eq27tbWQe+Ey1Ahh1ptdL1WgGCuEh4LY8cPAsE+Xj38G/FqgOfgMZV3qBGww1F38c3Wr7cVvDjH1NYpYq27b04usOQEaha6+PTVaRur2NzH1r1M/ClQpMjSZECBMBtxGBg/u54KYW2PAcdJk8mwITSExuROA5E5RVYU/A/dkHxaCnGtMiF9xRG1lbZI+zb6Hs/xiKFup+1ppBRXgIuHUaoacR+LbXeiPE6oHZt6sHV+ZhGsQkqMb/diNwow+KH1rJGiSai3kMH7y9SU2jd2UD0yB7PBzb078tbe1+dTEY5aX/rdEMMAa/gHd3Ka/bOg8c3IvApYR1j0P2BNWidaALeM88TBcHDAydAke32X6s6aiqyPMme8MZ1v1Q3EkhNMger/LGrbNZ9q1Rt9r/1gwyQkPAD2+Era969lrLPigGnkTgFevh6BaYe4vaMHOn/3Qw6OnA6IKFAkrATxyyXeDUVK3sE3/2w7Duh+KpgEN/e2vfx6pSM2OkNyvUaAYcoSHgm1+Ed+/27LW2rAQjknSnGnPd46oR1tSrVNrdQI/A3bFQQAk42I7CG4/6v5l/vwjcnAPurIzekp6eKBb/N12dKgNl5Bk67U8z6AgNAU/Nh/YTrg/ftcRWBB4Vo6wFVyPwpmrY/qaa7h2bpCK9pqqB3U+lx0JxNQJ3kInSVO1f/xtUqXpMUu90+roy1R/FHdsmLkX1x7aMwA9vUCmS2j7RDEJCQ8DTCtTtiXL3X2s5jceSBDc6Em58TvWbnnOz+j5ngrp1Z5CuNZ/8EXa85fnrneFOGiGoCDsxx46AByACh77T6evK3NvANLDORNn/scqwKVrokyVqNAOJEBFwcyVe/SH3X2sM67UuKXe1I2FXJxQ/o4bcGsUihoBXe9h8sbsLPn8QXv0OHPras2M4o70RouLca3M6dApUWQl4V6cSVX8MM7bGsh9K7QH3/G+D7Amq0Kq7W32/b43q4aJ7lmgGISEi4OZ+0PUeROAmexF4hmsR+K4V0HAY5t7ae19KrvLDqz2MwBuOqFxy2QUvXaOmmfsaV6bxWDN0ijony8kzzTWADFAEbu5I2N2lLtYeCfg46GxVVaWt9ar/t7ZPNIOU0BDwhEzVt9qjCNzOZp6rEfi6x5UHP3ZJ731COC4acYaxQbfkAehog+XfUkVCvsTVRlaWDJ2irKJjFh5yTxl9oCLw4+qC2d3h3gamgWUmStnn6iKpBVwzSAkNARdCiai9Xh2OsOcFuxKBV5cqEZh9o5qLaEnOBM8tFKPL3thz4LInoXILvH27b5trmZqdj1OzxtZGZiD6oBgYHnhPF8JC949hmYmy72N1EcsbpAMXNGFPaAg4qI1MjzYxzfnQ/SLwdNXPpKvT/mvXPwmRMTDz+v6PZU9Q0WKTB4Oa6w6owpiUPBi3BBb9SlV5fvFX949lj/ZG9y2UzFFqcpFlKmEg+qAYJGapJmNV29X3nmxixqdB8jAVge9bA4Wn6vJ5zaAltATcEwvF1AQiUm3oWWIU89gbZNDWoPLPJy3tLTKxpCcTxYN88NoD6nyMDcZT7obJl8Hq+2HXB+4fzxaeWCgRkTBkoipYMmg0C3higDxwUJPjI6LUXoMnZI9T4l13QNsnmkFNCAl4viq8MSJqV2lvVNG3dRGHkZVgzwffs1KJ4Gw7U+J6MlE8EPC6A32jSyHgokdg2FR47WbY9B/nAxac4ckmJphL6rf22jlNVWpiT3Scw5f5BONCWbFeWWaeDgrOntD7yUELuGYQ49VEHiFEGdAIdAGdUkr/9eo0csHry1WU6CrtTbZzoY0I3J4PfmQTRMban5+YNEQJm7sCLiXUlvX3ZWMS4OoX4NkL4K3b1H05k1QDplFnwoj5aqPzxCH1MzhRoSylLhOc9Vv1ekush1i4ytApsOFZdfy0fHMOeADsE+iNwOsPeSe8hg+emq9sIY1mkOKLkWpnSCmPOX+al/SkEh5yT8BNjbbLyRPMFX72IvDKzTB0sn3/VAhVUu9uJkprnaoqteXvpubBDzdC1TZlAexbo7Jg1j5i+1jRCapRV3oRzL+972OeWCjQdyMzLV9VYQbC/4bejoTg2QamgZGJMkqXz2sGN6EzE9MYq+XuRqa9SNRRBN7drQR8yhWOj50zXm0+Sum6UNQ66fEREaGslGFT4ZQ7VdR98CuoWKc2XlPz1M8irUB9/++L4Mu/w5zvQHS8OkbPPEwPLJSciYBQF5Hx5ykrJ3+u+8fxhASLvQZPNjANhk6BYdNV6wONZhDjrYBLYKUQQgKPSSkft36CEOJW4FaAgoICz98pKUdtRLqbSmgvEnXkgdcdUBWcw6c7Pnb2BJXJ0ngUUoa5th4jB9xVgYpJgDGL1T9bnHYPPHuesj3mfV/d19muptu42sjKktgk1bXv6BZ1IWjy8zBjS2IS1f9xZ5t3EXhsEnz3U58tS6MZqHi7iblASjkTOBe4TQjRr+GElPJxKeVsKeXs7Oxsz99JCBV9upuJYj1OzSA2RWU62IrAj2xSt8OmOT62J5ko3uQ426JwAYw4Bb74myoKAotGVh4IOPRuZLY3KDENlIAL0RuF++rno9EMYrwScCnlEfNtNfAG4N/P2mkF7pfTm+xsYgqhLAhbEXhlicr/zp7g+NieZKLUHVBVjdabjt5w+v9Tm40b/62+7+kF7oWA15XBsb3q+0D0QTEwfHAt4BqNUzwWcCFEohAi2fgaOBuwM9LFR3iSC97eYN9KiLdTjVm5GYZMcj4NPTFLRYzuCHjtAc9KxB1ReCoUzFeFQJ3t7vcCt8bYyDQm2QSiD4pBQpaqyIxLCdx7ajQhijcR+BDgCyHEZmAd8J6U0kdVKHZIzVfd6gyLwBlSmtMI7QhZQga01PV/TeVmtQnmCjkT3I/Avdmgs4UQcNpPofEIbHre/V7g1hjDHfauUreB6INiMPkymHNL4N5PowlhPN7ElFLuB5yYxD7GSCU8UdGb6+uIzjbVzMhRBG5sKhrUHVAbk842MA1yJkDJi65lonS0QmOl7yNwUO1u80+Cz/8K5z+o7nO1F7g1yUNVFFyxXn0fyAh8xrcD914aTYgTOpWYYFHM46KN0mMl2Pk4nmDDAz9Som7dicBNja61hK0zZ9D4OgKH3ii8oULljoPnEbgQKgqX3WovwJ/DjDUajceEmIAbgx1cTCV0tplneOCWXQArSyAiuneD0hnZbmxkejLn0R1GLYLc2b3etaceOPTaKElDdDGMRjNACS0BTxqqxNXVTBRnm3kJGaoU3dJTP1KiKj2jYl17jxyj/7QLAu7rFEJrhIDT/l/v955aKNC7kRmoFEKNRuM2oSXgEREqCnfVQrE10NiSnmpM80amuxuYoOyF5GGuTeepO6BENSHT+XM9ZcxZaoQYeG6hQN8IXKPRDEhCp5TeIDXf9XL6nl7gdiLRBIty+jTzwIi2etc3MA2yx7s23KH2AGQU+teSEAIu+Bvset+7DoKZY1Rv8JThPluaRqPxLaEn4GkFqtWrK7S74IFD70amuxuYBjkTVCl7d7f6lGCPugPmXiN+Zvh09y9C1kRGwbWv64IajWYAE1oWCigBb6rqLRt3hMkFDxx6i3mMDcwhk9xbU84E1RXQ0eZqd5fKQvHXBqY/GDFfR+AazQAmNAUcXEvb69nEtGOh2IrAcya4voFp4EomijGo1x8phBqNJiwJXQF3JZXQ6SamOb+5tc5iA9OD2iTLQbr2cNZGNoi0dXTR3tkV7GVoNBo3CT0BN/qCu5KJ0t6ohh5YT5Q3iIpRWSEttWpjtLXWM+84LkWty1EmirttZF2grtnEPz7eS0dXt1fH+c5z67nyX2u1iGs0IUboCXjyMNUG1pVMlPZG5x35EtKVcPdsYM7wbF3OMlFqDyh/PTXPs+Pb4IV1h/jTh7v4bHeNx8fo6OpmfVkdmytO8Pv3PJjvqdFogkboCXikeVq5KxG4yU4vcEviM1QEXlmiLgzubmAaFMxTU2zq7Fg7deZJ9PY+DXjA6p1qcO/K7VUeH2NPVROmzm7GDUnm32sP8s7mI75ankaj8TOhJ+DQ01Z23YFaPth2lO5uaft5jjoRGiRk9Ebg2RM8z52eeqW63fKy7cd93Ea2ttnEpvJ6oiIEH+2sosvez8AJWw/XA/DIt2YwsyCNn72+lQPHXOz2qHGJE60d9n9HNRovCFkBbztWxree+Jrv/WcDS/7+Ge9tqez/R2JyYTK7ZQQ+3IvmimkFqi/35hf79lYB9X1dmU/97092VSMl3HRKEcebTWw8VOf8RTbYUnGC5LgoRmUn8ci3ZhIdKfjBfzfS1qH9cF/Q2NbBwv/7mPvfdaHQS6Nxk5AU8F3t6cS0VDMzN4G/XDmNbgm3vbCxv5C3NzoX8IQMldHSctz9Ah5rpi2D2n29bVgNWmrVYAkfRuCrS6vJTo7ltjNGEx0pWLn9qEfH2Xr4BJOHpxIRIRieFs9frprOzsoG7ntnu8/WGs68u6WSE60dPLe2jJLy+mAvRzPICDkBf6vkME9u6SBCSJ5ZOpylM/P48M6FPLRsRh8hX7vvuP2BxpbEZ6i2qeC9gE+8SGW9lLzQ934fZ6B0dHXz2e4azhiXTWp8NPNHZbFyRxXSOvJ3gqmzm9LKRqbmpfbcd8a4HL5/+iheXFfOm5sO+2S94czLxeWMzEpkSHIcP399K51eZgxp+tLZ1c2Tn++nub0z2EsJCiEl4K9tqOCul0pIHjoSgMRWteEWGSG4aNrwHiE3dXZz/TPrMLU4GKdmYFRjikgYOtm7BcYmw4QLYfvrfStFfZwDXlxWR2NbJ2eOV42mzp40hIPHW9hd1eTWcXZXNWLq6maKhYAD/M9ZY5lbmMHP39jK3upGl4514Fgzlz/6FdsOn3BrDYOZPVWNbDpUz7K5BfzmoonsqGzg2a/Kgr2sQcXne4/xu/d28lZJeG6+h4yAv7y+nB+/upn5o7L46VVnqzutMlEMIX/jBwsYlZ1EV2sD5c1Osj6Maszs8RAd7/1Cp12tJvrsfr/3vjrftpH9eFc10ZGCU8aoCe5nTVBC7q6NsqVCie3U3LQ+90dFRvDwt2YQFx3JL97Y5lJk/6cPSyk+WMdPX92io0wzLxeXExUhuHRmLudMGsqZ43P4y6rdHKlvDfbSBg0bD6q9nw0HPdsDCnVCQsBf+OYQP31tCwvHZPPk9bOJy8wHEWG3L3h6Ygwv3DSLeGHije0n+GRXtf2DJ5irMb1t/mRQdBokD4fNy3vvqz2g8td9cYFApQ/OG5lJUqzqRZaTEseMgjRW7nAvnXDr4XpS46PJz+i/riEpcdy1eAzfHKjlY0c/P2Db4ROs2HqUOYXp7Khs4Jkvy9xahys8vHoPr25woX3CAKGjq5vXNx5m0YQcspJiEUJw30WT6JZS7y/4kOIyJdyebuKHOl4LuBAiUgixSQjxri8WZAuJZPGEHB67dhZx0ZEQGe00Fzw9ygRAfFIqtz6/wb6IGxG4t/63QUSkSincswqazO9pY5Dx5vJ61h2oxdTpXrR68Hgz+2qaOXN83zmVZ08cytbDJ9yK7rYePsGU3FSEnfa2V88toCgrkT+sKHUYVf955S5S46N56oY5LJ6goszy2haX1+GM1zdW8OdVu/nFG1upPBEa0evqndUcbzZx1Zz8nvvyMxK4Y9FYPtxexUduXmw1/eno6qakvJ746EgOHGvmWFN7sJcUcHwRgd8B+LWE79snjeCJ62Yr8TZIdTLYwdwH5VunTmJMTpJ9ER86FU79HzUN3QVaTJ1U1DkRp2nL1DDlra+q761ywFtMnSx74muufGwt0+9fyY3PrOOpLw6w62ijU7tiTak6h34CPknZKKtcFIa2ji52HW3s539bEh0ZwU/PGcee6iZe22g7+i0uq+XjXTV877RRpMRFc9/FkxECfv2Wa9aLM8qONfOrN7cxJTcVKeEvK3d7fcxA8EpxOTnJsSwck93n/ptPLWLskCTufXs7Labw3HjzFaWVjbR2dPVcJDeGoY3ilYALIfKA84EnfbMch+/V9460Asfl9OZOhIkpafz35pN6RPxnr2/l2S8P8NXeY9Q0tiMjImHRryHR8ZScrRUn+PkbW5n7+9Wc+edPqWl0cLXPGa+m4mx+AUwt0HS0TwS+akcVLaYufrpkHJfPyuNgbQu/fXcH5/ztM+b+72rWlNoX4TWl1YzKTmREZt9pO6OykxiVncjKHa754LuONtLRJZmaa1/AAZZMHsqMgjT+smo3raa+ueFSSv704S6ykmK5fv4IAHLT4rn7rLF8vKuG97ZW2jxmW0cXv3hjK0v+9hn7auxvvJo6u7lj+SYiIwT/unYWNywo5NWNFZQebXDpHINFVUMbH++q5vJZeURF9v0Ti46M4PeXTuFwfSt//2gPAN3dksoTrazdd5yX1h/iyc/36740LlB8UHURvWF+ITGREWHpg3s70OFvwE8Bu8nWQohbgVsBCgoKvHw7C9IKYOvL0NWhLBVrejoRJpOWEMN/bz6JH7+yhRVbK3lxXUfP09ITohkzJJnCzARGZCZSkJHAiMwERmQkIiLgrZIjLF93iO1HGoiLjmDhmGxW7qjiw+1HuWbeCPvrm7YM3v8p7FqhvreIwN8uOcLw1Di+t3AUERHqwnSkvpUv9x7jyc8PcOfyEt6/cyG5aX296ab2Tr7ZX9sjltacPWkoj3+2nxMtHaQm2PiZWLDFnC0y2YmACyH4+XkTuOJfa3n6ywPcdsbonse+2HuMbw7U8psLJ5IQ0/urdMP8Qt4sOcx97+zg1DEq1dFgb3UTt7+wkdKjjSTHRnH5o1/xzI1zmZ6f1u+9/7JqN5srTvDPb88kNy2e204fzUvry/nDilKeu2muw3UHk9c2VtAt4YrZ+TYfn1OYwVWz83nyiwOsLq3mUG1LPystISaKb53kw7+XQUjxwTqGp8ZRmJXI5NwULeDuIIS4AKiWUm4QQpxu73lSyseBxwFmz57tu3ritHyVv91w2HZ2R884NZVGmJYQw5PXz0ZKSU1jO7urmthT3cjuqib2Vjfy8a4aahr72gQRArolTBiWwv0XT+Li6bmkxEWx6M+f8v62SscCPvly+PDn8Okf1ffmCLyu2cSnu2v4zqlFPeINMDwtnitm5zO3KIPz/v45dy0v4YVbTuoTwX2x5ximru6e9EFrzp44hEc/2ceaXVVcOsNx06xtFSdIT4gmL935xuqcwgzOMh/76jn5ZCbFIqXkwQ93kZsWzzIroYmKjOCBpVO56JEv+OMHpfzvpWq+5usbK/jlm9uIi47k2RvnMCIzkeue/oZlj3/No9fM5PRxvbbQl3uP8dhn+1g2N5/zpgwDIDUhmtvPGM3vV+zkiz3HerJwBhJSSl4prmBuYQZFWfZnkt5z7niONrQRFx3BmeNz+gQO3//vBp7+8gDL5ubb3Z/QKMtk1giVhDC7MINnvyqjvbOL2Cjf9Rsa6HgTgS8ALhJCnAfEASlCiP9IKa/xzdKc0NMXvNyxgFsV8gghyEmJIyclrp8AtJg6OVTbwsHjLRw63kJ9q4mzJw5lal7fjb7zpgzjn5/s5XhTO5lJdoY/JGbCmHNg13vqe/MaV2yrpLNbcvG0XJsvG5GZyO8uncxdL23mkY/3cufisT2PfVxaTXJcFLML022+dlpeGjnJsazc7lzAtxw+wZS8NJcF4v8tGcfZf/2Mh9fs5TcXTWLVjio2V5zg/y6bavMPZnJuKjcuKOKpLw5w7uShvF1yhFc2VDC3KIOHrp7B0FTVc+a178/nhqfXc/Nzxfzf5VNZOjOP403t3PVSCSOzEvnVBX1H0F03fwTPrS3jD+/v5J1Rp/S5CA4E1pfVceBYc59PKrZIT4yx+ynipgVF/M8rm/l8zzEWjs22+Zxw53B9K5Un2phtFvCZBek8/tl+th1u6BH1cMBjD1xK+TMpZZ6UshC4GlgTMPEGCwG3s5HpbJyaDRJiohg/NIVzJg3lloUj+ck545mW31/kzpsyjG4JHzrrAjh9mXkNKT0FQ2+VHGFMThIThtkv8b90Rh6XzsjlodV7WF+mfL7ubsmaXdUsHJtNdKTt/7aICMFZE4fw6e4ah71M2jq62F3V6NT/tmR0TjJXzSngP18fZH9NE39euZuRWYksnWn7QgRw91ljyU2L59qn1vHqxgp+eOZoXrj5pB7xBshJjuOl785jblEGd7+8mcc/28f/e20L9S0dPLxsZh9rBiA2KpKfnDOO7UcaeGvzwKsUfWl9OUmxUZw3ZajHx7hg2jCykmJ55ssDPlzZ4KLY/Hcxu1D9XRmivcHsi4cLIZEHbpOUPEDYF/D2Xg/c10wYpjzzFXY26XoYc7aa+pNeCEJwpL6VdQdquXj6cKeR7/0XT1JpZy9u4kRLB9uPNFDT2M6Z43Icvu7sSUNpMXXx5d5jdp+zo7KBrm7p1P+25q7FY4iOjODap9axq6qRO88a22+TzpLE2Cj+eNlUJg1P4d83zeV/zh5n8/nJcdE8c+Mczp86jP9dUcpHO6v52XnjmTg8xeZxL5w6nCm5qTz44e4B1XSrsa2DFVsruXDasH4XHneIjYrk2nkj+HhXjcNN3nBm48E6EmIiGT9U/X1nJ8cyIjMh7Hxwnwi4lPITKeUFvjiWy0TFqOIYe5kopr4euC8RQnDelGGs3X+c2maTgzXGwkWPwBm/AOBtc6/ti+zYJ5Ykx0Xz0NUzqG5s557Xt7C6tAoh4PRxjj9Snzwyk+TYKIc9wo1y96kOUghtkZMSxy2nFnG4vpXxQ5O5wOxNO+KUMVm896NTOXWM43XHRkXy8NUzuO2MUVwzr4Ab5hfafW5EhOBn543ncH0rz7lRmt7VLdlwsNZvrV3f3VJJa0eX3c1Ld/j2vAJiIiN41g9FUa6y40gD337ya3Ydda2dQiApPljH9Py0PgHBrBHpbDhY55P01VAhdCNwUDZKXZntx9qb1ICGKA/7ezvhvCnD6OqWfOisfH3CBTBuCaDskxkFaRRkJrj0HtPy0/jxOeN4f9tRnvhsP9Pz0+x77mZioiI4fXwOq3ZW2Y1Ot1ScICsphmGp7v9sbj1tFOdOHsrvLpnsc/85IkLwk3PG87tLpjj9hDJ/VBZnjMvmkY/3UufoImrB31fv4bJH1/LvtWU+WG0v1Q1t/OvTffx11W7G5CQxw0ZGjbtkJcVy0fThvLqhghMtHc5f4GM2HKzj6sfX8uXe4zz+2f6Av78jmto72VnZ0ON/G8wakc6xJhOHfFhENtAJbQEfOlkNIu62IVRGJ0I/7eJPGp7CCFdsFDO7qxrZWdnAxdOGu/U+t546klNGZ9Fs6mLReMf2icG35hZQ22zin5/ss/n41grHFZiOSIqN4tFrZvV4j8HknnMn0NzeyX3vbHcada0vq+WRNXuIiYzgb6v3UN/imujbo72zi/e3VnLTs+s5+YE1PPB+KfkZCfxhqfOLj6vcuKCQ1o4ulq93YfqUC3R0dbO/pokmJ537vthzjGuf+oaMxBgWTxjCiq2VNLYF5iIipWT5ukNsrbDfFG1zeT3dEmZaCfjsEep30iivDwe8zQMPLnlzYf2TUL2zfydBV3qBe4Fhozz+2X7qmk2kJ8Y4fP7bJUeIEHD+VPcEPCJC8Jcrp/G793aydKZr8zRPHpXJhdOG869P97F0Ri6FFulsLaZO9lQ3cs4k26mIocS4ocnctXgsf161mzFDku1mfjS0dXDn8hLy0lX/+CsfW8vfV+/h3gtdH58npaTseAvry2pZf6CWj3ZWUdfSwZCUWG5dOJLLZ+UxKtu3dt2k4anMG5nBv9ce5DunFDncb7CmurGNb/bXsqdapcnuqWqi7HgzHV2SlLgoblxQxE0LivrVC6zcfpTbX9hEUVYiz988l8N1rXy0s4p3t1SybK5/89KllNz3zg6e/aqMoqxEVt210OY5F5fVIUR/AR+Tk0RyXBQbDtVx2SzfzZ4dyIS2gOfPUbfl39gWcGe9wL3kvMnDePSTfazccZSr5tj/5ZZS8tbmwywYnUV2smMLxBY5KXE8tMy9Ycu/PH8CH5dW85t3tvPMDXN6osKdlQ10S5iSl+b2OgYit585mn01Tfzpw10UZSX25Ixb8qs3t3G0oY1Xv3cyMwrSuXpuAc+vPci3TxrB6Bz7vyO1zSbe2HSY4rJa1pfV9fTaSEuI5pTRWVw2K4+FY7KJ9GMq400Lirj1+Q2s3FFl89ysae/s4snPD/DImr20dnQRIVRq6uicJM6aOITCzERWl1bx99V7eOqLA1x38gi+c0oRmUmxvLGpgh+/soXJuak8d+Mc0hJiyE6KZUxOEi+tL/ergHd3S37x5jZeXHeI+aMy+Wrfcd4qOWJTiIsP1jJuSDIpcX0vPhERgpkF6WwIQAT+xqYK9tc0k54QQ0ZiDOmJMWSab9MToomPjgxIDn9oC3h6ESRkqQk4c77T9zFXBhp7yeTcFPIz4nlvq2MB31ReT3ltK3csGmv3Ob5mSEocdy4ew+/e28mH26tYMlmltfW0kHVzA3OgIoTggcumUl7Xyt0vl5CbFs80Cw/6jU0VvFVyhP85aywzClTEdvdZY3mn5Aj/u2InT98wx+Zxjze1c+Vja9lX00xeejynjsliTmEGcwrTGZWdFLD880UThlCQkcDTXxxwKuAfl1Zz3zvbKTvewjmThnD7GWMYMySpbw8h4Mo5+ZQebeDhNXt59NN9PPNlGWdOyGHF1krmFWXyxPWzezpdCiG4ak4+v3tvJ7urGhk7xP6n2raOLrYdPkFBRgLZybEuC1hXt+Snr27htY0V3HbGKP7nrHFc+MgXPLRmDxdPH94nCu/qlpQcqufC6bY/yc4akc5fP9rNidaOPhXABp/trmH5+kPcuXisw3NxxLtbjnDXS5sdPicmKoK0+GjSE2JIS1C3t5852u3ML2eEtoALAfknQfm6/o+1uzAP0+u3VzbKU58foL7FRFqCbRvl7ZIjxERFBNy2uGF+Ia9uqOD+d7azcGwWCTFRbK04QU5yLENS/LO5GwzioiN57NpZXPKPL7n538W8ddsChqfFU17bwq/e3M6cwnR+YGGvZCXF8sNFo/nfFaV8uruG06yKZRraOrj+mXVU1LXywi0nMX9U8Co+IyME188v5Lfv7mBLRT1TbXxyOni8mfvf2cHq0mpGZify75vmOi0AGj80hX98ayZ7qxv5x8f7eKvkMGeOy+Ef357ZT/AvnZHLA++X8vL6cn5pVVhlyX3vbOfFdSorLDkuitE5SYzKTmJ0ThLjhyYza0Q6yVZRc0dXN3e/vJl3Nh/h7rPG8qNFYwC4a/FYbv53Ma9vOsyVFlk9u6saaWzv7LeBaTB7RDpSwqZDdX0qe0F9orrrpRKON5tYtaOK7502itvOGN3vfB1RXtvCz17fyvT8NF767jxaTV3UNpv6/Ktv7aCuxUR9s/m2tYP9x5r80t8mtAUclI2y6z1oPt63IZWpCVKcf+T0lvOnDOOxT/ezckdVn180g86ubt7dcoTFE3L6/fL6m6jICH57yWSu+NdaHl6zl/+3ZLyqwPRxFDAQyEqK5ekb5rD0n19x83PFLP/uPO5Yvgkh4K9XTe9nc9wwv4gXvjnEb9/dwYI7Tu2J8lpNXdz8bDGllY08cd3soIq3wZWz8/jrqt08+sk+rj15BEfq26isb+XIiVYO17fx9b7jREcKfn7eeG6YX0RMlOte+eicZP561XR+dcFE0uKjbX6yyEyKZfGEIby+6TA/XTLe5vE3HqrjxXXlXDYzj6l5qeytbmJvdROf7a7p6eMeIZSvf1JRBnOLMphekMav3tzGh9uruOfc8XzvtFE9x1s0IYepeak8vGYPl87I7SleKzbneRsbltZMy08jMkKw8WB/Af/N29tpaOvghVtO4tXiCh5es5f3tlTy+0uncPIox83sQP0t37F8E0h4eNkMYqMiiY2KJC0hhpFBKpgNfQHPM5cjV6yDcef23t/e5JciHmum5KaSlx7Piq2VNgX8q33HOdZkcin32x/MKczgspl5PPn5fs6dPJR9NU1cMNX/F7ZgMHZIMo98awY3Pbues//yGUcb2nho2Qzy0vunbcZERfDz8yZw6/MbeGHdIa47uRBTZzff+88G1h+s5eFlMzjDxawff5McF80Vs/N45ssy3t/Wm7aalRRLblocl8/O445FY7z6VJXhZBP+qjn5fLD9KGtKq1gyue/vT2dXN798YxtDU+K4/+JJJMb2lZUTrR1sP3yCrw/U8s3+4/z764M8+UVvlem9F07kxgV9++ULIbhr8VhufHY9r22o4Gqz/77xYB3ZybE2h5CAKh6bMCy5R+gNPtx+lLfNUf78UVnMH5XFpTNz+cUb21Rr59l5/Py8CXY/RYNKQ914qJ6Hl80gP8O1VGB/E/oCPnyGyvcutxJwk3+zUAyEEJw/ZRhPf3mgTxfA7m7J+rJa/r56D8lxUZwxPng9LX523nhW7TjKd5/fgJSDx/+2xenjcrj3wknc+/Z2ls7I5SIHaZtnTRzC/FGZ/GXVbi6YOpxfvbmNT3fX8MDSKVzgZraQv7n7rLFMz08jOymW4WnxDE2Nc+ujv7csHJvN0JQ4Xlpf3k/An//6IDsqG/jnt2f2E29ADd4encX80erTTFtHF5vL61lfVsvYIcmcPcl224HTx2UzPT+Nh9fsZenMPGKiIig+WMusgnSH/vqsgnRe2VBBZ1c3UZER1LeY+MUb25g4LIXvn94b5Z86JpsP71zI31fv4YnP97N6ZzU/O28Cl83M7Xf8tfuO88jHe7lydh4XupkK7E9COw8cICYBhk5RG5kGUprTCP27iWlw7pRhdHRJVu44yqHjLfx11W5Oe/Bjrnr8a0orG/jJOeOC2iEtKymWn5wzjsoTatCyrzdSBhrXzy/kjR/M5w+XTXH4PCEEv7pgIg2tHZz/0Oe8t7WSX5w3oSfaG0gkx0Vz8fRc5o/OojArMaDiDcqLv2xWLp/uruHoid6B3dUNbfx55W4Wjs3m3Mmu9X+Ji47kpJGZ3H7mGLviDeYo/KyxHK5v5ZUN5VQ3tFFe22q3mZvBrMIMWkxdlJorSO9/Zwf1LSb+dMXUfn2E4mMiuefc8bxz+ymMyEzgx69s5qrH+laf1jabuPMllVr5m4tcTz0NBKEv4KBslMMboMtcoNDRqlrN+jmN0GBaXiq5afHc/84OFv7pYx5as4cRGYn89apprP/lYq47uTAg63DEt04a0WP35CQPng1Me8woSHfpojlhWApXzy2g8kQbPzpzNLcsHBmA1YUmV8zKp1vSZzrT71fsxNTVzf0XTfJL2tzCMVnMLEjjH2v2snb/cQCn3QZ7G1vVsXpnFa9vOswPzhjNpOH2A5eJw1N49Xvz+eNlU9hd3cj5D33OH1bspLm9k5++uoW65g4eunqGVz1u/MHAWo2n5M+FdY9B1TY1nLinE6H/LRRQkcJNpxTxSnE5F04bzqUzchme5psBxr4iMkLw3E1zaWgNfFn2QOfXF0zkginDXNrICmcKsxI5qSiDl4vL+f5po/h6v8rV/tGiMX2KxXyJEIK7zxrHNU99wx9WlBIbFeFQiEFNhRqWGsea0mpKjzYwfmgytztp7wsqj/yqOQWcNXEof3y/lMc+28+L6w7R0NbJry+YOCA/uQ4OAc8z5/JWrFcCbqcXuD/5zilFfOeUIudPDCIZiTFON6vCkbjoyB5/VuOYq+bkc/fLm/ly3zF+8/Z2CjIS+IGFr+wPFozOZE5hOuvL6phbmOFSls3MEem8t6WSyAjBk9fNcSszJyMxhj9ePpUr5+Rx3zs7yM9I4MYFhV6cgf8YHBZKWgEkDe3NB/egF7hGo3HOuZOHkRwbxY9e3MS+mmbuu3iS3/14wwsHmOXE/zYw8sS/d9pIh4O7HTFrRAZv334K//jWzAE7GWlwROBCqHzwCrOA9/QC1wKu0fiS+JhILpw+nBe+OcSSSUM5w0l/el8xf1QWDy+bwbyRrtlcl87Ipb2ze8BGzr5icAg4qI3Mne9AU7VFBG57IIBGo/GcmxYUcaCmmV9faL8q0x+4k76XlhDTpzBosDJ4BDzfXNBTvg46zWlO2kLRaHzO6JwkXrx1XrCXoWGweOAAw6ZDRLSyUYKwianRaDSBxmMBF0LECSHWCSE2CyG2CyHu8+XC3CY6DoZNg/L1ehNTo9GEBd5E4O3AmVLKacB0YIkQIrifq/LnwpGN0Grug6AjcI1GM4jxWMClwhiZHW3+F9xponlzlP996GuIToCI4JWvazQajb/xygMXQkQKIUqAamCVlPIbG8+5VQhRLIQorqmp8ebtnJN/krot/0ZH3xqNZtDjlYBLKbuklNOBPGCuEGKyjec8LqWcLaWcnZ3t5458qbmQkgvdnQEro9doNJpg4ZMsFCllPfAJsMQXx/MKo6xeb2BqNJpBjjdZKNlCiDTz1/HAYqDUR+vyHCMfPADDHDQajSaYeFPIMwx4TggRiboQvCylfNc3y/ICwwfXEbhGoxnkeCzgUsotwAwfrsU3DJ0KkbF6E1Oj0Qx6Bk8pvUFUDJz7AGSNC/ZKNBqNxq8MPgEHmH1TsFeg0Wg0fmfw9ELRaDSaMEMLuEaj0YQoWsA1Go0mRNECrtFoNCGKFnCNRqMJUbSAazQaTYiiBVyj0WhCFC3gGo1GE6IIKQM3g0EIUQMc9PDlWcAxHy5nIBMu5xou5wnhc67hcp4Q2HMdIaXs1487oALuDUKIYinl7GCvIxCEy7mGy3lC+JxruJwnDIxz1RaKRqPRhChawDUajSZECSUBfzzYCwgg4XKu4XKeED7nGi7nCQPgXEPGA9doNBpNX0IpAtdoNBqNBVrANRqNJkQJCQEXQiwRQuwSQuwVQtwT7PX4CiHE00KIaiHENov7MoQQq4QQe8y36cFco68QQuQLIT4WQuwUQmwXQtxhvn9Qna8QIk4IsU4Isdl8nveZ7x9U52kghIgUQmwSQrxr/n6wnmeZEGKrEKJECFFsvi/o5zrgBdw8NPkfwLnARGCZEGJicFflM54Flljddw+wWko5Blht/n4w0An8j5RyAjAPuM38/zjYzrcdOFNKOQ2YDiwRQsxj8J2nwR3ATovvB+t5ApwhpZxukfsd9HMd8AIOzAX2Sin3SylNwHLg4iCvySdIKT8Daq3uvhh4zvz1c8AlgVyTv5BSVkopN5q/bkT90ecyyM5XKprM30ab/0kG2XkCCCHygPOBJy3uHnTn6YCgn2soCHguUG7xfYX5vsHKECllJSjRA3KCvB6fI4QoBGYA3zAIz9dsK5QA1cAqKeWgPE/gb8BPgW6L+wbjeYK6CK8UQmwQQtxqvi/o5xoKQ42Fjft07mOIIoRIAl4D7pRSNghh6783tJFSdgHThRBpwBtCiMlBXpLPEUJcAFRLKTcIIU4P8nICwQIp5REhRA6wSghRGuwFQWhE4BVAvsX3ecCRIK0lEFQJIYYBmG+rg7wenyGEiEaJ93+llK+b7x605yulrAc+Qe1zDLbzXABcJIQoQ9maZwoh/sPgO08ApJRHzLfVwBsoazfo5xoKAr4eGCOEKBJCxABXA28HeU3+5G3gevPX1wNvBXEtPkOoUPspYKeU8i8WDw2q8xVCZJsjb4QQ8cBioJRBdp5Syp9JKfOklIWov8k1UsprGGTnCSCESBRCJBtfA2cD2xgA5xoSlZhCiPNQflsk8LSU8vfBXZFvEEK8CJyOaktZBdwLvAm8DBQAh4ArpJTWG50hhxDiFOBzYCu9nunPUT74oDlfIcRU1IZWJCpAellKeb8QIpNBdJ6WmC2UH0spLxiM5ymEGImKukHZzi9IKX8/EM41JARco9FoNP0JBQtFo9FoNDbQAq7RaDQhihZwjUajCVG0gGs0Gk2IogVco9FoQhQt4BqNRhOiaAHXaDSaEOX/A8Zqx3vod2cuAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluate the model\n",
    "train_loss, train_acc = model8.evaluate(X_train, Y_train, verbose=0)\n",
    "test_loss, test_acc = model8.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Train loss: %.3f, Validation loss: %.3f' % (train_loss, test_loss))\n",
    "# plot training history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latest=tf.train.latest_checkpoint(checkpoint_dir)\n",
    "weights_file = 'wavelength_467/Weights-003--2.79134.hdf5' # choose the best checkpoint \n",
    "model8.load_weights(weights_file) # load it\n",
    "model8.compile(loss='mean_absolute_percentage_error', optimizer='adam', metrics=['mean_absolute_percentage_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model8.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percentage error on test set:  [0.02454674 0.04591819 0.04352269]\n"
     ]
    }
   ],
   "source": [
    "error= mean_absolute_percentage_error(Y_test, Y_pred, multioutput='raw_values')   \n",
    "print('Mean absolute percentage error on test set: ', error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
