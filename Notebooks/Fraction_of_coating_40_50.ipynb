{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "from tensorflow import keras\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv(\"database.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q_abs</th>\n",
       "      <th>q_sca</th>\n",
       "      <th>g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.11691</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>0.003798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.15300</td>\n",
       "      <td>0.001005</td>\n",
       "      <td>0.008979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.20060</td>\n",
       "      <td>0.002514</td>\n",
       "      <td>0.022970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.23162</td>\n",
       "      <td>0.004187</td>\n",
       "      <td>0.037044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.25746</td>\n",
       "      <td>0.005988</td>\n",
       "      <td>0.051049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     q_abs     q_sca         g\n",
       "0  0.11691  0.000389  0.003798\n",
       "1  0.15300  0.001005  0.008979\n",
       "2  0.20060  0.002514  0.022970\n",
       "3  0.23162  0.004187  0.037044\n",
       "4  0.25746  0.005988  0.051049"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = df1.iloc[:,25:28]\n",
    "X = df1.iloc[:,:8]\n",
    "Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1580, 36)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set=df1[(df1['fraction_of_coating']<40) | (df1['fraction_of_coating']>50)]\n",
    "test_set=df1[(df1['fraction_of_coating']==40) | (df1['fraction_of_coating']==50)]\n",
    "test_set.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = train_set.iloc[:,25:28]\n",
    "X_train = train_set.iloc[:,:8]\n",
    "Y_test = test_set.iloc[:,25:28]\n",
    "X_test = test_set.iloc[:,:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_x=StandardScaler()\n",
    "#scaling_y=StandardScaler()\n",
    "X_train=scaling_x.fit_transform(X_train)\n",
    "X_test=scaling_x.transform(X_test)\n",
    "#Y_train=scaling_y.fit_transform(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters:\n",
    "1. No of hideen layers\n",
    "2. No of neurons in hidden layers\n",
    "3. Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model=keras.Sequential()\n",
    "    \n",
    "    for i in range(hp.Int('num_layers', 3,10)):\n",
    "        model.add(layers.Dense(units=hp.Int('units_' + str(i),\n",
    "                                           min_value=32,\n",
    "                                           max_value=256,\n",
    "                                           step=32),\n",
    "                              activation='relu'))\n",
    "        model.add(layers.Dense(3, activation='linear'))\n",
    "        model.compile(optimizer=keras.optimizers.Adam(\n",
    "            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),\n",
    "        loss='mean_absolute_percentage_error',\n",
    "        metrics=['mean_absolute_percentage_error'])\n",
    "        \n",
    "    return model\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner=RandomSearch(build_model,\n",
    "                  objective='mean_absolute_percentage_error',\n",
    "                  max_trials=8,\n",
    "                  executions_per_trial=3,\n",
    "                   directory= 'project1',\n",
    "                   project_name='fraction_of_coating_40_50'\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 5\n",
      "num_layers (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 3, 'max_value': 10, 'step': 1, 'sampling': None}\n",
      "units_0 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 256, 'step': 32, 'sampling': None}\n",
      "learning_rate (Choice)\n",
      "{'default': 0.01, 'conditions': [], 'values': [0.01, 0.001, 0.0001], 'ordered': True}\n",
      "units_1 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 256, 'step': 32, 'sampling': None}\n",
      "units_2 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 256, 'step': 32, 'sampling': None}\n"
     ]
    }
   ],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 03m 03s]\n",
      "mean_absolute_percentage_error: 19.752186457316082\n",
      "\n",
      "Best mean_absolute_percentage_error So Far: 2.6286720434824624\n",
      "Total elapsed time: 00h 15m 22s\n",
      "\n",
      "Search: Running Trial #6\n",
      "\n",
      "Hyperparameter    |Value             |Best Value So Far \n",
      "num_layers        |9                 |3                 \n",
      "units_0           |96                |224               \n",
      "learning_rate     |0.001             |0.0001            \n",
      "units_1           |160               |32                \n",
      "units_2           |192               |192               \n",
      "units_3           |192               |96                \n",
      "units_4           |224               |128               \n",
      "units_5           |224               |224               \n",
      "units_6           |128               |224               \n",
      "units_7           |160               |128               \n",
      "units_8           |160               |64                \n",
      "\n",
      "Epoch 1/200\n",
      "258/258 [==============================] - 1s 2ms/step - loss: 73.1351 - mean_absolute_percentage_error: 73.1351\n",
      "Epoch 2/200\n",
      "258/258 [==============================] - 1s 2ms/step - loss: 27.0154 - mean_absolute_percentage_error: 27.0154\n",
      "Epoch 3/200\n",
      "258/258 [==============================] - 1s 2ms/step - loss: 23.7930 - mean_absolute_percentage_error: 23.7930\n",
      "Epoch 4/200\n",
      "258/258 [==============================] - 1s 2ms/step - loss: 23.0238 - mean_absolute_percentage_error: 23.0238\n",
      "Epoch 5/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 22.9680 - mean_absolute_percentage_error: 22.9680\n",
      "Epoch 6/200\n",
      "258/258 [==============================] - 1s 2ms/step - loss: 21.8131 - mean_absolute_percentage_error: 21.8131\n",
      "Epoch 7/200\n",
      "258/258 [==============================] - 1s 2ms/step - loss: 21.6823 - mean_absolute_percentage_error: 21.6823\n",
      "Epoch 8/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 21.5599 - mean_absolute_percentage_error: 21.5599\n",
      "Epoch 9/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 21.2646 - mean_absolute_percentage_error: 21.2646\n",
      "Epoch 10/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 20.9447 - mean_absolute_percentage_error: 20.9447\n",
      "Epoch 11/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 21.0024 - mean_absolute_percentage_error: 21.0024\n",
      "Epoch 12/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 20.7156 - mean_absolute_percentage_error: 20.7156\n",
      "Epoch 13/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 20.5072 - mean_absolute_percentage_error: 20.5072\n",
      "Epoch 14/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 20.8735 - mean_absolute_percentage_error: 20.8735\n",
      "Epoch 15/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 20.7054 - mean_absolute_percentage_error: 20.7054\n",
      "Epoch 16/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 20.6552 - mean_absolute_percentage_error: 20.6552\n",
      "Epoch 17/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 20.4094 - mean_absolute_percentage_error: 20.4094\n",
      "Epoch 18/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 20.6242 - mean_absolute_percentage_error: 20.6242\n",
      "Epoch 19/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 20.6348 - mean_absolute_percentage_error: 20.6348\n",
      "Epoch 20/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 20.0956 - mean_absolute_percentage_error: 20.0956\n",
      "Epoch 21/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 20.4144 - mean_absolute_percentage_error: 20.4144\n",
      "Epoch 22/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 19.9813 - mean_absolute_percentage_error: 19.9813\n",
      "Epoch 23/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 20.0773 - mean_absolute_percentage_error: 20.0773\n",
      "Epoch 24/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 20.2397 - mean_absolute_percentage_error: 20.2397\n",
      "Epoch 25/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.9851 - mean_absolute_percentage_error: 19.9851\n",
      "Epoch 26/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 20.0278 - mean_absolute_percentage_error: 20.0278\n",
      "Epoch 27/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 19.9279 - mean_absolute_percentage_error: 19.9279\n",
      "Epoch 28/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.9740 - mean_absolute_percentage_error: 19.9740\n",
      "Epoch 29/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.9678 - mean_absolute_percentage_error: 19.9678\n",
      "Epoch 30/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 20.0466 - mean_absolute_percentage_error: 20.0466\n",
      "Epoch 31/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 19.6676 - mean_absolute_percentage_error: 19.6676\n",
      "Epoch 32/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.7739 - mean_absolute_percentage_error: 19.7739\n",
      "Epoch 33/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 19.9885 - mean_absolute_percentage_error: 19.9885\n",
      "Epoch 34/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 19.7092 - mean_absolute_percentage_error: 19.7092\n",
      "Epoch 35/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 19.6450 - mean_absolute_percentage_error: 19.6450\n",
      "Epoch 36/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 19.6359 - mean_absolute_percentage_error: 19.6359\n",
      "Epoch 37/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.6980 - mean_absolute_percentage_error: 19.6980\n",
      "Epoch 38/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 19.7948 - mean_absolute_percentage_error: 19.7948\n",
      "Epoch 39/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 19.4065 - mean_absolute_percentage_error: 19.4065\n",
      "Epoch 40/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.9903 - mean_absolute_percentage_error: 19.9903\n",
      "Epoch 41/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.4366 - mean_absolute_percentage_error: 19.4366\n",
      "Epoch 42/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.9311 - mean_absolute_percentage_error: 19.9311\n",
      "Epoch 43/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.4845 - mean_absolute_percentage_error: 19.4845\n",
      "Epoch 44/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.6410 - mean_absolute_percentage_error: 19.6410\n",
      "Epoch 45/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.7031 - mean_absolute_percentage_error: 19.7031\n",
      "Epoch 46/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.4302 - mean_absolute_percentage_error: 19.4302\n",
      "Epoch 47/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.4153 - mean_absolute_percentage_error: 19.4153\n",
      "Epoch 48/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 19.6190 - mean_absolute_percentage_error: 19.6190\n",
      "Epoch 49/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 19.7389 - mean_absolute_percentage_error: 19.7389\n",
      "Epoch 50/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 19.4725 - mean_absolute_percentage_error: 19.4725\n",
      "Epoch 51/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 19.5766 - mean_absolute_percentage_error: 19.5766\n",
      "Epoch 52/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 19.5043 - mean_absolute_percentage_error: 19.5043\n",
      "Epoch 53/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 19.4605 - mean_absolute_percentage_error: 19.4605\n",
      "Epoch 54/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 19.3177 - mean_absolute_percentage_error: 19.3177\n",
      "Epoch 55/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.4489 - mean_absolute_percentage_error: 19.4489\n",
      "Epoch 56/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.4773 - mean_absolute_percentage_error: 19.4773\n",
      "Epoch 57/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.4246 - mean_absolute_percentage_error: 19.4246\n",
      "Epoch 58/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 19.3166 - mean_absolute_percentage_error: 19.3166\n",
      "Epoch 59/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "258/258 [==============================] - 0s 2ms/step - loss: 19.2998 - mean_absolute_percentage_error: 19.2998\n",
      "Epoch 60/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.4250 - mean_absolute_percentage_error: 19.4250\n",
      "Epoch 61/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.4598 - mean_absolute_percentage_error: 19.4598\n",
      "Epoch 62/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 19.2161 - mean_absolute_percentage_error: 19.2161\n",
      "Epoch 63/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 19.2392 - mean_absolute_percentage_error: 19.2392\n",
      "Epoch 64/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.3879 - mean_absolute_percentage_error: 19.3879\n",
      "Epoch 65/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.8845 - mean_absolute_percentage_error: 19.8845\n",
      "Epoch 66/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.0564 - mean_absolute_percentage_error: 19.0564\n",
      "Epoch 67/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.4179 - mean_absolute_percentage_error: 19.4179\n",
      "Epoch 68/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.3511 - mean_absolute_percentage_error: 19.3511\n",
      "Epoch 69/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.3443 - mean_absolute_percentage_error: 19.3443\n",
      "Epoch 70/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.3223 - mean_absolute_percentage_error: 19.3223\n",
      "Epoch 71/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.7083 - mean_absolute_percentage_error: 19.7083\n",
      "Epoch 72/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.3445 - mean_absolute_percentage_error: 19.3445\n",
      "Epoch 73/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.6191 - mean_absolute_percentage_error: 19.6191\n",
      "Epoch 74/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.2585 - mean_absolute_percentage_error: 19.2585\n",
      "Epoch 75/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.3668 - mean_absolute_percentage_error: 19.3668\n",
      "Epoch 76/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.3268 - mean_absolute_percentage_error: 19.3268\n",
      "Epoch 77/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.2067 - mean_absolute_percentage_error: 19.2067\n",
      "Epoch 78/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.3064 - mean_absolute_percentage_error: 19.3064\n",
      "Epoch 79/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.1974 - mean_absolute_percentage_error: 19.1974\n",
      "Epoch 80/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.2511 - mean_absolute_percentage_error: 19.2511\n",
      "Epoch 81/200\n",
      "258/258 [==============================] - 2s 7ms/step - loss: 19.0428 - mean_absolute_percentage_error: 19.0428\n",
      "Epoch 82/200\n",
      "258/258 [==============================] - ETA: 0s - loss: 19.5335 - mean_absolute_percentage_error: 19.53 - 0s 1ms/step - loss: 19.5296 - mean_absolute_percentage_error: 19.5296\n",
      "Epoch 83/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.1081 - mean_absolute_percentage_error: 19.1081\n",
      "Epoch 84/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.1907 - mean_absolute_percentage_error: 19.1907\n",
      "Epoch 85/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.2668 - mean_absolute_percentage_error: 19.2668\n",
      "Epoch 86/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.2066 - mean_absolute_percentage_error: 19.2066\n",
      "Epoch 87/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.1324 - mean_absolute_percentage_error: 19.1324\n",
      "Epoch 88/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.4375 - mean_absolute_percentage_error: 19.4375\n",
      "Epoch 89/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 19.0241 - mean_absolute_percentage_error: 19.0241\n",
      "Epoch 90/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.1433 - mean_absolute_percentage_error: 19.1433\n",
      "Epoch 91/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 19.0190 - mean_absolute_percentage_error: 19.0190\n",
      "Epoch 92/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 18.9731 - mean_absolute_percentage_error: 18.9731\n",
      "Epoch 93/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.0468 - mean_absolute_percentage_error: 19.0468\n",
      "Epoch 94/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.1130 - mean_absolute_percentage_error: 19.1130\n",
      "Epoch 95/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.0592 - mean_absolute_percentage_error: 19.0592\n",
      "Epoch 96/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.2560 - mean_absolute_percentage_error: 19.2560\n",
      "Epoch 97/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.1957 - mean_absolute_percentage_error: 19.1957\n",
      "Epoch 98/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.7658 - mean_absolute_percentage_error: 18.7658\n",
      "Epoch 99/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.9084 - mean_absolute_percentage_error: 18.9084\n",
      "Epoch 100/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.8341 - mean_absolute_percentage_error: 18.8341\n",
      "Epoch 101/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.9322 - mean_absolute_percentage_error: 18.9322\n",
      "Epoch 102/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 18.9177 - mean_absolute_percentage_error: 18.9177\n",
      "Epoch 103/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 18.7393 - mean_absolute_percentage_error: 18.7393\n",
      "Epoch 104/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 18.9884 - mean_absolute_percentage_error: 18.9884\n",
      "Epoch 105/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.8976 - mean_absolute_percentage_error: 18.8976\n",
      "Epoch 106/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.7549 - mean_absolute_percentage_error: 18.7549\n",
      "Epoch 107/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 18.8982 - mean_absolute_percentage_error: 18.8982\n",
      "Epoch 108/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 18.8303 - mean_absolute_percentage_error: 18.8303\n",
      "Epoch 109/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 18.8191 - mean_absolute_percentage_error: 18.8191\n",
      "Epoch 110/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 19.1543 - mean_absolute_percentage_error: 19.1543\n",
      "Epoch 111/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 18.8156 - mean_absolute_percentage_error: 18.8156\n",
      "Epoch 112/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.0078 - mean_absolute_percentage_error: 19.0078\n",
      "Epoch 113/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 18.7894 - mean_absolute_percentage_error: 18.7894\n",
      "Epoch 114/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 18.6589 - mean_absolute_percentage_error: 18.6589\n",
      "Epoch 115/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 18.6858 - mean_absolute_percentage_error: 18.6858\n",
      "Epoch 116/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 18.6623 - mean_absolute_percentage_error: 18.6623\n",
      "Epoch 117/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.3224 - mean_absolute_percentage_error: 19.3224\n",
      "Epoch 118/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.7253 - mean_absolute_percentage_error: 18.7253\n",
      "Epoch 119/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 18.4875 - mean_absolute_percentage_error: 18.4875\n",
      "Epoch 120/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.5570 - mean_absolute_percentage_error: 18.5570\n",
      "Epoch 121/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.8421 - mean_absolute_percentage_error: 18.8421\n",
      "Epoch 122/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.7915 - mean_absolute_percentage_error: 18.7915\n",
      "Epoch 123/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.7521 - mean_absolute_percentage_error: 18.7521\n",
      "Epoch 124/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.5944 - mean_absolute_percentage_error: 18.5944\n",
      "Epoch 125/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.6380 - mean_absolute_percentage_error: 18.6380\n",
      "Epoch 126/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.5905 - mean_absolute_percentage_error: 18.5905\n",
      "Epoch 127/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.7249 - mean_absolute_percentage_error: 18.7249: 0s - loss: 18.8955 - mean_absolute_percentage_error:\n",
      "Epoch 128/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.5038 - mean_absolute_percentage_error: 18.5038\n",
      "Epoch 129/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 18.3754 - mean_absolute_percentage_error: 18.3754\n",
      "Epoch 130/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.6269 - mean_absolute_percentage_error: 18.6269\n",
      "Epoch 131/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.5991 - mean_absolute_percentage_error: 18.5991\n",
      "Epoch 132/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.6026 - mean_absolute_percentage_error: 18.6026\n",
      "Epoch 133/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 18.6543 - mean_absolute_percentage_error: 18.6543\n",
      "Epoch 134/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.6828 - mean_absolute_percentage_error: 18.6828\n",
      "Epoch 135/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.6266 - mean_absolute_percentage_error: 18.6266\n",
      "Epoch 136/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.6976 - mean_absolute_percentage_error: 18.6976\n",
      "Epoch 137/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.9033 - mean_absolute_percentage_error: 18.9033\n",
      "Epoch 138/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.8594 - mean_absolute_percentage_error: 18.8594\n",
      "Epoch 139/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.5375 - mean_absolute_percentage_error: 18.5375\n",
      "Epoch 140/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.5138 - mean_absolute_percentage_error: 18.5138\n",
      "Epoch 141/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.6323 - mean_absolute_percentage_error: 18.6323\n",
      "Epoch 142/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.5713 - mean_absolute_percentage_error: 18.5713\n",
      "Epoch 143/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.6206 - mean_absolute_percentage_error: 18.6206\n",
      "Epoch 144/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.4854 - mean_absolute_percentage_error: 18.4854\n",
      "Epoch 145/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.4442 - mean_absolute_percentage_error: 18.4442\n",
      "Epoch 146/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 18.3494 - mean_absolute_percentage_error: 18.3494\n",
      "Epoch 147/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.5230 - mean_absolute_percentage_error: 18.5230\n",
      "Epoch 148/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.4874 - mean_absolute_percentage_error: 18.4874\n",
      "Epoch 149/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.5187 - mean_absolute_percentage_error: 18.5187\n",
      "Epoch 150/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.5051 - mean_absolute_percentage_error: 18.5051\n",
      "Epoch 151/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.6312 - mean_absolute_percentage_error: 18.6312\n",
      "Epoch 152/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.4927 - mean_absolute_percentage_error: 18.4927\n",
      "Epoch 153/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.5280 - mean_absolute_percentage_error: 18.5280\n",
      "Epoch 154/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.3388 - mean_absolute_percentage_error: 18.3388\n",
      "Epoch 155/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 18.3145 - mean_absolute_percentage_error: 18.3145\n",
      "Epoch 156/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.9290 - mean_absolute_percentage_error: 18.9290\n",
      "Epoch 157/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.4495 - mean_absolute_percentage_error: 18.4495\n",
      "Epoch 158/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.5818 - mean_absolute_percentage_error: 18.5818\n",
      "Epoch 159/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.5557 - mean_absolute_percentage_error: 18.5557\n",
      "Epoch 160/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.6726 - mean_absolute_percentage_error: 18.6726\n",
      "Epoch 161/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.5148 - mean_absolute_percentage_error: 18.5148\n",
      "Epoch 162/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.5596 - mean_absolute_percentage_error: 18.5596\n",
      "Epoch 163/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.4852 - mean_absolute_percentage_error: 18.4852\n",
      "Epoch 164/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 18.4616 - mean_absolute_percentage_error: 18.4616\n",
      "Epoch 165/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.6213 - mean_absolute_percentage_error: 18.6213\n",
      "Epoch 166/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.5215 - mean_absolute_percentage_error: 18.5215\n",
      "Epoch 167/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.4551 - mean_absolute_percentage_error: 18.4551\n",
      "Epoch 168/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.6237 - mean_absolute_percentage_error: 18.6237\n",
      "Epoch 169/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.5080 - mean_absolute_percentage_error: 18.5080\n",
      "Epoch 170/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.3978 - mean_absolute_percentage_error: 18.3978\n",
      "Epoch 171/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.4229 - mean_absolute_percentage_error: 18.4229\n",
      "Epoch 172/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.7131 - mean_absolute_percentage_error: 18.7131\n",
      "Epoch 173/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.5992 - mean_absolute_percentage_error: 18.5992\n",
      "Epoch 174/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.4330 - mean_absolute_percentage_error: 18.4330\n",
      "Epoch 175/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.4971 - mean_absolute_percentage_error: 18.4971\n",
      "Epoch 176/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.6367 - mean_absolute_percentage_error: 18.6367\n",
      "Epoch 177/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.5313 - mean_absolute_percentage_error: 18.5313\n",
      "Epoch 178/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.3504 - mean_absolute_percentage_error: 18.3504\n",
      "Epoch 179/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 18.6318 - mean_absolute_percentage_error: 18.6318\n",
      "Epoch 180/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 18.4324 - mean_absolute_percentage_error: 18.4324\n",
      "Epoch 181/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.5152 - mean_absolute_percentage_error: 18.5152\n",
      "Epoch 182/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.4062 - mean_absolute_percentage_error: 18.4062\n",
      "Epoch 183/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 18.2326 - mean_absolute_percentage_error: 18.2326\n",
      "Epoch 184/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.6067 - mean_absolute_percentage_error: 18.6067\n",
      "Epoch 185/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 18.5305 - mean_absolute_percentage_error: 18.5305\n",
      "Epoch 186/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 18.5211 - mean_absolute_percentage_error: 18.5211\n",
      "Epoch 187/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "258/258 [==============================] - 0s 2ms/step - loss: 18.4152 - mean_absolute_percentage_error: 18.4152\n",
      "Epoch 188/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.3959 - mean_absolute_percentage_error: 18.3959\n",
      "Epoch 189/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.4009 - mean_absolute_percentage_error: 18.4009\n",
      "Epoch 190/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.3930 - mean_absolute_percentage_error: 18.3930\n",
      "Epoch 191/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.3445 - mean_absolute_percentage_error: 18.3445\n",
      "Epoch 192/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.3794 - mean_absolute_percentage_error: 18.3794\n",
      "Epoch 193/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.5773 - mean_absolute_percentage_error: 18.5773\n",
      "Epoch 194/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.4137 - mean_absolute_percentage_error: 18.4137\n",
      "Epoch 195/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.4174 - mean_absolute_percentage_error: 18.4174\n",
      "Epoch 196/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.2843 - mean_absolute_percentage_error: 18.2843\n",
      "Epoch 197/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.3353 - mean_absolute_percentage_error: 18.3353\n",
      "Epoch 198/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.4873 - mean_absolute_percentage_error: 18.4873\n",
      "Epoch 199/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.4001 - mean_absolute_percentage_error: 18.4001\n",
      "Epoch 200/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 18.7471 - mean_absolute_percentage_error: 18.7471\n",
      "Epoch 1/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 84.5384 - mean_absolute_percentage_error: 84.5384\n",
      "Epoch 2/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 35.2491 - mean_absolute_percentage_error: 35.2491\n",
      "Epoch 3/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 25.8521 - mean_absolute_percentage_error: 25.8521\n",
      "Epoch 4/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 23.7426 - mean_absolute_percentage_error: 23.7426\n",
      "Epoch 5/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 23.0382 - mean_absolute_percentage_error: 23.0382\n",
      "Epoch 6/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 22.5284 - mean_absolute_percentage_error: 22.5284\n",
      "Epoch 7/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 22.2104 - mean_absolute_percentage_error: 22.2104\n",
      "Epoch 8/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 22.0525 - mean_absolute_percentage_error: 22.0525\n",
      "Epoch 9/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 21.5413 - mean_absolute_percentage_error: 21.5413\n",
      "Epoch 10/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 21.4976 - mean_absolute_percentage_error: 21.4976\n",
      "Epoch 11/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 21.3237 - mean_absolute_percentage_error: 21.3237\n",
      "Epoch 12/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 20.5277 - mean_absolute_percentage_error: 20.5277\n",
      "Epoch 13/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 20.8024 - mean_absolute_percentage_error: 20.8024\n",
      "Epoch 14/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 20.3473 - mean_absolute_percentage_error: 20.3473\n",
      "Epoch 15/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 20.5596 - mean_absolute_percentage_error: 20.5596\n",
      "Epoch 16/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 20.6344 - mean_absolute_percentage_error: 20.6344\n",
      "Epoch 17/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 20.5172 - mean_absolute_percentage_error: 20.5172\n",
      "Epoch 18/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 20.2981 - mean_absolute_percentage_error: 20.2981\n",
      "Epoch 19/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 20.7878 - mean_absolute_percentage_error: 20.7878\n",
      "Epoch 20/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 20.3995 - mean_absolute_percentage_error: 20.3995\n",
      "Epoch 21/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 20.2162 - mean_absolute_percentage_error: 20.2162\n",
      "Epoch 22/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 20.2508 - mean_absolute_percentage_error: 20.2508\n",
      "Epoch 23/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.9736 - mean_absolute_percentage_error: 19.9736\n",
      "Epoch 24/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 20.2406 - mean_absolute_percentage_error: 20.2406\n",
      "Epoch 25/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 20.1299 - mean_absolute_percentage_error: 20.1299\n",
      "Epoch 26/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.9159 - mean_absolute_percentage_error: 19.9159\n",
      "Epoch 27/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 20.1650 - mean_absolute_percentage_error: 20.1650\n",
      "Epoch 28/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 20.0463 - mean_absolute_percentage_error: 20.0463\n",
      "Epoch 29/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 19.6617 - mean_absolute_percentage_error: 19.6617\n",
      "Epoch 30/200\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 19.9224 - mean_absolute_percentage_error: 19.9224\n",
      "Epoch 31/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 20.0638 - mean_absolute_percentage_error: 20.0638\n",
      "Epoch 32/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.6336 - mean_absolute_percentage_error: 19.6336\n",
      "Epoch 33/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.5545 - mean_absolute_percentage_error: 19.5545\n",
      "Epoch 34/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.6232 - mean_absolute_percentage_error: 19.6232\n",
      "Epoch 35/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.6631 - mean_absolute_percentage_error: 19.6631\n",
      "Epoch 36/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.9583 - mean_absolute_percentage_error: 19.9583\n",
      "Epoch 37/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.7699 - mean_absolute_percentage_error: 19.7699\n",
      "Epoch 38/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.9736 - mean_absolute_percentage_error: 19.9736\n",
      "Epoch 39/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.6940 - mean_absolute_percentage_error: 19.6940\n",
      "Epoch 40/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.5892 - mean_absolute_percentage_error: 19.5892\n",
      "Epoch 41/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.3976 - mean_absolute_percentage_error: 19.3976\n",
      "Epoch 42/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.3097 - mean_absolute_percentage_error: 19.3097\n",
      "Epoch 43/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.5745 - mean_absolute_percentage_error: 19.5745\n",
      "Epoch 44/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.6255 - mean_absolute_percentage_error: 19.6255\n",
      "Epoch 45/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.5602 - mean_absolute_percentage_error: 19.5602\n",
      "Epoch 46/200\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 19.9604 - mean_absolute_percentage_error: 19.9604\n",
      "Epoch 47/200\n",
      "222/258 [========================>.....] - ETA: 0s - loss: 19.4725 - mean_absolute_percentage_error: 19.4725"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-b9cf9505df0e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtuner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\tuk\\lib\\site-packages\\keras_tuner\\engine\\base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[1;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m             \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m             \u001b[1;31m# `results` is None indicates user updated oracle in `run_trial()`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresults\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tuk\\lib\\site-packages\\keras_tuner\\engine\\tuner.py\u001b[0m in \u001b[0;36mrun_trial\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    302\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m             \u001b[0mcopied_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"callbacks\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 304\u001b[1;33m             \u001b[0mobj_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_and_fit_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcopied_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m             \u001b[1;31m# objective left unspecified,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tuk\\lib\\site-packages\\keras_tuner\\engine\\tuner.py\u001b[0m in \u001b[0;36m_build_and_fit_model\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[0mhp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_try_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrun_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tuk\\lib\\site-packages\\keras_tuner\\engine\\hypermodel.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[0mIf\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mit\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \"\"\"\n\u001b[1;32m--> 137\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tuk\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tuk\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tuk\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tuk\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tuk\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tuk\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tuk\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tuk\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tuk\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tuner.search(X_train, Y_train, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in project1\\fraction_of_coating_40_50\n",
      "Showing 10 best trials\n",
      "Objective(name='mean_absolute_percentage_error', direction='min')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 3\n",
      "units_0: 224\n",
      "learning_rate: 0.0001\n",
      "units_1: 32\n",
      "units_2: 192\n",
      "units_3: 96\n",
      "units_4: 128\n",
      "units_5: 224\n",
      "units_6: 224\n",
      "units_7: 128\n",
      "units_8: 64\n",
      "Score: 2.6286720434824624\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 6\n",
      "units_0: 64\n",
      "learning_rate: 0.0001\n",
      "units_1: 128\n",
      "units_2: 160\n",
      "units_3: 192\n",
      "units_4: 224\n",
      "units_5: 224\n",
      "units_6: 224\n",
      "units_7: 128\n",
      "units_8: 224\n",
      "Score: 5.309459368387858\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 6\n",
      "units_0: 160\n",
      "learning_rate: 0.01\n",
      "units_1: 96\n",
      "units_2: 224\n",
      "units_3: 32\n",
      "units_4: 192\n",
      "units_5: 96\n",
      "units_6: 96\n",
      "units_7: 192\n",
      "units_8: 160\n",
      "Score: 19.752186457316082\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 9\n",
      "units_0: 160\n",
      "learning_rate: 0.01\n",
      "units_1: 224\n",
      "units_2: 96\n",
      "units_3: 32\n",
      "units_4: 32\n",
      "units_5: 32\n",
      "units_6: 32\n",
      "units_7: 32\n",
      "units_8: 32\n",
      "Score: 23.124784469604492\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 9\n",
      "units_0: 192\n",
      "learning_rate: 0.01\n",
      "units_1: 160\n",
      "units_2: 32\n",
      "units_3: 64\n",
      "units_4: 64\n",
      "units_5: 160\n",
      "units_6: 128\n",
      "units_7: 32\n",
      "units_8: 160\n",
      "Score: 43.62338511149088\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 192)               1728      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 224)               43232     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                7200      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 192)               6336      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 96)                18528     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               12416     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 224)               28896     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 224)               50400     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               28800     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 205,987\n",
      "Trainable params: 205,987\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NN_model2 = Sequential()\n",
    "\n",
    "# The Input Layer :\n",
    "NN_model2.add(Dense(192, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n",
    "\n",
    "# The Hidden Layers :\n",
    "NN_model2.add(Dense(224, kernel_initializer='normal',activation='relu'))\n",
    "NN_model2.add(Dense(32, kernel_initializer='normal',activation='relu'))\n",
    "NN_model2.add(Dense(192, kernel_initializer='normal',activation='relu'))\n",
    "NN_model2.add(Dense(96, kernel_initializer='normal',activation='relu'))\n",
    "NN_model2.add(Dense(128, kernel_initializer='normal',activation='relu'))\n",
    "NN_model2.add(Dense(224, kernel_initializer='normal',activation='relu'))\n",
    "NN_model2.add(Dense(224, kernel_initializer='normal',activation='relu'))\n",
    "NN_model2.add(Dense(128, kernel_initializer='normal',activation='relu'))\n",
    "NN_model2.add(Dense(64, kernel_initializer='normal',activation='relu'))\n",
    "\n",
    "# The Output Layer :\n",
    "NN_model2.add(Dense(3, kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "\n",
    "# Compile the network :\n",
    "NN_model2.compile(loss='mean_absolute_percentage_error', optimizer='adam', metrics=['accuracy'])\n",
    "NN_model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"fraction_of_coating_40_50/Weights-{epoch:03d}--{val_loss:.5f}.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, verbose=1, monitor='val_loss',save_best_only=True, mode='auto')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # patient early stopping\n",
    "es = EarlyStopping(monitor='val_loss', patience=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_csv=CSVLogger('fraction_of_coating_40_50_loss_logs.csv', separator=',', append=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_list=[checkpoint, es, log_csv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "200/207 [===========================>..] - ETA: 0s - loss: 48.5396 - accuracy: 0.8350\n",
      "Epoch 00001: val_loss improved from inf to 25.94093, saving model to fraction_of_coating_40_50\\Weights-001--25.94093.hdf5\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 47.9577 - accuracy: 0.8355 - val_loss: 25.9409 - val_accuracy: 0.9618\n",
      "Epoch 2/500\n",
      "199/207 [===========================>..] - ETA: 0s - loss: 26.0744 - accuracy: 0.8552\n",
      "Epoch 00002: val_loss did not improve from 25.94093\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 26.2169 - accuracy: 0.8564 - val_loss: 32.7970 - val_accuracy: 0.9618\n",
      "Epoch 3/500\n",
      "194/207 [===========================>..] - ETA: 0s - loss: 25.4433 - accuracy: 0.8574\n",
      "Epoch 00003: val_loss improved from 25.94093 to 25.52165, saving model to fraction_of_coating_40_50\\Weights-003--25.52165.hdf5\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 25.4562 - accuracy: 0.8564 - val_loss: 25.5216 - val_accuracy: 0.9618\n",
      "Epoch 4/500\n",
      "186/207 [=========================>....] - ETA: 0s - loss: 23.9499 - accuracy: 0.8560\n",
      "Epoch 00004: val_loss improved from 25.52165 to 23.44154, saving model to fraction_of_coating_40_50\\Weights-004--23.44154.hdf5\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 23.9327 - accuracy: 0.8564 - val_loss: 23.4415 - val_accuracy: 0.9618\n",
      "Epoch 5/500\n",
      "190/207 [==========================>...] - ETA: 0s - loss: 23.3826 - accuracy: 0.8541\n",
      "Epoch 00005: val_loss did not improve from 23.44154\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 23.1913 - accuracy: 0.8564 - val_loss: 25.7003 - val_accuracy: 0.9618\n",
      "Epoch 6/500\n",
      "196/207 [===========================>..] - ETA: 0s - loss: 23.2784 - accuracy: 0.8565\n",
      "Epoch 00006: val_loss did not improve from 23.44154\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 23.2425 - accuracy: 0.8564 - val_loss: 25.9083 - val_accuracy: 0.9618\n",
      "Epoch 7/500\n",
      "185/207 [=========================>....] - ETA: 0s - loss: 22.5578 - accuracy: 0.8551\n",
      "Epoch 00007: val_loss improved from 23.44154 to 23.41262, saving model to fraction_of_coating_40_50\\Weights-007--23.41262.hdf5\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 22.3355 - accuracy: 0.8564 - val_loss: 23.4126 - val_accuracy: 0.9618\n",
      "Epoch 8/500\n",
      "201/207 [============================>.] - ETA: 0s - loss: 21.8957 - accuracy: 0.8560\n",
      "Epoch 00008: val_loss did not improve from 23.41262\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 21.8979 - accuracy: 0.8564 - val_loss: 24.4148 - val_accuracy: 0.9618\n",
      "Epoch 9/500\n",
      "197/207 [===========================>..] - ETA: 0s - loss: 21.3800 - accuracy: 0.8569\n",
      "Epoch 00009: val_loss improved from 23.41262 to 22.58067, saving model to fraction_of_coating_40_50\\Weights-009--22.58067.hdf5\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 21.3993 - accuracy: 0.8564 - val_loss: 22.5807 - val_accuracy: 0.9618\n",
      "Epoch 10/500\n",
      "195/207 [===========================>..] - ETA: 0s - loss: 18.5690 - accuracy: 0.9053\n",
      "Epoch 00010: val_loss improved from 22.58067 to 21.45832, saving model to fraction_of_coating_40_50\\Weights-010--21.45832.hdf5\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 18.3120 - accuracy: 0.9078 - val_loss: 21.4583 - val_accuracy: 0.9667\n",
      "Epoch 11/500\n",
      "198/207 [===========================>..] - ETA: 0s - loss: 12.1951 - accuracy: 0.9664\n",
      "Epoch 00011: val_loss improved from 21.45832 to 19.22277, saving model to fraction_of_coating_40_50\\Weights-011--19.22277.hdf5\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 12.2022 - accuracy: 0.9665 - val_loss: 19.2228 - val_accuracy: 0.9194\n",
      "Epoch 12/500\n",
      "199/207 [===========================>..] - ETA: 0s - loss: 10.6979 - accuracy: 0.9684\n",
      "Epoch 00012: val_loss improved from 19.22277 to 16.73621, saving model to fraction_of_coating_40_50\\Weights-012--16.73621.hdf5\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 10.7115 - accuracy: 0.9686 - val_loss: 16.7362 - val_accuracy: 0.9424\n",
      "Epoch 13/500\n",
      "191/207 [==========================>...] - ETA: 0s - loss: 8.5988 - accuracy: 0.9674\n",
      "Epoch 00013: val_loss improved from 16.73621 to 15.23354, saving model to fraction_of_coating_40_50\\Weights-013--15.23354.hdf5\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 8.5553 - accuracy: 0.9685 - val_loss: 15.2335 - val_accuracy: 0.9206\n",
      "Epoch 14/500\n",
      "194/207 [===========================>..] - ETA: 0s - loss: 8.8716 - accuracy: 0.9699\n",
      "Epoch 00014: val_loss improved from 15.23354 to 14.50713, saving model to fraction_of_coating_40_50\\Weights-014--14.50713.hdf5\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 8.7923 - accuracy: 0.9698 - val_loss: 14.5071 - val_accuracy: 0.9352\n",
      "Epoch 15/500\n",
      "190/207 [==========================>...] - ETA: 0s - loss: 8.8869 - accuracy: 0.9748\n",
      "Epoch 00015: val_loss improved from 14.50713 to 13.32109, saving model to fraction_of_coating_40_50\\Weights-015--13.32109.hdf5\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 8.8028 - accuracy: 0.9748 - val_loss: 13.3211 - val_accuracy: 0.9339\n",
      "Epoch 16/500\n",
      "204/207 [============================>.] - ETA: 0s - loss: 8.5447 - accuracy: 0.9709\n",
      "Epoch 00016: val_loss improved from 13.32109 to 12.74043, saving model to fraction_of_coating_40_50\\Weights-016--12.74043.hdf5\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 8.5432 - accuracy: 0.9710 - val_loss: 12.7404 - val_accuracy: 0.9327\n",
      "Epoch 17/500\n",
      "195/207 [===========================>..] - ETA: 0s - loss: 8.4217 - accuracy: 0.9712\n",
      "Epoch 00017: val_loss did not improve from 12.74043\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 8.6424 - accuracy: 0.9710 - val_loss: 13.7104 - val_accuracy: 0.9758\n",
      "Epoch 18/500\n",
      "193/207 [==========================>...] - ETA: 0s - loss: 8.6736 - accuracy: 0.9700\n",
      "Epoch 00018: val_loss improved from 12.74043 to 11.85502, saving model to fraction_of_coating_40_50\\Weights-018--11.85502.hdf5\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 8.6060 - accuracy: 0.9703 - val_loss: 11.8550 - val_accuracy: 0.9358\n",
      "Epoch 19/500\n",
      "199/207 [===========================>..] - ETA: 0s - loss: 8.1520 - accuracy: 0.9711\n",
      "Epoch 00019: val_loss improved from 11.85502 to 11.78724, saving model to fraction_of_coating_40_50\\Weights-019--11.78724.hdf5\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 8.1249 - accuracy: 0.9710 - val_loss: 11.7872 - val_accuracy: 0.9588\n",
      "Epoch 20/500\n",
      "194/207 [===========================>..] - ETA: 0s - loss: 7.2390 - accuracy: 0.9705\n",
      "Epoch 00020: val_loss did not improve from 11.78724\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 7.2212 - accuracy: 0.9704 - val_loss: 12.2335 - val_accuracy: 0.9721\n",
      "Epoch 21/500\n",
      "186/207 [=========================>....] - ETA: 0s - loss: 7.6146 - accuracy: 0.9706\n",
      "Epoch 00021: val_loss did not improve from 11.78724\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 7.5736 - accuracy: 0.9710 - val_loss: 11.9212 - val_accuracy: 0.9200\n",
      "Epoch 22/500\n",
      "191/207 [==========================>...] - ETA: 0s - loss: 7.0767 - accuracy: 0.9737\n",
      "Epoch 00022: val_loss did not improve from 11.78724\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 7.1572 - accuracy: 0.9727 - val_loss: 12.3854 - val_accuracy: 0.9206\n",
      "Epoch 23/500\n",
      "198/207 [===========================>..] - ETA: 0s - loss: 7.3567 - accuracy: 0.9738\n",
      "Epoch 00023: val_loss did not improve from 11.78724\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 7.3727 - accuracy: 0.9733 - val_loss: 12.5481 - val_accuracy: 0.9267\n",
      "Epoch 24/500\n",
      "195/207 [===========================>..] - ETA: 0s - loss: 7.0845 - accuracy: 0.9715\n",
      "Epoch 00024: val_loss improved from 11.78724 to 10.21615, saving model to fraction_of_coating_40_50\\Weights-024--10.21615.hdf5\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 7.0331 - accuracy: 0.9720 - val_loss: 10.2162 - val_accuracy: 0.9182\n",
      "Epoch 25/500\n",
      "196/207 [===========================>..] - ETA: 0s - loss: 6.6668 - accuracy: 0.9721\n",
      "Epoch 00025: val_loss did not improve from 10.21615\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 6.7352 - accuracy: 0.9724 - val_loss: 12.2412 - val_accuracy: 0.9461\n",
      "Epoch 26/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191/207 [==========================>...] - ETA: 0s - loss: 6.1366 - accuracy: 0.9756\n",
      "Epoch 00026: val_loss did not improve from 10.21615\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 6.1580 - accuracy: 0.9748 - val_loss: 13.5993 - val_accuracy: 0.9509\n",
      "Epoch 27/500\n",
      "201/207 [============================>.] - ETA: 0s - loss: 6.7395 - accuracy: 0.9787\n",
      "Epoch 00027: val_loss did not improve from 10.21615\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 6.7309 - accuracy: 0.9786 - val_loss: 13.3427 - val_accuracy: 0.9303\n",
      "Epoch 28/500\n",
      "185/207 [=========================>....] - ETA: 0s - loss: 5.8201 - accuracy: 0.9848\n",
      "Epoch 00028: val_loss did not improve from 10.21615\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 5.7742 - accuracy: 0.9848 - val_loss: 12.2184 - val_accuracy: 0.9709\n",
      "Epoch 29/500\n",
      "199/207 [===========================>..] - ETA: 0s - loss: 5.0827 - accuracy: 0.9881\n",
      "Epoch 00029: val_loss did not improve from 10.21615\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 5.0955 - accuracy: 0.9880 - val_loss: 10.4592 - val_accuracy: 0.9818\n",
      "Epoch 30/500\n",
      "192/207 [==========================>...] - ETA: 0s - loss: 4.8925 - accuracy: 0.9886\n",
      "Epoch 00030: val_loss improved from 10.21615 to 7.78849, saving model to fraction_of_coating_40_50\\Weights-030--7.78849.hdf5\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 4.8253 - accuracy: 0.9888 - val_loss: 7.7885 - val_accuracy: 0.9873\n",
      "Epoch 31/500\n",
      "189/207 [==========================>...] - ETA: 0s - loss: 4.6953 - accuracy: 0.9901\n",
      "Epoch 00031: val_loss did not improve from 7.78849\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 4.8218 - accuracy: 0.9898 - val_loss: 9.6505 - val_accuracy: 0.9824\n",
      "Epoch 32/500\n",
      "205/207 [============================>.] - ETA: 0s - loss: 4.9851 - accuracy: 0.9907\n",
      "Epoch 00032: val_loss did not improve from 7.78849\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 4.9801 - accuracy: 0.9906 - val_loss: 8.8385 - val_accuracy: 0.9830\n",
      "Epoch 33/500\n",
      "190/207 [==========================>...] - ETA: 0s - loss: 4.4498 - accuracy: 0.9921\n",
      "Epoch 00033: val_loss did not improve from 7.78849\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 4.4213 - accuracy: 0.9921 - val_loss: 8.0838 - val_accuracy: 0.9770\n",
      "Epoch 34/500\n",
      "183/207 [=========================>....] - ETA: 0s - loss: 4.5609 - accuracy: 0.9920\n",
      "Epoch 00034: val_loss improved from 7.78849 to 7.52397, saving model to fraction_of_coating_40_50\\Weights-034--7.52397.hdf5\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 4.5083 - accuracy: 0.9918 - val_loss: 7.5240 - val_accuracy: 0.9830\n",
      "Epoch 35/500\n",
      "187/207 [==========================>...] - ETA: 0s - loss: 4.4748 - accuracy: 0.9925\n",
      "Epoch 00035: val_loss improved from 7.52397 to 6.80784, saving model to fraction_of_coating_40_50\\Weights-035--6.80784.hdf5\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 4.3754 - accuracy: 0.9917 - val_loss: 6.8078 - val_accuracy: 0.9873\n",
      "Epoch 36/500\n",
      "197/207 [===========================>..] - ETA: 0s - loss: 3.7509 - accuracy: 0.9921\n",
      "Epoch 00036: val_loss did not improve from 6.80784\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 3.8128 - accuracy: 0.9920 - val_loss: 11.4411 - val_accuracy: 0.9788\n",
      "Epoch 37/500\n",
      "183/207 [=========================>....] - ETA: 0s - loss: 4.7429 - accuracy: 0.9886\n",
      "Epoch 00037: val_loss improved from 6.80784 to 6.79428, saving model to fraction_of_coating_40_50\\Weights-037--6.79428.hdf5\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 4.5977 - accuracy: 0.9894 - val_loss: 6.7943 - val_accuracy: 0.9873\n",
      "Epoch 38/500\n",
      "195/207 [===========================>..] - ETA: 0s - loss: 4.2351 - accuracy: 0.9902\n",
      "Epoch 00038: val_loss improved from 6.79428 to 6.67578, saving model to fraction_of_coating_40_50\\Weights-038--6.67578.hdf5\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 4.1820 - accuracy: 0.9901 - val_loss: 6.6758 - val_accuracy: 0.9848\n",
      "Epoch 39/500\n",
      "188/207 [==========================>...] - ETA: 0s - loss: 4.2057 - accuracy: 0.9895\n",
      "Epoch 00039: val_loss did not improve from 6.67578\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 4.1793 - accuracy: 0.9895 - val_loss: 9.1620 - val_accuracy: 0.9727\n",
      "Epoch 40/500\n",
      "207/207 [==============================] - ETA: 0s - loss: 3.7375 - accuracy: 0.9944\n",
      "Epoch 00040: val_loss did not improve from 6.67578\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 3.7375 - accuracy: 0.9944 - val_loss: 10.2593 - val_accuracy: 0.9921\n",
      "Epoch 41/500\n",
      "197/207 [===========================>..] - ETA: 0s - loss: 4.2323 - accuracy: 0.9924\n",
      "Epoch 00041: val_loss did not improve from 6.67578\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 4.2424 - accuracy: 0.9927 - val_loss: 7.0376 - val_accuracy: 0.9745\n",
      "Epoch 42/500\n",
      "197/207 [===========================>..] - ETA: 0s - loss: 4.3091 - accuracy: 0.9918\n",
      "Epoch 00042: val_loss did not improve from 6.67578\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 4.2768 - accuracy: 0.9920 - val_loss: 7.1609 - val_accuracy: 0.9836\n",
      "Epoch 43/500\n",
      "205/207 [============================>.] - ETA: 0s - loss: 3.9020 - accuracy: 0.9918\n",
      "Epoch 00043: val_loss did not improve from 6.67578\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 3.8979 - accuracy: 0.9917 - val_loss: 7.3918 - val_accuracy: 0.9903\n",
      "Epoch 44/500\n",
      "188/207 [==========================>...] - ETA: 0s - loss: 3.8381 - accuracy: 0.9922\n",
      "Epoch 00044: val_loss did not improve from 6.67578\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 3.8008 - accuracy: 0.9923 - val_loss: 9.1959 - val_accuracy: 0.9945\n",
      "Epoch 45/500\n",
      "189/207 [==========================>...] - ETA: 0s - loss: 3.8662 - accuracy: 0.9906\n",
      "Epoch 00045: val_loss did not improve from 6.67578\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 3.8763 - accuracy: 0.9905 - val_loss: 8.4202 - val_accuracy: 0.9903\n",
      "Epoch 46/500\n",
      "198/207 [===========================>..] - ETA: 0s - loss: 4.1251 - accuracy: 0.9915\n",
      "Epoch 00046: val_loss did not improve from 6.67578\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 4.1255 - accuracy: 0.9915 - val_loss: 8.3571 - val_accuracy: 0.9824\n",
      "Epoch 47/500\n",
      "190/207 [==========================>...] - ETA: 0s - loss: 4.0779 - accuracy: 0.9918\n",
      "Epoch 00047: val_loss did not improve from 6.67578\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 4.0368 - accuracy: 0.9917 - val_loss: 7.2515 - val_accuracy: 0.9721\n",
      "Epoch 48/500\n",
      "191/207 [==========================>...] - ETA: 0s - loss: 4.0305 - accuracy: 0.9920\n",
      "Epoch 00048: val_loss did not improve from 6.67578\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 4.0522 - accuracy: 0.9921 - val_loss: 8.4532 - val_accuracy: 0.9879\n",
      "Epoch 49/500\n",
      "191/207 [==========================>...] - ETA: 0s - loss: 3.9322 - accuracy: 0.9907\n",
      "Epoch 00049: val_loss improved from 6.67578 to 6.52230, saving model to fraction_of_coating_40_50\\Weights-049--6.52230.hdf5\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 3.9213 - accuracy: 0.9911 - val_loss: 6.5223 - val_accuracy: 0.9570\n",
      "Epoch 50/500\n",
      "193/207 [==========================>...] - ETA: 0s - loss: 3.9647 - accuracy: 0.9924\n",
      "Epoch 00050: val_loss did not improve from 6.52230\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 3.9284 - accuracy: 0.9927 - val_loss: 7.5093 - val_accuracy: 0.9448\n",
      "Epoch 51/500\n",
      "197/207 [===========================>..] - ETA: 0s - loss: 3.7813 - accuracy: 0.9914\n",
      "Epoch 00051: val_loss did not improve from 6.52230\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 3.7980 - accuracy: 0.9915 - val_loss: 7.5336 - val_accuracy: 0.9939\n",
      "Epoch 52/500\n",
      "201/207 [============================>.] - ETA: 0s - loss: 4.4035 - accuracy: 0.9907\n",
      "Epoch 00052: val_loss improved from 6.52230 to 6.35743, saving model to fraction_of_coating_40_50\\Weights-052--6.35743.hdf5\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 4.3916 - accuracy: 0.9908 - val_loss: 6.3574 - val_accuracy: 0.9685\n",
      "Epoch 53/500\n",
      "188/207 [==========================>...] - ETA: 0s - loss: 3.7893 - accuracy: 0.9932\n",
      "Epoch 00053: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 3.8291 - accuracy: 0.9933 - val_loss: 9.8720 - val_accuracy: 0.9903\n",
      "Epoch 54/500\n",
      "199/207 [===========================>..] - ETA: 0s - loss: 3.8180 - accuracy: 0.9939\n",
      "Epoch 00054: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 3.8221 - accuracy: 0.9938 - val_loss: 9.6659 - val_accuracy: 0.9776\n",
      "Epoch 55/500\n",
      "203/207 [============================>.] - ETA: 0s - loss: 3.6964 - accuracy: 0.9928\n",
      "Epoch 00055: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 3.7148 - accuracy: 0.9929 - val_loss: 9.4938 - val_accuracy: 0.9927\n",
      "Epoch 56/500\n",
      "187/207 [==========================>...] - ETA: 0s - loss: 3.6260 - accuracy: 0.9935\n",
      "Epoch 00056: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 3.6654 - accuracy: 0.9938 - val_loss: 8.8702 - val_accuracy: 0.9903\n",
      "Epoch 57/500\n",
      "189/207 [==========================>...] - ETA: 0s - loss: 3.8202 - accuracy: 0.9929\n",
      "Epoch 00057: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 3.8457 - accuracy: 0.9932 - val_loss: 7.7862 - val_accuracy: 0.9812\n",
      "Epoch 58/500\n",
      "196/207 [===========================>..] - ETA: 0s - loss: 4.1210 - accuracy: 0.9935\n",
      "Epoch 00058: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 4.0799 - accuracy: 0.9935 - val_loss: 6.7433 - val_accuracy: 0.9764\n",
      "Epoch 59/500\n",
      "191/207 [==========================>...] - ETA: 0s - loss: 3.7835 - accuracy: 0.9949\n",
      "Epoch 00059: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 3.8169 - accuracy: 0.9947 - val_loss: 8.6129 - val_accuracy: 0.9745\n",
      "Epoch 60/500\n",
      "192/207 [==========================>...] - ETA: 0s - loss: 3.6827 - accuracy: 0.9930\n",
      "Epoch 00060: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 3.6531 - accuracy: 0.9930 - val_loss: 9.0111 - val_accuracy: 0.9848\n",
      "Epoch 61/500\n",
      "195/207 [===========================>..] - ETA: 0s - loss: 3.7913 - accuracy: 0.9942\n",
      "Epoch 00061: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 3.8460 - accuracy: 0.9935 - val_loss: 7.5774 - val_accuracy: 0.9879\n",
      "Epoch 62/500\n",
      "188/207 [==========================>...] - ETA: 0s - loss: 3.2878 - accuracy: 0.9947\n",
      "Epoch 00062: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 3.3531 - accuracy: 0.9947 - val_loss: 7.6155 - val_accuracy: 0.9758\n",
      "Epoch 63/500\n",
      "192/207 [==========================>...] - ETA: 0s - loss: 3.6476 - accuracy: 0.9932\n",
      "Epoch 00063: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 3.6396 - accuracy: 0.9932 - val_loss: 6.8463 - val_accuracy: 0.9818\n",
      "Epoch 64/500\n",
      "205/207 [============================>.] - ETA: 0s - loss: 3.8526 - accuracy: 0.9945\n",
      "Epoch 00064: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 3.8441 - accuracy: 0.9945 - val_loss: 7.8767 - val_accuracy: 0.9752\n",
      "Epoch 65/500\n",
      "187/207 [==========================>...] - ETA: 0s - loss: 3.5256 - accuracy: 0.9933\n",
      "Epoch 00065: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 3.5080 - accuracy: 0.9935 - val_loss: 7.7301 - val_accuracy: 0.9679\n",
      "Epoch 66/500\n",
      "190/207 [==========================>...] - ETA: 0s - loss: 4.0326 - accuracy: 0.9926\n",
      "Epoch 00066: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 3.9888 - accuracy: 0.9927 - val_loss: 9.0636 - val_accuracy: 0.9770\n",
      "Epoch 67/500\n",
      "193/207 [==========================>...] - ETA: 0s - loss: 3.3803 - accuracy: 0.9922\n",
      "Epoch 00067: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 3.4622 - accuracy: 0.9924 - val_loss: 8.4618 - val_accuracy: 0.9764\n",
      "Epoch 68/500\n",
      "192/207 [==========================>...] - ETA: 0s - loss: 3.6541 - accuracy: 0.9930\n",
      "Epoch 00068: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 3.6727 - accuracy: 0.9924 - val_loss: 7.5109 - val_accuracy: 0.9861\n",
      "Epoch 69/500\n",
      "190/207 [==========================>...] - ETA: 0s - loss: 3.3304 - accuracy: 0.9928\n",
      "Epoch 00069: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 3.3022 - accuracy: 0.9929 - val_loss: 7.1948 - val_accuracy: 0.9952\n",
      "Epoch 70/500\n",
      "207/207 [==============================] - ETA: 0s - loss: 3.4015 - accuracy: 0.9915\n",
      "Epoch 00070: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 3.4015 - accuracy: 0.9915 - val_loss: 6.9084 - val_accuracy: 0.9830\n",
      "Epoch 71/500\n",
      "205/207 [============================>.] - ETA: 0s - loss: 4.0827 - accuracy: 0.9933\n",
      "Epoch 00071: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 4.0764 - accuracy: 0.9933 - val_loss: 7.3136 - val_accuracy: 0.9794\n",
      "Epoch 72/500\n",
      "196/207 [===========================>..] - ETA: 0s - loss: 3.6764 - accuracy: 0.9930\n",
      "Epoch 00072: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 3.6693 - accuracy: 0.9932 - val_loss: 7.0529 - val_accuracy: 0.9836\n",
      "Epoch 73/500\n",
      "205/207 [============================>.] - ETA: 0s - loss: 3.3334 - accuracy: 0.9933\n",
      "Epoch 00073: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 3.3325 - accuracy: 0.9932 - val_loss: 7.7037 - val_accuracy: 0.9830\n",
      "Epoch 74/500\n",
      "191/207 [==========================>...] - ETA: 0s - loss: 3.4915 - accuracy: 0.9939\n",
      "Epoch 00074: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 3.4783 - accuracy: 0.9941 - val_loss: 8.8742 - val_accuracy: 0.9915\n",
      "Epoch 75/500\n",
      "191/207 [==========================>...] - ETA: 0s - loss: 3.4197 - accuracy: 0.9925\n",
      "Epoch 00075: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 3.4552 - accuracy: 0.9926 - val_loss: 8.5802 - val_accuracy: 0.9782\n",
      "Epoch 76/500\n",
      "197/207 [===========================>..] - ETA: 0s - loss: 3.5907 - accuracy: 0.9924\n",
      "Epoch 00076: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 3.5536 - accuracy: 0.9926 - val_loss: 7.7225 - val_accuracy: 0.9903\n",
      "Epoch 77/500\n",
      "194/207 [===========================>..] - ETA: 0s - loss: 3.4147 - accuracy: 0.9940\n",
      "Epoch 00077: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 3.3972 - accuracy: 0.9941 - val_loss: 8.3389 - val_accuracy: 0.9812\n",
      "Epoch 78/500\n",
      "199/207 [===========================>..] - ETA: 0s - loss: 3.4439 - accuracy: 0.9929\n",
      "Epoch 00078: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 3.4151 - accuracy: 0.9929 - val_loss: 9.3656 - val_accuracy: 0.9697\n",
      "Epoch 79/500\n",
      "203/207 [============================>.] - ETA: 0s - loss: 3.0962 - accuracy: 0.9938\n",
      "Epoch 00079: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 3.0987 - accuracy: 0.9939 - val_loss: 8.6969 - val_accuracy: 0.9909\n",
      "Epoch 80/500\n",
      "189/207 [==========================>...] - ETA: 0s - loss: 3.0829 - accuracy: 0.9939\n",
      "Epoch 00080: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 3.1102 - accuracy: 0.9939 - val_loss: 9.2781 - val_accuracy: 0.9812\n",
      "Epoch 81/500\n",
      "205/207 [============================>.] - ETA: 0s - loss: 3.1097 - accuracy: 0.9960\n",
      "Epoch 00081: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 3.1108 - accuracy: 0.9961 - val_loss: 7.7295 - val_accuracy: 0.9794\n",
      "Epoch 82/500\n",
      "206/207 [============================>.] - ETA: 0s - loss: 3.3267 - accuracy: 0.9927\n",
      "Epoch 00082: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 3.3253 - accuracy: 0.9927 - val_loss: 7.7669 - val_accuracy: 0.9739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/500\n",
      "187/207 [==========================>...] - ETA: 0s - loss: 3.4489 - accuracy: 0.9913\n",
      "Epoch 00083: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 3.4918 - accuracy: 0.9914 - val_loss: 8.7205 - val_accuracy: 0.9709\n",
      "Epoch 84/500\n",
      "190/207 [==========================>...] - ETA: 0s - loss: 3.5137 - accuracy: 0.9936\n",
      "Epoch 00084: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 3.4474 - accuracy: 0.9938 - val_loss: 6.8910 - val_accuracy: 0.9770\n",
      "Epoch 85/500\n",
      "187/207 [==========================>...] - ETA: 0s - loss: 3.1317 - accuracy: 0.9928\n",
      "Epoch 00085: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 3.1155 - accuracy: 0.9933 - val_loss: 7.0707 - val_accuracy: 0.9939\n",
      "Epoch 86/500\n",
      "197/207 [===========================>..] - ETA: 0s - loss: 3.8046 - accuracy: 0.9933\n",
      "Epoch 00086: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 3.7774 - accuracy: 0.9933 - val_loss: 10.5653 - val_accuracy: 0.9788\n",
      "Epoch 87/500\n",
      "183/207 [=========================>....] - ETA: 0s - loss: 3.5141 - accuracy: 0.9920\n",
      "Epoch 00087: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 3.5660 - accuracy: 0.9924 - val_loss: 8.4948 - val_accuracy: 0.9909\n",
      "Epoch 88/500\n",
      "186/207 [=========================>....] - ETA: 0s - loss: 2.9431 - accuracy: 0.9960\n",
      "Epoch 00088: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 2.9563 - accuracy: 0.9959 - val_loss: 7.3711 - val_accuracy: 0.9836\n",
      "Epoch 89/500\n",
      "185/207 [=========================>....] - ETA: 0s - loss: 3.3142 - accuracy: 0.9936\n",
      "Epoch 00089: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 3.3522 - accuracy: 0.9936 - val_loss: 8.7104 - val_accuracy: 0.9915\n",
      "Epoch 90/500\n",
      "204/207 [============================>.] - ETA: 0s - loss: 3.0164 - accuracy: 0.9949\n",
      "Epoch 00090: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 3.0185 - accuracy: 0.9950 - val_loss: 7.1685 - val_accuracy: 0.9909\n",
      "Epoch 91/500\n",
      "201/207 [============================>.] - ETA: 0s - loss: 3.3536 - accuracy: 0.9924\n",
      "Epoch 00091: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 3.3416 - accuracy: 0.9923 - val_loss: 7.1868 - val_accuracy: 0.9903\n",
      "Epoch 92/500\n",
      "199/207 [===========================>..] - ETA: 0s - loss: 3.3071 - accuracy: 0.9942\n",
      "Epoch 00092: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 3.3395 - accuracy: 0.9944 - val_loss: 9.7493 - val_accuracy: 0.9921\n",
      "Epoch 93/500\n",
      "198/207 [===========================>..] - ETA: 0s - loss: 3.3489 - accuracy: 0.9929\n",
      "Epoch 00093: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 3.3542 - accuracy: 0.9930 - val_loss: 7.1141 - val_accuracy: 0.9758\n",
      "Epoch 94/500\n",
      "192/207 [==========================>...] - ETA: 0s - loss: 3.1902 - accuracy: 0.9924\n",
      "Epoch 00094: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 3.1782 - accuracy: 0.9923 - val_loss: 7.9717 - val_accuracy: 0.9873\n",
      "Epoch 95/500\n",
      "193/207 [==========================>...] - ETA: 0s - loss: 3.2603 - accuracy: 0.9934\n",
      "Epoch 00095: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 3.3105 - accuracy: 0.9935 - val_loss: 10.0170 - val_accuracy: 0.9782\n",
      "Epoch 96/500\n",
      "188/207 [==========================>...] - ETA: 0s - loss: 3.0873 - accuracy: 0.9940\n",
      "Epoch 00096: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 3.0894 - accuracy: 0.9942 - val_loss: 10.5182 - val_accuracy: 0.9861\n",
      "Epoch 97/500\n",
      "193/207 [==========================>...] - ETA: 0s - loss: 3.2012 - accuracy: 0.9945\n",
      "Epoch 00097: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 3.1666 - accuracy: 0.9945 - val_loss: 7.8842 - val_accuracy: 0.9873\n",
      "Epoch 98/500\n",
      "185/207 [=========================>....] - ETA: 0s - loss: 3.5773 - accuracy: 0.9961\n",
      "Epoch 00098: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 3.5690 - accuracy: 0.9956 - val_loss: 7.6573 - val_accuracy: 0.9691\n",
      "Epoch 99/500\n",
      "190/207 [==========================>...] - ETA: 0s - loss: 3.5310 - accuracy: 0.9933\n",
      "Epoch 00099: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 3.4387 - accuracy: 0.9936 - val_loss: 8.4540 - val_accuracy: 0.9867\n",
      "Epoch 100/500\n",
      "205/207 [============================>.] - ETA: 0s - loss: 3.1746 - accuracy: 0.9931\n",
      "Epoch 00100: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 3.1821 - accuracy: 0.9932 - val_loss: 7.1222 - val_accuracy: 0.9909\n",
      "Epoch 101/500\n",
      "199/207 [===========================>..] - ETA: 0s - loss: 3.0883 - accuracy: 0.9920\n",
      "Epoch 00101: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 3.1232 - accuracy: 0.9921 - val_loss: 7.3115 - val_accuracy: 0.9915\n",
      "Epoch 102/500\n",
      "195/207 [===========================>..] - ETA: 0s - loss: 3.0704 - accuracy: 0.9942\n",
      "Epoch 00102: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 3.0793 - accuracy: 0.9944 - val_loss: 6.8274 - val_accuracy: 0.9915\n",
      "Epoch 103/500\n",
      "189/207 [==========================>...] - ETA: 0s - loss: 3.1291 - accuracy: 0.9940\n",
      "Epoch 00103: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 3.0920 - accuracy: 0.9938 - val_loss: 6.8111 - val_accuracy: 0.9830\n",
      "Epoch 104/500\n",
      "207/207 [==============================] - ETA: 0s - loss: 3.1276 - accuracy: 0.9944\n",
      "Epoch 00104: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 3.1276 - accuracy: 0.9944 - val_loss: 6.7979 - val_accuracy: 0.9788\n",
      "Epoch 105/500\n",
      "187/207 [==========================>...] - ETA: 0s - loss: 2.9467 - accuracy: 0.9926\n",
      "Epoch 00105: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 2.9394 - accuracy: 0.9933 - val_loss: 6.8761 - val_accuracy: 0.9836\n",
      "Epoch 106/500\n",
      "201/207 [============================>.] - ETA: 0s - loss: 3.0243 - accuracy: 0.9947\n",
      "Epoch 00106: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 3.0146 - accuracy: 0.9945 - val_loss: 7.6014 - val_accuracy: 0.9891\n",
      "Epoch 107/500\n",
      "187/207 [==========================>...] - ETA: 0s - loss: 2.8880 - accuracy: 0.9938\n",
      "Epoch 00107: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 2.8935 - accuracy: 0.9941 - val_loss: 6.8355 - val_accuracy: 0.9848\n",
      "Epoch 108/500\n",
      "204/207 [============================>.] - ETA: 0s - loss: 3.0291 - accuracy: 0.9937\n",
      "Epoch 00108: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 3.0276 - accuracy: 0.9938 - val_loss: 7.2599 - val_accuracy: 0.9812\n",
      "Epoch 109/500\n",
      "205/207 [============================>.] - ETA: 0s - loss: 3.0907 - accuracy: 0.9936\n",
      "Epoch 00109: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 3.0923 - accuracy: 0.9936 - val_loss: 7.3493 - val_accuracy: 0.9770\n",
      "Epoch 110/500\n",
      "188/207 [==========================>...] - ETA: 0s - loss: 3.3268 - accuracy: 0.9929\n",
      "Epoch 00110: val_loss did not improve from 6.35743\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 3.3250 - accuracy: 0.9929 - val_loss: 8.8898 - val_accuracy: 0.9836\n",
      "Epoch 111/500\n",
      "185/207 [=========================>....] - ETA: 0s - loss: 3.1590 - accuracy: 0.9934\n",
      "Epoch 00111: val_loss improved from 6.35743 to 5.57394, saving model to fraction_of_coating_40_50\\Weights-111--5.57394.hdf5\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 3.2065 - accuracy: 0.9938 - val_loss: 5.5739 - val_accuracy: 0.9903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/500\n",
      "197/207 [===========================>..] - ETA: 0s - loss: 3.3501 - accuracy: 0.9949\n",
      "Epoch 00112: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 3.3310 - accuracy: 0.9950 - val_loss: 8.3391 - val_accuracy: 0.9606\n",
      "Epoch 113/500\n",
      "189/207 [==========================>...] - ETA: 0s - loss: 2.9072 - accuracy: 0.9944\n",
      "Epoch 00113: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 2.8756 - accuracy: 0.9945 - val_loss: 6.9780 - val_accuracy: 0.9855\n",
      "Epoch 114/500\n",
      "186/207 [=========================>....] - ETA: 0s - loss: 3.2416 - accuracy: 0.9946\n",
      "Epoch 00114: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 3.3837 - accuracy: 0.9947 - val_loss: 7.2916 - val_accuracy: 0.9867\n",
      "Epoch 115/500\n",
      "192/207 [==========================>...] - ETA: 0s - loss: 3.6248 - accuracy: 0.9937\n",
      "Epoch 00115: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 3.5980 - accuracy: 0.9933 - val_loss: 6.9111 - val_accuracy: 0.9824\n",
      "Epoch 116/500\n",
      "194/207 [===========================>..] - ETA: 0s - loss: 3.0427 - accuracy: 0.9931\n",
      "Epoch 00116: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 3.0425 - accuracy: 0.9935 - val_loss: 6.4996 - val_accuracy: 0.9861\n",
      "Epoch 117/500\n",
      "204/207 [============================>.] - ETA: 0s - loss: 3.2683 - accuracy: 0.9940\n",
      "Epoch 00117: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 3.2714 - accuracy: 0.9941 - val_loss: 8.6043 - val_accuracy: 0.9812\n",
      "Epoch 118/500\n",
      "202/207 [============================>.] - ETA: 0s - loss: 3.0936 - accuracy: 0.9954\n",
      "Epoch 00118: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 3.1013 - accuracy: 0.9951 - val_loss: 10.6699 - val_accuracy: 0.9794\n",
      "Epoch 119/500\n",
      "195/207 [===========================>..] - ETA: 0s - loss: 3.3395 - accuracy: 0.9941\n",
      "Epoch 00119: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 3.3487 - accuracy: 0.9941 - val_loss: 8.6785 - val_accuracy: 0.9848\n",
      "Epoch 120/500\n",
      "195/207 [===========================>..] - ETA: 0s - loss: 3.3148 - accuracy: 0.9952\n",
      "Epoch 00120: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 3.3056 - accuracy: 0.9951 - val_loss: 7.5347 - val_accuracy: 0.9867\n",
      "Epoch 121/500\n",
      "189/207 [==========================>...] - ETA: 0s - loss: 3.0522 - accuracy: 0.9940\n",
      "Epoch 00121: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 3.0807 - accuracy: 0.9936 - val_loss: 7.6102 - val_accuracy: 0.9758\n",
      "Epoch 122/500\n",
      "185/207 [=========================>....] - ETA: 0s - loss: 3.1660 - accuracy: 0.9941\n",
      "Epoch 00122: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 3.1648 - accuracy: 0.9941 - val_loss: 6.5809 - val_accuracy: 0.9824\n",
      "Epoch 123/500\n",
      "187/207 [==========================>...] - ETA: 0s - loss: 3.4708 - accuracy: 0.9936\n",
      "Epoch 00123: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 3.4387 - accuracy: 0.9936 - val_loss: 8.1650 - val_accuracy: 0.9830\n",
      "Epoch 124/500\n",
      "186/207 [=========================>....] - ETA: 0s - loss: 3.0927 - accuracy: 0.9955\n",
      "Epoch 00124: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 3.1209 - accuracy: 0.9953 - val_loss: 6.1422 - val_accuracy: 0.9836\n",
      "Epoch 125/500\n",
      "185/207 [=========================>....] - ETA: 0s - loss: 2.8235 - accuracy: 0.9936\n",
      "Epoch 00125: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 2.8469 - accuracy: 0.9942 - val_loss: 9.0482 - val_accuracy: 0.9830\n",
      "Epoch 126/500\n",
      "206/207 [============================>.] - ETA: 0s - loss: 2.6771 - accuracy: 0.9953\n",
      "Epoch 00126: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 2.6762 - accuracy: 0.9953 - val_loss: 8.2565 - val_accuracy: 0.9867\n",
      "Epoch 127/500\n",
      "206/207 [============================>.] - ETA: 0s - loss: 2.8000 - accuracy: 0.9953\n",
      "Epoch 00127: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 2.8000 - accuracy: 0.9953 - val_loss: 7.3624 - val_accuracy: 0.9848\n",
      "Epoch 128/500\n",
      "199/207 [===========================>..] - ETA: 0s - loss: 3.0823 - accuracy: 0.9947\n",
      "Epoch 00128: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 3.0882 - accuracy: 0.9947 - val_loss: 7.0111 - val_accuracy: 0.9848\n",
      "Epoch 129/500\n",
      "203/207 [============================>.] - ETA: 0s - loss: 2.9643 - accuracy: 0.9926\n",
      "Epoch 00129: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 2.9655 - accuracy: 0.9926 - val_loss: 6.9446 - val_accuracy: 0.9721\n",
      "Epoch 130/500\n",
      "205/207 [============================>.] - ETA: 0s - loss: 3.0449 - accuracy: 0.9934\n",
      "Epoch 00130: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 3.0413 - accuracy: 0.9935 - val_loss: 9.0799 - val_accuracy: 0.9812\n",
      "Epoch 131/500\n",
      "185/207 [=========================>....] - ETA: 0s - loss: 2.8548 - accuracy: 0.9948\n",
      "Epoch 00131: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 2.8937 - accuracy: 0.9948 - val_loss: 6.7069 - val_accuracy: 0.9861\n",
      "Epoch 132/500\n",
      "189/207 [==========================>...] - ETA: 0s - loss: 2.6907 - accuracy: 0.9964\n",
      "Epoch 00132: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 2.6972 - accuracy: 0.9967 - val_loss: 7.6377 - val_accuracy: 0.9855\n",
      "Epoch 133/500\n",
      "189/207 [==========================>...] - ETA: 0s - loss: 2.9662 - accuracy: 0.9919\n",
      "Epoch 00133: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 3.0016 - accuracy: 0.9915 - val_loss: 9.9055 - val_accuracy: 0.9867\n",
      "Epoch 134/500\n",
      "194/207 [===========================>..] - ETA: 0s - loss: 2.9302 - accuracy: 0.9948\n",
      "Epoch 00134: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 2.9270 - accuracy: 0.9950 - val_loss: 7.9569 - val_accuracy: 0.9830\n",
      "Epoch 135/500\n",
      "190/207 [==========================>...] - ETA: 0s - loss: 2.9710 - accuracy: 0.9929\n",
      "Epoch 00135: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 3.0029 - accuracy: 0.9930 - val_loss: 9.8751 - val_accuracy: 0.9915\n",
      "Epoch 136/500\n",
      "195/207 [===========================>..] - ETA: 0s - loss: 2.9154 - accuracy: 0.9944\n",
      "Epoch 00136: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 2.9692 - accuracy: 0.9942 - val_loss: 7.1075 - val_accuracy: 0.9909\n",
      "Epoch 137/500\n",
      "194/207 [===========================>..] - ETA: 0s - loss: 3.2865 - accuracy: 0.9939\n",
      "Epoch 00137: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 3.2955 - accuracy: 0.9938 - val_loss: 7.7193 - val_accuracy: 0.9491\n",
      "Epoch 138/500\n",
      "190/207 [==========================>...] - ETA: 0s - loss: 2.9789 - accuracy: 0.9952\n",
      "Epoch 00138: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 2.9495 - accuracy: 0.9951 - val_loss: 9.8047 - val_accuracy: 0.9879\n",
      "Epoch 139/500\n",
      "193/207 [==========================>...] - ETA: 0s - loss: 2.8514 - accuracy: 0.9937\n",
      "Epoch 00139: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 2.8411 - accuracy: 0.9939 - val_loss: 6.2650 - val_accuracy: 0.9903\n",
      "Epoch 140/500\n",
      "185/207 [=========================>....] - ETA: 0s - loss: 2.6793 - accuracy: 0.9951\n",
      "Epoch 00140: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 2.7067 - accuracy: 0.9947 - val_loss: 7.0896 - val_accuracy: 0.9812\n",
      "Epoch 141/500\n",
      "203/207 [============================>.] - ETA: 0s - loss: 3.0928 - accuracy: 0.9938\n",
      "Epoch 00141: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 3.0895 - accuracy: 0.9939 - val_loss: 8.4517 - val_accuracy: 0.9897\n",
      "Epoch 142/500\n",
      "196/207 [===========================>..] - ETA: 0s - loss: 2.7203 - accuracy: 0.9935\n",
      "Epoch 00142: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 2.7370 - accuracy: 0.9935 - val_loss: 8.1069 - val_accuracy: 0.9818\n",
      "Epoch 143/500\n",
      "201/207 [============================>.] - ETA: 0s - loss: 2.9016 - accuracy: 0.9942\n",
      "Epoch 00143: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 2.9197 - accuracy: 0.9944 - val_loss: 9.2895 - val_accuracy: 0.9733\n",
      "Epoch 144/500\n",
      "202/207 [============================>.] - ETA: 0s - loss: 2.8492 - accuracy: 0.9940\n",
      "Epoch 00144: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 2.8385 - accuracy: 0.9939 - val_loss: 8.3945 - val_accuracy: 0.9891\n",
      "Epoch 145/500\n",
      "192/207 [==========================>...] - ETA: 0s - loss: 3.0263 - accuracy: 0.9943\n",
      "Epoch 00145: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 2.9808 - accuracy: 0.9945 - val_loss: 7.6396 - val_accuracy: 0.9861\n",
      "Epoch 146/500\n",
      "194/207 [===========================>..] - ETA: 0s - loss: 2.9325 - accuracy: 0.9961\n",
      "Epoch 00146: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 2.9307 - accuracy: 0.9959 - val_loss: 6.9432 - val_accuracy: 0.9885\n",
      "Epoch 147/500\n",
      "197/207 [===========================>..] - ETA: 0s - loss: 2.9486 - accuracy: 0.9932\n",
      "Epoch 00147: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 2.9411 - accuracy: 0.9933 - val_loss: 8.3438 - val_accuracy: 0.9867\n",
      "Epoch 148/500\n",
      "200/207 [===========================>..] - ETA: 0s - loss: 2.9356 - accuracy: 0.9948\n",
      "Epoch 00148: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 2.9269 - accuracy: 0.9948 - val_loss: 7.2152 - val_accuracy: 0.9873\n",
      "Epoch 149/500\n",
      "190/207 [==========================>...] - ETA: 0s - loss: 2.7491 - accuracy: 0.9956\n",
      "Epoch 00149: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 2.7160 - accuracy: 0.9951 - val_loss: 6.7654 - val_accuracy: 0.9915\n",
      "Epoch 150/500\n",
      "199/207 [===========================>..] - ETA: 0s - loss: 2.7712 - accuracy: 0.9942\n",
      "Epoch 00150: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 2.7699 - accuracy: 0.9941 - val_loss: 7.7129 - val_accuracy: 0.9842\n",
      "Epoch 151/500\n",
      "201/207 [============================>.] - ETA: 0s - loss: 2.5130 - accuracy: 0.9947\n",
      "Epoch 00151: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 2.5136 - accuracy: 0.9948 - val_loss: 6.9269 - val_accuracy: 0.9667\n",
      "Epoch 152/500\n",
      "201/207 [============================>.] - ETA: 0s - loss: 2.7777 - accuracy: 0.9942\n",
      "Epoch 00152: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 2.7900 - accuracy: 0.9944 - val_loss: 6.9374 - val_accuracy: 0.9867\n",
      "Epoch 153/500\n",
      "199/207 [===========================>..] - ETA: 0s - loss: 2.9201 - accuracy: 0.9947\n",
      "Epoch 00153: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 2.9190 - accuracy: 0.9945 - val_loss: 8.7654 - val_accuracy: 0.9903\n",
      "Epoch 154/500\n",
      "203/207 [============================>.] - ETA: 0s - loss: 3.0040 - accuracy: 0.9951\n",
      "Epoch 00154: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 3.0096 - accuracy: 0.9951 - val_loss: 7.6029 - val_accuracy: 0.9848\n",
      "Epoch 155/500\n",
      "202/207 [============================>.] - ETA: 0s - loss: 2.6949 - accuracy: 0.9946\n",
      "Epoch 00155: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 2.6956 - accuracy: 0.9947 - val_loss: 9.2342 - val_accuracy: 0.9903\n",
      "Epoch 156/500\n",
      "196/207 [===========================>..] - ETA: 0s - loss: 2.7264 - accuracy: 0.9960\n",
      "Epoch 00156: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 2.7183 - accuracy: 0.9958 - val_loss: 7.4350 - val_accuracy: 0.9897\n",
      "Epoch 157/500\n",
      "187/207 [==========================>...] - ETA: 0s - loss: 2.7604 - accuracy: 0.9935\n",
      "Epoch 00157: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 2.7266 - accuracy: 0.9938 - val_loss: 6.6221 - val_accuracy: 0.9848\n",
      "Epoch 158/500\n",
      "184/207 [=========================>....] - ETA: 0s - loss: 3.1778 - accuracy: 0.9947\n",
      "Epoch 00158: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 3.1097 - accuracy: 0.9944 - val_loss: 7.6701 - val_accuracy: 0.9842\n",
      "Epoch 159/500\n",
      "191/207 [==========================>...] - ETA: 0s - loss: 2.8632 - accuracy: 0.9944\n",
      "Epoch 00159: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 2.8241 - accuracy: 0.9944 - val_loss: 7.5377 - val_accuracy: 0.9891\n",
      "Epoch 160/500\n",
      "192/207 [==========================>...] - ETA: 0s - loss: 2.9886 - accuracy: 0.9951\n",
      "Epoch 00160: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 2.9522 - accuracy: 0.9953 - val_loss: 7.5576 - val_accuracy: 0.9891\n",
      "Epoch 161/500\n",
      "191/207 [==========================>...] - ETA: 0s - loss: 2.5991 - accuracy: 0.9957\n",
      "Epoch 00161: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 2.5864 - accuracy: 0.9959 - val_loss: 7.4671 - val_accuracy: 0.9885\n",
      "Epoch 162/500\n",
      "183/207 [=========================>....] - ETA: 0s - loss: 2.6178 - accuracy: 0.9932\n",
      "Epoch 00162: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 2.6352 - accuracy: 0.9935 - val_loss: 9.1521 - val_accuracy: 0.9879\n",
      "Epoch 163/500\n",
      "195/207 [===========================>..] - ETA: 0s - loss: 2.8805 - accuracy: 0.9941\n",
      "Epoch 00163: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 2.8959 - accuracy: 0.9944 - val_loss: 7.6520 - val_accuracy: 0.9891\n",
      "Epoch 164/500\n",
      "195/207 [===========================>..] - ETA: 0s - loss: 2.6044 - accuracy: 0.9947\n",
      "Epoch 00164: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 2.5956 - accuracy: 0.9950 - val_loss: 6.7417 - val_accuracy: 0.9903\n",
      "Epoch 165/500\n",
      "197/207 [===========================>..] - ETA: 0s - loss: 2.9543 - accuracy: 0.9940\n",
      "Epoch 00165: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 2.9380 - accuracy: 0.9942 - val_loss: 8.4861 - val_accuracy: 0.9861\n",
      "Epoch 166/500\n",
      "203/207 [============================>.] - ETA: 0s - loss: 3.1001 - accuracy: 0.9958\n",
      "Epoch 00166: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 3.0937 - accuracy: 0.9959 - val_loss: 7.8022 - val_accuracy: 0.9891\n",
      "Epoch 167/500\n",
      "183/207 [=========================>....] - ETA: 0s - loss: 2.6745 - accuracy: 0.9939\n",
      "Epoch 00167: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 2.8525 - accuracy: 0.9938 - val_loss: 7.0358 - val_accuracy: 0.9867\n",
      "Epoch 168/500\n",
      "199/207 [===========================>..] - ETA: 0s - loss: 2.6509 - accuracy: 0.9937\n",
      "Epoch 00168: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 2.6436 - accuracy: 0.9936 - val_loss: 7.9202 - val_accuracy: 0.9885\n",
      "Epoch 169/500\n",
      "191/207 [==========================>...] - ETA: 0s - loss: 2.7043 - accuracy: 0.9944\n",
      "Epoch 00169: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 2.7603 - accuracy: 0.9945 - val_loss: 7.1567 - val_accuracy: 0.9891\n",
      "Epoch 170/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199/207 [===========================>..] - ETA: 0s - loss: 2.7703 - accuracy: 0.9939\n",
      "Epoch 00170: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 2.7681 - accuracy: 0.9938 - val_loss: 7.5571 - val_accuracy: 0.9891\n",
      "Epoch 171/500\n",
      "189/207 [==========================>...] - ETA: 0s - loss: 2.7441 - accuracy: 0.9952\n",
      "Epoch 00171: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 2.7319 - accuracy: 0.9950 - val_loss: 7.9777 - val_accuracy: 0.9909\n",
      "Epoch 172/500\n",
      "202/207 [============================>.] - ETA: 0s - loss: 2.9846 - accuracy: 0.9941\n",
      "Epoch 00172: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 2.9744 - accuracy: 0.9941 - val_loss: 7.2096 - val_accuracy: 0.9885\n",
      "Epoch 173/500\n",
      "183/207 [=========================>....] - ETA: 0s - loss: 2.6900 - accuracy: 0.9952\n",
      "Epoch 00173: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 2.7224 - accuracy: 0.9953 - val_loss: 6.1277 - val_accuracy: 0.9897\n",
      "Epoch 174/500\n",
      "189/207 [==========================>...] - ETA: 0s - loss: 2.9398 - accuracy: 0.9942\n",
      "Epoch 00174: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 2.9031 - accuracy: 0.9947 - val_loss: 8.4834 - val_accuracy: 0.9788\n",
      "Epoch 175/500\n",
      "190/207 [==========================>...] - ETA: 0s - loss: 2.7793 - accuracy: 0.9949\n",
      "Epoch 00175: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 2.7560 - accuracy: 0.9948 - val_loss: 6.2435 - val_accuracy: 0.9842\n",
      "Epoch 176/500\n",
      "188/207 [==========================>...] - ETA: 0s - loss: 2.6420 - accuracy: 0.9942\n",
      "Epoch 00176: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 2.6378 - accuracy: 0.9936 - val_loss: 8.5091 - val_accuracy: 0.9903\n",
      "Epoch 177/500\n",
      "187/207 [==========================>...] - ETA: 0s - loss: 2.9563 - accuracy: 0.9938\n",
      "Epoch 00177: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 2.9154 - accuracy: 0.9941 - val_loss: 9.8590 - val_accuracy: 0.9867\n",
      "Epoch 178/500\n",
      "201/207 [============================>.] - ETA: 0s - loss: 3.0713 - accuracy: 0.9938\n",
      "Epoch 00178: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 3.0698 - accuracy: 0.9935 - val_loss: 7.7368 - val_accuracy: 0.9921\n",
      "Epoch 179/500\n",
      "200/207 [===========================>..] - ETA: 0s - loss: 2.8116 - accuracy: 0.9953\n",
      "Epoch 00179: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 2.7954 - accuracy: 0.9953 - val_loss: 6.1473 - val_accuracy: 0.9867\n",
      "Epoch 180/500\n",
      "202/207 [============================>.] - ETA: 0s - loss: 3.2585 - accuracy: 0.9943\n",
      "Epoch 00180: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 3.2533 - accuracy: 0.9944 - val_loss: 8.8479 - val_accuracy: 0.9794\n",
      "Epoch 181/500\n",
      "204/207 [============================>.] - ETA: 0s - loss: 2.7085 - accuracy: 0.9951\n",
      "Epoch 00181: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 2.7249 - accuracy: 0.9951 - val_loss: 10.0628 - val_accuracy: 0.9897\n",
      "Epoch 182/500\n",
      "192/207 [==========================>...] - ETA: 0s - loss: 2.8222 - accuracy: 0.9940\n",
      "Epoch 00182: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 2.8054 - accuracy: 0.9936 - val_loss: 7.7506 - val_accuracy: 0.9885\n",
      "Epoch 183/500\n",
      "189/207 [==========================>...] - ETA: 0s - loss: 2.6215 - accuracy: 0.9940\n",
      "Epoch 00183: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 2.6557 - accuracy: 0.9941 - val_loss: 8.8582 - val_accuracy: 0.9897\n",
      "Epoch 184/500\n",
      "205/207 [============================>.] - ETA: 0s - loss: 2.8151 - accuracy: 0.9939\n",
      "Epoch 00184: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 2.8121 - accuracy: 0.9938 - val_loss: 8.2234 - val_accuracy: 0.9921\n",
      "Epoch 185/500\n",
      "201/207 [============================>.] - ETA: 0s - loss: 2.7409 - accuracy: 0.9953\n",
      "Epoch 00185: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 2.7422 - accuracy: 0.9955 - val_loss: 6.4988 - val_accuracy: 0.9848\n",
      "Epoch 186/500\n",
      "200/207 [===========================>..] - ETA: 0s - loss: 2.6400 - accuracy: 0.9955\n",
      "Epoch 00186: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 2.6552 - accuracy: 0.9955 - val_loss: 9.0999 - val_accuracy: 0.9721\n",
      "Epoch 187/500\n",
      "193/207 [==========================>...] - ETA: 0s - loss: 2.7668 - accuracy: 0.9950\n",
      "Epoch 00187: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 2.7582 - accuracy: 0.9950 - val_loss: 9.2331 - val_accuracy: 0.9848\n",
      "Epoch 188/500\n",
      "185/207 [=========================>....] - ETA: 0s - loss: 2.5610 - accuracy: 0.9936\n",
      "Epoch 00188: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 2.5962 - accuracy: 0.9938 - val_loss: 7.9117 - val_accuracy: 0.9915\n",
      "Epoch 189/500\n",
      "205/207 [============================>.] - ETA: 0s - loss: 2.5870 - accuracy: 0.9948\n",
      "Epoch 00189: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 2.5881 - accuracy: 0.9948 - val_loss: 8.9912 - val_accuracy: 0.9897\n",
      "Epoch 190/500\n",
      "196/207 [===========================>..] - ETA: 0s - loss: 2.9821 - accuracy: 0.9943\n",
      "Epoch 00190: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 2.9570 - accuracy: 0.9945 - val_loss: 7.1020 - val_accuracy: 0.9800\n",
      "Epoch 191/500\n",
      "200/207 [===========================>..] - ETA: 0s - loss: 2.5731 - accuracy: 0.9942\n",
      "Epoch 00191: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 2.5624 - accuracy: 0.9942 - val_loss: 7.9110 - val_accuracy: 0.9909\n",
      "Epoch 192/500\n",
      "200/207 [===========================>..] - ETA: 0s - loss: 2.6342 - accuracy: 0.9942\n",
      "Epoch 00192: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 2.6172 - accuracy: 0.9939 - val_loss: 8.1062 - val_accuracy: 0.9909\n",
      "Epoch 193/500\n",
      "202/207 [============================>.] - ETA: 0s - loss: 3.1068 - accuracy: 0.9940\n",
      "Epoch 00193: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 3.0908 - accuracy: 0.9939 - val_loss: 7.7674 - val_accuracy: 0.9861\n",
      "Epoch 194/500\n",
      "190/207 [==========================>...] - ETA: 0s - loss: 2.7315 - accuracy: 0.9951\n",
      "Epoch 00194: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 2.7624 - accuracy: 0.9948 - val_loss: 8.6417 - val_accuracy: 0.9903\n",
      "Epoch 195/500\n",
      "198/207 [===========================>..] - ETA: 0s - loss: 3.0360 - accuracy: 0.9945\n",
      "Epoch 00195: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 3.0195 - accuracy: 0.9947 - val_loss: 8.6504 - val_accuracy: 0.9800\n",
      "Epoch 196/500\n",
      "197/207 [===========================>..] - ETA: 0s - loss: 2.6924 - accuracy: 0.9951\n",
      "Epoch 00196: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 2.6937 - accuracy: 0.9950 - val_loss: 6.9436 - val_accuracy: 0.9885\n",
      "Epoch 197/500\n",
      "200/207 [===========================>..] - ETA: 0s - loss: 2.7136 - accuracy: 0.9934\n",
      "Epoch 00197: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 2.7348 - accuracy: 0.9933 - val_loss: 6.7356 - val_accuracy: 0.9897\n",
      "Epoch 198/500\n",
      "198/207 [===========================>..] - ETA: 0s - loss: 2.8844 - accuracy: 0.9943\n",
      "Epoch 00198: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 2.8960 - accuracy: 0.9942 - val_loss: 7.9337 - val_accuracy: 0.9879\n",
      "Epoch 199/500\n",
      "195/207 [===========================>..] - ETA: 0s - loss: 2.9707 - accuracy: 0.9929\n",
      "Epoch 00199: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 2.9160 - accuracy: 0.9932 - val_loss: 7.2848 - val_accuracy: 0.9873\n",
      "Epoch 200/500\n",
      "202/207 [============================>.] - ETA: 0s - loss: 2.6346 - accuracy: 0.9950\n",
      "Epoch 00200: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 2.6274 - accuracy: 0.9950 - val_loss: 8.4629 - val_accuracy: 0.9873\n",
      "Epoch 201/500\n",
      "197/207 [===========================>..] - ETA: 0s - loss: 2.9141 - accuracy: 0.9960\n",
      "Epoch 00201: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 2.8978 - accuracy: 0.9958 - val_loss: 7.0883 - val_accuracy: 0.9891\n",
      "Epoch 202/500\n",
      "182/207 [=========================>....] - ETA: 0s - loss: 2.7462 - accuracy: 0.9943\n",
      "Epoch 00202: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 2.6655 - accuracy: 0.9947 - val_loss: 8.3119 - val_accuracy: 0.9848\n",
      "Epoch 203/500\n",
      "200/207 [===========================>..] - ETA: 0s - loss: 2.9176 - accuracy: 0.9945\n",
      "Epoch 00203: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 2.9057 - accuracy: 0.9947 - val_loss: 7.2772 - val_accuracy: 0.9891\n",
      "Epoch 204/500\n",
      "196/207 [===========================>..] - ETA: 0s - loss: 2.4444 - accuracy: 0.9946\n",
      "Epoch 00204: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 2.4377 - accuracy: 0.9948 - val_loss: 7.5831 - val_accuracy: 0.9836\n",
      "Epoch 205/500\n",
      "204/207 [============================>.] - ETA: 0s - loss: 2.9754 - accuracy: 0.9934\n",
      "Epoch 00205: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 2.9708 - accuracy: 0.9935 - val_loss: 7.2702 - val_accuracy: 0.9891\n",
      "Epoch 206/500\n",
      "206/207 [============================>.] - ETA: 0s - loss: 2.5962 - accuracy: 0.9941\n",
      "Epoch 00206: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 2.5968 - accuracy: 0.9941 - val_loss: 8.9499 - val_accuracy: 0.9612\n",
      "Epoch 207/500\n",
      "194/207 [===========================>..] - ETA: 0s - loss: 2.3782 - accuracy: 0.9953\n",
      "Epoch 00207: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 2.4133 - accuracy: 0.9948 - val_loss: 8.6867 - val_accuracy: 0.9891\n",
      "Epoch 208/500\n",
      "191/207 [==========================>...] - ETA: 0s - loss: 2.6106 - accuracy: 0.9953\n",
      "Epoch 00208: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 2.6208 - accuracy: 0.9953 - val_loss: 7.7743 - val_accuracy: 0.9612\n",
      "Epoch 209/500\n",
      "197/207 [===========================>..] - ETA: 0s - loss: 2.6956 - accuracy: 0.9959\n",
      "Epoch 00209: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 2.6724 - accuracy: 0.9959 - val_loss: 7.1600 - val_accuracy: 0.9842\n",
      "Epoch 210/500\n",
      "203/207 [============================>.] - ETA: 0s - loss: 2.6497 - accuracy: 0.9958\n",
      "Epoch 00210: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 2.6390 - accuracy: 0.9959 - val_loss: 7.4226 - val_accuracy: 0.9824\n",
      "Epoch 211/500\n",
      "194/207 [===========================>..] - ETA: 0s - loss: 2.6084 - accuracy: 0.9945\n",
      "Epoch 00211: val_loss did not improve from 5.57394\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 2.5699 - accuracy: 0.9947 - val_loss: 8.4760 - val_accuracy: 0.9879\n",
      "Epoch 00211: early stopping\n"
     ]
    }
   ],
   "source": [
    "history= NN_model2.fit(X_train, Y_train, epochs=500, batch_size=32, validation_split = 0.2, callbacks=callback_list)\n",
    "#history= NN_model.fit(X_train, Y_train, epochs=7, batch_size=32, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latest=tf.train.latest_checkpoint(checkpoint_dir)\n",
    "weights_file = 'fraction_of_coating_40_50/Weights-111--5.57394.hdf5' # choose the best checkpoint \n",
    "NN_model2.load_weights(weights_file) # load it\n",
    "NN_model2.compile(loss='mean_absolute_percentage_error', optimizer='adam', metrics=['mean_absolute_percentage_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_model2.save('fraction_of_coating_40_50_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.255, Test loss: 4.066\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluate the model\n",
    "train_loss, train_acc = NN_model2.evaluate(X_train, Y_train, verbose=0)\n",
    "test_loss, test_acc = NN_model2.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Train loss: %.3f, Test loss: %.3f' % (train_loss, test_loss))\n",
    "# plot training history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = NN_model2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percentage error on test set:  [0.02922553 0.05432222 0.03843735]\n"
     ]
    }
   ],
   "source": [
    "error= mean_absolute_percentage_error(Y_test, Y_pred, multioutput='raw_values')   \n",
    "print('Mean absolute percentage error on test set: ', error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
